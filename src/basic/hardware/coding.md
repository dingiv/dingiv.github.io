# 编码
编码是使用计算机处理数据的前提。数据需要通过计算机的编码方式进行**输入**，**处理**，**输出**，**存储**等。计算机使用二进制数字编码，数据通过转化为二进制数存储在计算机中，并通过二进制的逻辑门电路进行处理。

编码就是通过一张表将一个字符**映射**成为一个二进制数。其核心特点是：二进制、离散、有限。
+ 基于二进制的逻辑门电路进行处理；
+ 使用多个逻辑门电路，形成规模化，从而通过离散数学的理论来处理数据；
+ 规模化是有限的，不能无限制拓宽电路的逻辑门数量，多数计算机使用 32 位或者 64 位的规格来作为处理数据的**位宽**；

编码的本质是一种**协议**。如何理解协议？协议一种约定，一种承诺，一种共识，一种规范……一串数字本没有意义，但是，因为书写和阅读的人都使用了同一个协议，使得它完成了信息存储和传递的目的，它就像自然语言和文字一样，是信息的载体，是沟通的桥梁。其实自然语言也本没有意义，但是，使用语言的人们通过一种共识和协议，让语言和文字完成了承载信息的功能。

## 编码单位 Byte
字节是现代计算机处理数据的基本单位。一个字节代表 8 个 bit 位。为什么不是 4 个 bit，或者是 16 bit 为一个字节？字节作为计算机中数据处理和编码的标准单位，源于多个方面的综合考量。
+ 硬件设计：字节与存储和寻址的便利性，字节太小会导致地址空间膨胀，太大会导致存储空间的浪费；包括内存和磁盘，这些设备的存储单元均采用了 8 bit 的设计；
+ 历史遗留：字节是表示字符的理想单位。早期 ASCII 编码用 7 位表示字符（1 字节留 1 位校验），能表示 128 种字符（字母、数字、符号）；历史上的各种设备的位宽不同，但是可以使用 8 位作为一个基本公约数；

### 多字节数据编码


## ASCII
现代计算机起源于英语世界，编码英文世界中的字符可以使用很小的比特数 7 进行编码，而为了最为


## 纯数据和指令
