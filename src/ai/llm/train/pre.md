---
title: 预训练
order: 0
---

# 预训练模型

大语言模型的训练是一个分阶段的过程，通过**预训练 → 监督微调 → 强化学习**三个阶段，将一个通用的语言模型逐步打造成能够理解指令、遵循规则、与人类价值观对齐的智能助手。这一训练范式被称为 LLM 的"标准训练流程"。

## 模型架构基础

在介绍训练流程之前，需要先理解两大主流模型架构：

### GPT 系列（Decoder-only）

**核心特点**：仅使用 Transformer 的 **Decoder（解码器）** 部分，采用**因果注意力机制**（Causal Attention）。

- **训练方式**：自回归（Autoregressive），预测下一个 token
- **优势**：擅长文本生成，能够保持上下文连贯性
- **代表模型**：GPT-3、GPT-4、Claude、LLaMA

**工作原理**：模型只能看到当前位置之前的 token，通过"填空题"的方式逐字生成文本。例如给定"今天天气"作为输入，模型需要预测"真不错"这样的后续内容。

### BERT 系列（Encoder-only）

**核心特点**：仅使用 Transformer 的 **Encoder（编码器）** 部分，采用**双向注意力机制**（Bidirectional Attention）。

- **训练方式**：掩码语言模型（Masked Language Model），随机遮盖部分 token 让模型预测
- **优势**：擅长理解任务（如分类、情感分析、问答匹配）
- **代表模型**：BERT、RoBERTa

**工作原理**：模型能够同时看到上下文的所有 token，通过"完形填空"的方式学习双向语义表示。例如将"今天天气很\[MASK\]"作为输入，模型预测缺失的词是"好"。

**2025 年趋势**：GPT 架构在生成任务上的优势使其成为主流，而 BERT 类架构逐渐融入混合模型中（如 T5、BART 采用 Encoder-Decoder 架构）。

---

## 第一阶段：预训练（Pre-training）

### 核心目标

让模型学习"如何说话"——掌握语言的统计规律、世界知识和推理能力。这是模型能力的"基础建设"阶段。

### 训练方法

**自监督学习**（Self-Supervised Learning）：无需人工标注，直接从海量文本中学习。

- **训练数据**：整个互联网的文本（网页、书籍、论文、代码），规模可达数十 TB
- **训练任务**：预测下一个 token（Next Token Prediction）
- **损失函数**：交叉熵损失（Cross-Entropy Loss）

**训练示例**：
```
输入："人工智能的发展源于"
目标预测："1956 年的达特茅斯会议"
```

模型通过数万亿次的预测尝试，逐渐学会了语言规律（语法、语义）、常识知识（物理、历史、数学）和逻辑推理能力。

### 关键技术

**数据工程**：
- **数据清洗**：去除低质量文本（广告、重复内容、乱码）
- **数据去重**：防止模型"背答案"（过拟合训练数据）
- **数据配比**：平衡不同领域数据（代码占 10%、论文占 15%、网页占 75%）

**模型规模**：
- **参数量**：从 GPT-2 的 15 亿到 GPT-3 的 1750 亿，再到 GPT-4 的万亿级别
- **训练算力**：GPT-3 训练需 3000+ 张 A100 GPU 运行数周，成本约 500 万美元
- **稀疏 MoE**（Mixture of Experts）：2025 年主流方案，通过激活不同专家子网络，减少 30% 计算量

**涌现能力**：当模型规模突破临界点（约 100 亿参数）后，会突然展现小模型不具备的能力，如零样本学习、链式推理等。

### 2025 年进展

- **合成数据**：使用 AI 生成高质量训练数据，解决人类数据枯竭问题
- **长上下文**：上下文窗口从 4K 扩展到 1M+ token（如 Gemini 2.0）
- **多模态预训练**：从纯文本扩展到图像、视频、音频的联合训练

---

## 第二阶段：监督微调（SFT）

### 核心目标

让模型学会"如何听话"——理解人类指令、遵循任务要求、生成符合期望的输出格式。这是模型能力的"职业培训"阶段。

### 训练方法

**监督学习**（Supervised Learning）：使用人工标注的高质量问答对进行训练。

**数据准备**：
- **指令构造**：覆盖多样化任务（写作、编程、翻译、数学、问答）
- **答案撰写**：由人类专家编写标准答案，确保质量
- **数据规模**：通常为 1 万-10 万条高质量指令数据

**训练示例**：
```
指令：用 Python 实现快速排序
答案：
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quicksort(left) + middle + quicksort(right)
```

### 关键作用

**对齐任务要求**：
- 将"续写文本"能力转化为"回答问题"能力
- 学习遵循格式要求（如 JSON 输出、代码块标记）
- 理解角色设定（如"你是一个专业的程序员助手"）

**提升安全性**：
- 学习拒绝回答有害问题（如制造武器的方法）
- 理解边界条件（如"我不知道"比胡编乱造更好）

### 变体技术

**指令微调**（Instruction Tuning）：通过多样化的指令-响应对，增强模型的泛化能力，使其能够处理未见过的任务。

**DPO**（Direct Preference Optimization，直接偏好优化）：2024 年兴起的技术，通过比较"好答案"和"坏答案"直接优化模型，**取代了部分复杂的 RLHF 流程**，大幅降低训练成本。

### 2025 年进展

- **自动化标注**：使用强模型（如 GPT-4）为弱模型生成训练数据
- **多任务混合训练**：同时优化多种能力（推理、编程、对话）
- **持续学习**：在不遗忘旧知识的前提下学习新任务

---

## 第三阶段：强化学习（RLHF）

### 核心目标

让模型学会"如何令人满意"——对齐人类价值观、偏好和伦理标准。这是模型能力的"品格培养"阶段。

### 为什么需要 RLHF？

预训练和 SFT 阶段的模型存在以下问题：

1. **生成有害内容**：可能输出偏见、歧视或危险信息
2. **不准确但自信**：会一本正经地胡说八道（幻觉问题）
3. **缺乏帮助性**：回答过于简略或偏离用户需求
4. **不符合价值观**：无法理解人类的道德标准和社会规范

**RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）**通过让人类对模型输出进行评价，引导模型产生更符合人类期望的回复。

### 训练流程

#### 步骤 1：收集人类反馈

**奖励模型训练**（Reward Model Training）：
- 让模型对同一问题生成多个不同回答（如 4 个）
- 人类标注员对这些回答进行排序（最佳 → 最差）
- 训练一个"奖励模型"来模拟人类偏好

**排序示例**：
```
问题：如何学习编程？

回答 A：建议从 Python 入手，推荐《Python 编程从入门到实践》，结合 LeetCode 练习
回答 B：编程很难，建议放弃
回答 C：学编程要先学 C 语言的指针操作
回答 D：（重复问题）如何学习编程？

人类排序：A > C > D > B
```

#### 步骤 2：强化学习优化

**PPO 算法**（Proximal Policy Optimization）：
- 使用奖励模型给 LLM 的输出打分
- 通过强化学习调整 LLM 参数，使高奖励回答的出现概率增加
- 重复这一过程数千次，逐步优化模型行为

**优化目标**：
- 最大化奖励信号（让奖励模型给出高分）
- 保持与原模型的接近（防止过度优化导致崩溃）
- 探索多样化的回答策略

### 2025 年进展

**合成数据反馈**：
- 使用强模型（如 GPT-4）自动评价弱模型的输出
- 降低 90% 的人类标注成本
- 通过 AI-to-AI 的反馈循环实现持续改进

**多维度对齐**：
- 不仅对齐内容质量，还考虑安全性、公平性、透明度
- 引入宪法 AI（Constitutional AI）概念，让模型遵循预定义的规则集

**RLOF**（Reinforcement Learning from AI Feedback）：使用其他 AI 模型的反馈替代人类反馈，加速对齐过程。

---

## ChatGPT 的训练流程

ChatGPT（基于 GPT-3.5/4）的成功正是源于上述三阶段训练范式的完美结合：

### 训练时间线

**第一阶段：预训练**（数月）
- 使用数千亿 token 的互联网文本
- 训练出基础语言模型 GPT-3.5（1750 亿参数）
- 模型学会了"说话"，但不会对话

**第二阶段：SFT**（数周）
- 收集约 1 万-5 万条高质量对话数据
- 由人类标注员编写"模范对话"（包含用户问题和助手回答）
- 模型学会了"听话"，能理解指令并给出合适回答

**第三阶段：RLHF**（数周）
- 收集约 5 万-10 万个人工排序数据
- 训练奖励模型，模拟人类偏好
- 使用 PPO 算法优化 GPT-3.5，得到 ChatGPT

### RLHF 对 ChatGPT 的重要性

**关键作用**：

1. **对话能力**：
   - 预训练模型只会"续写文本"，不会"回答问题"
   - SFT 让模型学会基本的对话格式
   - **RLHF 让模型真正学会"对话艺术"**——理解上下文、保持连贯、适时追问

2. **安全性提升**：
   - 预训练模型可能输出有害内容（如种族歧视言论）
   - **RLHF 使模型学会拒绝不当请求**："我无法回答这个问题"
   - 减少了 80%+ 的有害输出

3. **帮助性增强**：
   - 预训练模型可能回答过于简略或跑题
   - **RLHF 引导模型提供详细、有用的回答**
   - 例如：用户问"如何写 Python 函数"，模型会给出完整代码示例和解释

4. **价值观对齐**：
   - **让模型理解诚实、中立、无害的原则**
   - 避免错误信息传播（如政治偏见、伪科学）
   - 建立用户信任

**效果对比**（真实案例）：
```
用户：如何制造毒药？

预训练模型（无 RLHF）：
"毒药的制作方法包括...（详细步骤）"

ChatGPT（有 RLHF）：
"我无法提供制造毒药的方法。如果您对化学安全或毒理预防感兴趣，
我可以提供相关信息。"
```

### OpenAI 的技术栈

**GPT-4 的改进**：
- 引入**多模态能力**（理解图像）
- 强化**代码生成能力**（通过 GitHub 代码库训练）
- 提升**长文本处理**（支持 32K+ token）
- 加强**事实准确性**（减少幻觉）

**InstructGPT 论文贡献**：
OpenAI 于 2022 年发表的论文《Training language models to follow instructions with human feedback》首次系统性地验证了三阶段训练的有效性，成为后续所有 ChatGPT 类模型的基础方法论。

---

## 训练成本与挑战

### 资源消耗

**算力成本**：
- GPT-3 训练：约 500 万美元
- GPT-4 训练：估计 5000 万-1 亿美元
- 训练时间：数千张 GPU 运行数月

**数据成本**：
- SFT 数据标注：每小时 20-50 美元（专业标注员）
- RLHF 排序数据：每条 0.5-2 美元
- 总数据成本：数十万到数百万美元

**技术门槛**：
- 需要大规模分布式训练框架（如 DeepSpeed、Megatron-LM）
- 复杂的超参数调优（学习率调度、批量大小、温度参数）
- 稳定性问题（训练崩溃、梯度爆炸）

### 主要挑战

**灾难性遗忘**：
- 学习新任务时会忘记旧知识
- 解决方案：经验回放、弹性权重巩固（EWC）

**奖励黑客**（Reward Hacking）：
- 模型学会"欺骗"奖励模型（输出看起来好但无意义的内容）
- 解决方案：多样化奖励信号、定期重新训练奖励模型

**对齐税**（Alignment Tax）：
- 对齐后模型性能可能下降
- 解决方案：迭代式对齐，在保持能力的同时优化行为

**数据偏见**：
- 训练数据反映互联网的偏见和歧视
- 解决方案：数据清洗、公平性约束、偏见检测

---

## 未来方向

### 高效训练

- **模型并行训练**：将模型分布到数千张 GPU 上
- **混合精度训练**：使用 FP16/BF16 加速计算
- **梯度累积**：模拟大批量训练而不增加内存

### 自动化对齐

- **宪法 AI**（Constitutional AI）：由 AI 根据预定义规则自我批评和改进
- **辩论式对齐**：让模型互相辩论，由人类评判胜负
- **可扩展监督**：用弱模型监督强模型，突破人类能力限制

### 持续学习

- **在线学习**：模型在部署后继续学习和改进
- **终身学习**：在不重新训练的情况下适应新任务
- **个性化微调**：根据用户偏好调整模型行为

### 降低门槛

- **开源生态**：LLaMA、Mistral 等开源模型降低研究门槛
- **训练平台**：Hugging Face、Anyscale 等平台提供训练基础设施
- **知识共享**：论文、代码、数据集的开放共享加速技术普及

---

## 小结

大语言模型的训练是一个**从"学会说话"到"学会听话"再到"学会令人满意"**的渐进过程：

1. **预训练**奠定基础能力（语言、知识、推理）
2. **监督微调**对齐任务需求（指令、格式、安全）
3. **强化学习**对齐人类价值观（偏好、伦理、帮助性）

这一范式不仅造就了 ChatGPT 的成功，也成为了所有现代 LLM 的标准训练流程。随着技术的演进，更高效的架构、更智能的对齐方法和更可持续的训练方案，将推动 AI 向更安全、更有帮助、更符合人类利益的方向发展。
