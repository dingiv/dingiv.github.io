---
title: 强化学习
order: 20
---

# 强化学习对齐

监督微调能让模型学会回答问题，但无法保证回答的质量、安全性和帮助性。强化学习对齐（RLHF，Reinforcement Learning from Human Feedback）的目标是让模型学会"如何令人满意"——对齐人类价值观、偏好和伦理标准。这是 ChatGPT 从"能用"到"好用"的关键，也是模型能力的"品格培养"阶段。

## 为什么需要 RLHF

预训练和监督微调后的模型存在明显问题。可能生成有害内容（偏见、歧视或危险信息），会一本正经地胡说八道（幻觉问题），回答过于简略或偏离用户需求，无法理解人类的道德标准和社会规范。这些问题的根源在于，预训练和 SFT 只教会模型"生成文本"，但没有教会模型"生成好的文本"。

"好"是主观的、多维度的。一个回答可能同时涉及准确性（信息是否正确）、帮助性（是否解决了用户问题）、安全性（是否包含有害内容）、诚实性（是否在不知道时承认而非编造）。监督微调可以教会模型基本的格式和边界，但难以优化这些主观质量——标注员很难为每个回答打出一个综合分数，而强化学习通过比较和排序恰好能解决这个问题。

RLHF 的核心思想是用人类偏好训练一个奖励模型，然后用强化学习优化语言模型使高奖励回答的出现概率增加。这种框架将人类的主观判断（哪个回答更好）转化为可优化的数学目标，让模型能够持续改进其行为。

## 奖励模型训练

RLHF 的第一步是训练奖励模型（Reward Model，RM）。奖励模型的目标是模拟人类偏好——给定问题和候选回答，预测人类会给这个回答打多少分。训练数据的收集方式是对同一问题生成多个不同回答（通常是 4-9 个），然后让人类标注员对这些回答进行排序而非打分。排序比打分更可靠——标注员更容易判断"A 比 B 好"，而非给出"A 是 8 分，B 是 6 分"这样的绝对分数。

排序数据需要覆盖多样化的场景。安全性场景包括危险内容（如何制造武器）、歧视性内容（种族、性别偏见）；帮助性场景包括回答完整性（是否解决了问题）、回答深度（是详细解释还是敷衍了事）；诚实性场景包括事实准确性（是否编造信息）、不确定性表达（不知道时是否承认）。每个维度都需要足够的样例，否则奖励模型会过度优化某些维度而忽略其他。

标注员培训是奖励模型成功的关键。不同标注员的标准差异会导致数据不一致，进而导致奖励模型困惑。需要制定明确的标注指南——什么样的回答是有帮助的、什么样的回答是安全的、如何处理模棱两可的情况。定期进行标注员一致性测试，确保标注质量。实践中，OpenAI 等公司投入了大量资源在标注流程上，这是 RLHF 成功的隐形成本。

奖励模型本身是一个小型语言模型（通常是几百 million 参数），在排序数据上训练。损失函数使用排名损失（Ranking Loss）——对于排序对（回答 A > 回答 B），奖励模型应该输出 RM(A) > RM(B)。训练好的奖励模型可以快速给新的回答打分，作为强化学习的优化目标。

## 强化学习优化

第二步是使用奖励模型优化语言模型。常用的算法是 PPO（Proximal Policy Optimization），一种策略梯度强化学习算法。语言模型被视为策略（policy），它根据输入生成回答；奖励模型提供奖励信号（reward）；PPO 算法更新策略参数使高奖励回答的出现概率增加。

PPO 的核心思想是"保守更新"——在优化奖励的同时，防止策略变化过大导致训练崩溃。具体实现上，PPO 使用重要性采样（importance sampling）和裁剪（clipping）技术，限制每步更新时策略的变化幅度。这看似简单，但对训练稳定性至关重要——早期的强化学习算法（如 REINFORCE）在语言模型上经常崩溃，而 PPO 的出现使得 RLHF 变得实用。

训练过程中需要注意几个关键点。首先是 KL 散度约束——防止模型过度优化奖励函数而偏离原始语言模型。KL 散度衡量两个概率分布的差异，约束过大则模型改进有限，约束过小则可能破坏预训练学到的通用能力。实践中通常将 KL 惩罚系数设为 0.1-0.2，需要根据验证集调整。

其次是奖励模型的准确性。奖励模型本身是在人类排序数据上训练的，如果排序数据有偏差或覆盖不足，奖励模型会学到错误的偏好。强化学习会利用这些错误——模型可能学会"欺骗"奖励模型（生成看起来好但无意义的内容），这种现象被称为"奖励黑客"（reward hacking）。解决方案是定期重新训练奖励模型、多样化排序数据、引入额外的安全约束。

PPO 的实现复杂度高。需要同时运行四个模型：语言模型（策略）、参考语言模型（计算 KL 散度）、奖励模型（打分）、价值函数（估计未来奖励的折现和）。每个前向传播需要多次模型调用，计算开销巨大。实践中，通常使用开源库如 TRL、Ray RLlib 加速开发，但调优仍需要大量经验。

## DPO：简化的替代方案

DPO（Direct Preference Optimization，直接偏好优化）是 2024 年兴起的替代方案，彻底改变了 RLHF 的格局。DPO 的洞察是：奖励模型只是中间步骤，真正需要的是语言模型参数。通过数学推导，DPO 证明了可以直接使用偏好对（好答案 vs 坏答案）优化语言模型，无需显式训练奖励模型。

从技术角度看，DPO 将 RLHF 的两阶段流程（先训练奖励模型，再用 RL 优化）简化为单阶段优化。损失函数直接作用于偏好对，数学上等价于 RLHF 的静态版本（奖励模型固定不变）。这个简化的好处巨大——实现简单、训练稳定、无需调优 KL 散度、不需要价值函数。

DPO 的优势在工程实践中明显。训练速度更快（无需四个模型同时运行）、显存占用更低（单个模型）、超参数更少（学习率是主要需要调的参数）、效果接近甚至超过传统 RLHF。对于资源受限的团队，DPO 使得对齐训练变得可行——单张消费级显卡就可以进行 DPO 训练。

当然，DPO 并非万能。对于需要长期规划的任务（如多轮对话、复杂推理），传统 RLHF 的价值函数估计仍有价值。但对于大多数文本生成任务，DPO 已经成为 2025 年的首选方案。

## 实践建议

RLHF/DPO 的数据质量决定效果。排序/偏好数据要覆盖多样化的场景，安全性、帮助性、诚实性三个维度都要有足够的样例。一个常见误区是只关注安全（避免有害内容），而忽略帮助性（模型变得过于谨慎，回答过于简略）。平衡是关键——既要安全，又要有帮助。

标注成本是 RLHF 的主要障碍。人工排序每条成本 0.5-2 美元，10 万条数据的成本就是 5-20 万美元。2025 年的趋势是使用 AI 辅助标注——用强模型（如 GPT-4）为弱模型的回答打分或排序，人工只校验部分样本。RLOF（Reinforcement Learning from AI Feedback）更进一步，完全使用 AI 模型的反馈替代人类反馈，能降低 90% 的标注成本。

训练稳定性需要持续监控。除了奖励曲线，还要监控 KL 散度（是否偏离原始模型太远）、回答长度（是否学会用长回答欺骗奖励模型）、多样性（是否总是生成相似的回答）。设置自动化脚本，在异常情况时暂停训练，能避免浪费计算资源。

评估对齐效果时，人工评估是必需的。自动指标（如奖励分数）只能作为参考，真正重要的是人类对模型输出的主观感受。建立评估集，定期评估模型在安全性、帮助性、诚实性上的表现，防止训练过程中的退化。

对于大多数团队，建议从 DPO 开始。实现简单、训练稳定、成本低，效果接近传统 RLHF。只有在 DPO 无法满足需求（如需要复杂的多步决策）时，才考虑完整的 PPO 流程。开源库如 TRL、llama-factory 都提供了 DPO 的实现，可以直接使用。

最后，对齐不是一次性的任务，而是持续的过程。模型部署后，用户反馈是宝贵的数据来源——收集用户的点赞/点踩、问题报告，用这些数据持续优化模型。建立反馈-重训-部署的闭环，让模型能够持续改进。
