---
title: 微调
order: 10
---

# 大模型微调


二、全参数微调（Full Fine-tuning）
是什么

更新模型全部参数

重新塑造模型分布

特点

表达能力最强

代价最大

灾难性遗忘风险高

现实使用情况

❌ 几乎没人对 >30B 模型这么干

✔ 只在：

7B 以下

极其明确的新任务

数据质量极高

本质

重新长一个脑子

三、参数高效微调（PEFT）——当前主流

这是你最该关心的部分。

1️⃣ LoRA（Low-Rank Adaptation）
原理一句话

不改原权重，只在关键矩阵旁边加一个低秩“偏移量”。

数学上：

W' = W + A·B

优点

显存友好

可插拔

可叠加（理论上）

典型用途

行业模型

风格模型

角色模型

工具使用能力增强

注意事项

LoRA 是能力记忆

不是事实记忆

LoRA 叠多了会冲突

2️⃣ Adapter / IA³

Adapter：在层间插小网络

IA³：只调缩放系数

特点：

比 LoRA 稳定

效果通常略弱

工程复杂度高

现实中：

LoRA 赢麻了

3️⃣ Prefix / Prompt Tuning

学一段“可训练的 prompt 向量”

不改模型结构

优点：

参数极少

快速试验

缺点：

表达能力有限

长期性能不如 LoRA

四、对齐型微调（Alignment Tuning）

这是改变“行为边界”的微调。

1️⃣ SFT（Supervised Fine-Tuning）

人工标注的「输入 → 理想输出」

奠定模型“基本人格”

📌 常见误区

以为 SFT 是教知识
实际上是教“怎么回答”

2️⃣ RLHF（Reinforcement Learning from Human Feedback）

奖励模型 + 强化学习

改善“主观质量”

代价：

贵

难复现

工程复杂

3️⃣ DPO / IPO（当前趋势）

不跑 RL

用偏好对直接优化

优点：

稳定

成本低

效果接近 RLHF

这是 2024–2025 的主流对齐方式。