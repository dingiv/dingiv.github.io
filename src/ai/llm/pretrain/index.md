---
title: 预训练
order: 0
---

# 预训练模型




1. 大批量语料训练，让模型学习几乎整个互联网的文本；
2. 监督微调 SFT，人工生成高质量问答模板；
3. 强化学习，模型自己和自己玩，自己摸索方案

RLHF： reinforcement learning with human feedback
自监督：使用一个 AI 来监督训练另一个 AI


LLM 训练

预训练
自监督学习大规模语料，目标如下一 token 预测。2025 年趋势：稀疏 MoE 架构减少计算 30%。
监督微调
使用标注数据对齐任务，如指令跟随。变体包括 DPO（直接偏好优化），取代部分 RLHF 以简化过程。
强化学习
RLHF（人类反馈强化学习）为主流，结合 PPO 算法。2025 更新：合成数据增强反馈循环，降低人类标注成本。