---
title: 基座模型
order: 0
---

# 基座模型



## 训练流程
GPT 模型的训练是当今大语言模型的开端，其训练不是一蹴而就的，而是通过**预训练、监督微调、强化学习**三个递进阶段，将一个语言模型逐步打造成能够理解指令、遵循规则、与人类价值观对齐的智能助手。这一范式被当作后来 LLM 的"标准训练流程"。

当今的大模型需要

### 预训练
自监督预训练的目标是让模型学会"如何说话"，即掌握语言的统计规律、世界知识和推理能力。这是模型能力的基础建设阶段，也是最消耗资源阶段。训练方法采用**自监督学习**，无需人工标注，直接从海量文本中学习预测下一个 token。

训练数据规模庞大，通常包含整个互联网的文本（网页、书籍、论文、代码），规模可达数十 TB。模型通过数万亿次的预测尝试，逐渐学会了语言规律（语法、语义）、常识知识（物理、历史、数学）和逻辑推理能力。GPT-3 使用了 3000+ 张 A100 GPU 运行数周，成本约 500 万美元，而 GPT-4 的训练成本估计在 5000 万到 1 亿美元之间。

数据工程是预训练成功的关键。原始数据需要经过严格清洗——去除低质量文本（广告、重复内容、乱码）、数据去重（防止模型"背答案"）、数据配比（平衡不同领域数据）。代码数据能提升模型的推理能力，论文数据增强专业知识，网页数据提供通识能力。实践中，一个常见的数据配比是代码占 10%、论文占 15%、网页占 75%，具体比例会根据目标任务调整。

当模型规模突破临界点（约 100 亿参数）后，会突然展现小模型不具备的能力，如零样本学习、链式推理等。这种现象被称为"涌现能力"，也是大语言模型区别于传统模型的核心特征。2025 年的技术趋势包括合成数据（使用 AI 生成高质量训练数据）、长上下文（上下文窗口从 4K 扩展到 1M+ token）、多模态预训练（从纯文本扩展到图像、视频、音频的联合训练）。

### 监督微调
**监督微调**（SFT）的目标是让模型学会"如何听话"，即理解人类指令、遵循任务要求、生成符合期望的输出格式。这是模型能力的"职业培训"阶段，将预训练获得的通用能力转化为实际任务能力。

训练方法采用监督学习，使用人工标注的高质量问答对进行训练。数据准备是关键环节——指令构造需要覆盖多样化任务（写作、编程、翻译、数学、问答），答案撰写由人类专家编写标准答案确保质量，数据规模通常为 1 万到 10 万条。与预训练的海量数据相比，SFT 数据量虽小但质量要求极高，一条高质量标注数据的价值远超一千条原始文本。

SFT 的核心作用是将"续写文本"能力转化为"回答问题"能力。预训练模型看到"今天天气"会继续生成"真不错"这样的续写，而经过 SFT 的模型能理解"今天天气怎么样"是一个需要回答的问题，并生成"今天天气晴朗，温度 25 度"这样的响应。此外，SFT 还让模型学习遵循格式要求（如 JSON 输出、代码块标记）、理解角色设定（如"你是一个专业的程序员助手"）、学习拒绝回答有害问题（如制造武器的方法）。

2024 年兴起的 DPO（Direct Preference Optimization，直接偏好优化）技术，通过比较"好答案"和"坏答案"直接优化模型，取代了部分复杂的 RLHF 流程，大幅降低训练成本。这已成为 2025 年的主流对齐方式。自动化标注也成为趋势——使用强模型（如 GPT-4）为弱模型生成训练数据，既降低成本又能保证质量。

### 强化学习
强化学习（RLHF）的目标是让模型学会"如何令人满意"，即对齐人类价值观、偏好和伦理标准。这是模型能力的"品格培养"阶段，也是让 ChatGPT 从"能用"到"好用"的关键。

预训练和 SFT 阶段的模型存在明显问题：可能生成有害内容（偏见、歧视或危险信息）、会一本正经地胡说八道（幻觉问题）、回答过于简略或偏离用户需求、无法理解人类的道德标准和社会规范。RLHF 通过让人类对模型输出进行评价，引导模型产生更符合人类期望的回复。

训练流程分为两个步骤。第一步是收集人类反馈训练奖励模型——让模型对同一问题生成多个不同回答（如 4 个），人类标注员对这些回答进行排序，然后训练一个"奖励模型"来模拟人类偏好。第二步是使用 PPO 算法（Proximal Policy Optimization）进行强化学习优化——奖励模型给 LLM 的输出打分，通过强化学习调整 LLM 参数使高奖励回答的出现概率增加，重复这一过程数千次逐步优化模型行为。

RLHF 对 ChatGPT 的成功至关重要。预训练模型只会"续写文本"，不会"回答问题"；SFT 让模型学会基本的对话格式；RLHF 让模型真正学会"对话艺术"——理解上下文、保持连贯、适时追问。更重要的是，RLHF 使模型学会拒绝不当请求（如"我无法回答这个问题"），减少了 80% 以上的有害输出，并引导模型提供详细、有用的回答。

2025 年的技术趋势包括合成数据反馈（使用强模型自动评价弱模型的输出，降低 90% 的人类标注成本）、多维度对齐（不仅对齐内容质量，还考虑安全性、公平性、透明度）、RLOF（Reinforcement Learning from AI Feedback，使用其他 AI 模型的反馈替代人类反馈）。

## 训练成本与挑战
大语言模型训练的成本结构需要认真考虑。算力成本方面，GPT-3 训练约 500 万美元，GPT-4 训练估计 5000 万到 1 亿美元，需要数千张 GPU 运行数月。数据成本方面，SFT 数据标注每小时 20 到 50 美元（专业标注员），RLHF 排序数据每条 0.5 到 2 美元，总数据成本达数十万到数百万美元。技术门槛同样不容忽视——需要大规模分布式训练框架（如 DeepSpeed、Megatron-LM）、复杂的超参数调优（学习率调度、批量大小、温度参数）、处理稳定性问题（训练崩溃、梯度爆炸）。

实践中常见的挑战包括灾难性遗忘（学习新任务时会忘记旧知识，解决方案是经验回放、弹性权重巩固）、奖励黑客（模型学会"欺骗"奖励模型输出看起来好但无意义的内容，解决方案是多样化奖励信号、定期重新训练奖励模型）、对齐税（对齐后模型性能可能下降，解决方案是迭代式对齐在保持能力的同时优化行为）、数据偏见（训练数据反映互联网的偏见和歧视，解决方案是数据清洗、公平性约束、偏见检测）。

## 工程实践
从工程角度，训练大语言模型需要系统性规划。模型架构选择时，Decoder-only 是当前主流，除非有明确的序列到序列任务需求（如机器翻译）才考虑 Encoder-Decoder。预训练阶段，数据质量比数量更重要——高质量的专业数据（论文、代码、教科书）的价值远超低质量的网页文本。建议从小规模实验开始（如 1B 参数模型），验证数据配比和训练流程，再扩展到大规模训练。

SFT 阶段，聚焦高质量指令数据。一个常见误区是认为 SFT 是教知识，实际上是教"怎么回答"——模型的知识来自预训练，SFT 只是教会它如何运用这些知识回答问题。数据标注时，要确保答案的多样性和准确性，避免模式坍塌（模型总是生成相似的回答）。DPO 是 2025 年的首选方案，相比传统 RLHF 更稳定、成本更低、效果接近。

RLHF 阶段，奖励模型的质量决定对齐效果。排序数据要覆盖多样化的场景（安全、帮助性、诚实性），标注员培训至关重要——不同标注员的标准差异会导致奖励模型困惑。训练时要注意 KL 散度约束，防止模型过度优化奖励函数而偏离原始语言模型。PPO 的实现复杂，可以考虑使用开源库如 TRL、Ray RLlib 加速开发。

训练基础设施方面，分布式训练是必备技能。数据并行适合大规模数据训练，模型并行适合超大模型，流水线并行能提高 GPU 利用率。混合精度训练（FP16/BF16）能显著加速，但需要处理数值稳定性问题。FlashAttention 等优化技巧通过优化内存访问模式，在不改变计算结果的前提下显著加速训练。

对于资源受限的团队，基于开源模型进行微调是更现实的选择。LLaMA、Mistral、Qwen 等开源模型提供了强大的基础能力，通过 SFT 和 DPO 进行领域适配，成本远低于从零训练。训练平台如 Hugging Face、Anyscale、Modal 提供了训练基础设施，降低了技术门槛。


# 预训练
预训练是大语言模型能力的基础建设阶段，也是最具技术挑战和资源消耗的环节。这一阶段的目标是让模型通过自监督学习，从海量文本中掌握语言的统计规律、世界知识和推理能力。简单来说，预训练是让模型学会"如何说话"——掌握语法、语义、常识知识，但还不会按照指令回答问题。

## 预训练的基本原理
预训练的核心思想是利用互联网上几乎无限的无标注文本，通过设计合适的自监督任务，让模型学习到通用的语言表示。最常用的任务是"预测下一个 token"（Next Token Prediction）：给定一个文本序列的前面部分，让模型预测接下来的词。例如，看到"人工智能的发展源于"，模型需要预测"1956 年的达特茅斯会议"这样的后续内容。

这种训练方式的巧妙之处在于，训练信号天然存在于文本本身——每个 token 的下一个 token 就是标签，无需人工标注。模型通过数万亿次的预测尝试，逐渐学会了语言的统计规律。更重要的是，这种预测任务迫使模型压缩输入文本中的信息到模型参数中，因为只有理解了上下文，才能准确预测下一个词。

从信息论角度看，预训练实际上是在学习数据的压缩表示。模型参数越多，能够存储的信息量越大，预测能力也越强。这也是为什么模型规模从 GPT-2 的 15 亿参数增长到 GPT-3 的 1750 亿，再到 GPT-4 的万亿级别——规模越大，模型能够学习和记忆的知识越多。

## 数据工程
数据质量比数据数量更重要，这是预训练实践中最重要的经验。原始的互联网文本充满噪声：广告、重复内容、乱码、低质量的 SEO 文章，这些都会严重影响模型性能。数据清洗是第一步，需要过滤掉明显低质量的内容。常用的过滤规则包括：语言检测（去除非目标语言文本）、困惑度过滤（使用小型语言模型识别不自然的文本）、启发式规则（去除过多标点符号或特殊字符的文本）。

数据去重同样关键。互联网上存在大量重复或高度相似的内容，如果不去重，模型会"记住"这些重复出现的文本，导致训练集上的表现虚高，但泛化能力差。实践中使用精确去重（完全相同的文档）和近似去重（使用 MinHash LSH 等算法检测相似文档）相结合，能将数据集规模减少 30-50%，同时提升模型性能。

数据配比是根据目标任务调整不同数据源的比例。代码数据能显著提升模型的推理能力，学术论文增强专业知识，高质量的网页文本提供通识能力，书籍数据提升叙事和长文本理解能力。一个常见的配比是：代码 10%、论文 15%、书籍 20%、网页 55%，具体比例会根据目标任务调整。值得注意的是，代码数据的价值被广泛认可——代码包含严谨的逻辑结构，训练时加入代码能提升模型的数学推理和问题分解能力。

2025 年的一个趋势是合成数据的使用。随着人类高质量文本接近枯竭，研究者开始使用强模型（如 GPT-4）生成训练数据。合成数据的关键是质量控制——使用复杂指令链让模型生成多样化、高质量的文本，然后用弱模型过滤掉可能包含错误的内容。这种方法在代码生成、数学推理等任务上已经取得显著效果。

## 训练基础设施

预训练需要在数千张 GPU 上运行数周甚至数月，这对训练基础设施提出了极高要求。分布式训练框架是基础——DeepSpeed、Megatron-LM、Alpa 等框架处理了模型并行、数据并行、流水线并行的复杂性，让开发者能够像写单机代码一样写分布式训练。

模型并行将模型的不同层分布到不同的 GPU 上，每张 GPU 只存储模型的一部分参数。前向传播时，数据需要在 GPU 之间传递激活值，反向传播时传递梯度。通信开销是主要瓶颈，因此需要仔细设计并行策略——减少跨设备通信、使用计算和通信重叠（在计算的同时进行数据传输）。流水线并行将模型的不同层分组到不同的 GPU 上，每个 GPU 处理一批数据的几层，然后传递给下一组 GPU，能提高 GPU 利用率但增加了实现复杂度。

混合精度训练（FP16/BF16）是标准实践。使用半精度浮点数能减少内存占用、提高计算速度，但需要处理数值稳定性问题——梯度过小会下溢，梯度过大会溢出。常用的解决方案是损失缩放（loss scaling），在反向传播前将损失乘以一个大数，更新参数后再除回来。BF16（Brain Float 16）因为指数位与 FP32 相同，数值范围更大，逐渐成为主流选择。

训练稳定性是另一个挑战。大规模训练经常遇到梯度爆炸（loss 突然变成 NaN）、损失尖峰（loss 短暂飙升后恢复）、权重发散等问题。常用的防护措施包括梯度裁剪（限制梯度最大值）、自适应学习率（AdamW 优化器）、warm-up 调度（训练初期使用较小的学习率）。即使如此，训练崩溃仍时有发生，因此需要定期保存检查点（checkpoint），以便从崩溃点恢复。

## 超参数调优
学习率调度对训练效果影响巨大。预训练通常使用 cosine 学习率衰减——初始学习率经过 warm-up 期逐渐增加到最大值，然后按照余弦曲线衰减到接近零。warm-up 期通常占总训练步数的 5-10%，防止训练初期的大梯度更新破坏预训练权重。最大学习率的选择需要实验确定，太大可能导致训练不稳定，太小则训练速度慢。常用的启发式规则是：批量大小越大，学习率按比例增大（线性缩放规则）。

批量大小影响训练速度和模型质量。大批量训练能提高 GPU 利用率，但可能损害泛化能力。实践中常用"梯度累积"来模拟大批量——每个小批量计算梯度但不立即更新参数，而是累积多个小批量的梯度后再更新。这样既保持了小批量的泛化优势，又获得了大批量的训练速度。常见的配置是每张 GPU 的批量大小为 4-8，累积 8-16 步后再更新。

权重衰减（weight decay）是正则化技术，通过对大权重施加惩罚防止过拟合。AdamW 优化器将权重衰减与自适应学习率解耦，成为预训练的标准选择。权重衰减系数通常设为 0.01-0.1，具体值需要验证集调整。值得注意的是，权重衰减对大模型的影响较小——大模型本身就具有很强的正则化效应（参数量远大于训练数据量）。

训练步数的确定需要平衡计算预算和模型性能。一个有用的经验法则是"训练到过拟合"——持续训练直到验证集损失不再下降甚至开始上升，然后回退到最佳检查点。实践中，预训练通常需要数千亿 token 的训练数据，模型规模越大，需要的数据量越多。Chinchilla 最优规模理论指出，给定计算预算，模型大小和数据量应该按比例增长——不是一味追求大模型，而是平衡模型和数据。

## 涌现能力
当模型规模突破临界点后，会突然展现小模型不具备的能力。这种现象被称为"涌现能力"，包括零样本学习（无需示例即可完成新任务）、上下文学习（从几个示例中学习新任务）、思维链推理（通过分步推理解决复杂问题）。这些能力不是显式训练的，而是从大量训练数据中自发涌现的。

涌现能力的临界点取决于任务难度。简单的文本生成任务（如续写故事）在小模型上就能实现，而复杂的推理任务（如数学证明、代码生成）需要更大的模型。研究表明，涌现能力不是渐进式的，而是"相变"式的——模型规模小于临界点时，能力接近随机；超过临界点后，能力突然提升。这也解释了为什么 GPT-3 的出现如此震撼——它是第一个跨越涌现临界点的模型，展示了令人惊讶的推理和生成能力。

从工程角度看，涌现能力带来了新的挑战。小模型可以预测其行为，而大模型的能力难以在训练前预判。这要求开发者在训练过程中持续评估模型在多样化任务上的表现，而不仅仅是看损失曲线。评估集需要覆盖推理、编程、数学、对话等多种能力，以便及时发现和引导涌现能力的出现。



# 强化学习对齐
监督微调能让模型学会回答问题，但无法保证回答的质量、安全性和帮助性。强化学习对齐（RLHF，Reinforcement Learning from Human Feedback）的目标是让模型学会"如何令人满意"——对齐人类价值观、偏好和伦理标准。这是 ChatGPT 从"能用"到"好用"的关键，也是模型能力的"品格培养"阶段。

## 为什么需要 RLHF
预训练和监督微调后的模型存在明显问题。可能生成有害内容（偏见、歧视或危险信息），会一本正经地胡说八道（幻觉问题），回答过于简略或偏离用户需求，无法理解人类的道德标准和社会规范。这些问题的根源在于，预训练和 SFT 只教会模型"生成文本"，但没有教会模型"生成好的文本"。

"好"是主观的、多维度的。一个回答可能同时涉及准确性（信息是否正确）、帮助性（是否解决了用户问题）、安全性（是否包含有害内容）、诚实性（是否在不知道时承认而非编造）。监督微调可以教会模型基本的格式和边界，但难以优化这些主观质量——标注员很难为每个回答打出一个综合分数，而强化学习通过比较和排序恰好能解决这个问题。

RLHF 的核心思想是用人类偏好训练一个奖励模型，然后用强化学习优化语言模型使高奖励回答的出现概率增加。这种框架将人类的主观判断（哪个回答更好）转化为可优化的数学目标，让模型能够持续改进其行为。

## 奖励模型训练
RLHF 的第一步是训练奖励模型（Reward Model，RM）。奖励模型的目标是模拟人类偏好——给定问题和候选回答，预测人类会给这个回答打多少分。训练数据的收集方式是对同一问题生成多个不同回答（通常是 4-9 个），然后让人类标注员对这些回答进行排序而非打分。排序比打分更可靠——标注员更容易判断"A 比 B 好"，而非给出"A 是 8 分，B 是 6 分"这样的绝对分数。

排序数据需要覆盖多样化的场景。安全性场景包括危险内容（如何制造武器）、歧视性内容（种族、性别偏见）；帮助性场景包括回答完整性（是否解决了问题）、回答深度（是详细解释还是敷衍了事）；诚实性场景包括事实准确性（是否编造信息）、不确定性表达（不知道时是否承认）。每个维度都需要足够的样例，否则奖励模型会过度优化某些维度而忽略其他。

标注员培训是奖励模型成功的关键。不同标注员的标准差异会导致数据不一致，进而导致奖励模型困惑。需要制定明确的标注指南——什么样的回答是有帮助的、什么样的回答是安全的、如何处理模棱两可的情况。定期进行标注员一致性测试，确保标注质量。实践中，OpenAI 等公司投入了大量资源在标注流程上，这是 RLHF 成功的隐形成本。

奖励模型本身是一个小型语言模型（通常是几百 million 参数），在排序数据上训练。损失函数使用排名损失（Ranking Loss）——对于排序对（回答 A > 回答 B），奖励模型应该输出 RM(A) > RM(B)。训练好的奖励模型可以快速给新的回答打分，作为强化学习的优化目标。

## 强化学习优化
第二步是使用奖励模型优化语言模型。常用的算法是 PPO（Proximal Policy Optimization），一种策略梯度强化学习算法。语言模型被视为策略（policy），它根据输入生成回答；奖励模型提供奖励信号（reward）；PPO 算法更新策略参数使高奖励回答的出现概率增加。

PPO 的核心思想是"保守更新"——在优化奖励的同时，防止策略变化过大导致训练崩溃。具体实现上，PPO 使用重要性采样（importance sampling）和裁剪（clipping）技术，限制每步更新时策略的变化幅度。这看似简单，但对训练稳定性至关重要——早期的强化学习算法（如 REINFORCE）在语言模型上经常崩溃，而 PPO 的出现使得 RLHF 变得实用。

训练过程中需要注意几个关键点。首先是 KL 散度约束——防止模型过度优化奖励函数而偏离原始语言模型。KL 散度衡量两个概率分布的差异，约束过大则模型改进有限，约束过小则可能破坏预训练学到的通用能力。实践中通常将 KL 惩罚系数设为 0.1-0.2，需要根据验证集调整。

其次是奖励模型的准确性。奖励模型本身是在人类排序数据上训练的，如果排序数据有偏差或覆盖不足，奖励模型会学到错误的偏好。强化学习会利用这些错误——模型可能学会"欺骗"奖励模型（生成看起来好但无意义的内容），这种现象被称为"奖励黑客"（reward hacking）。解决方案是定期重新训练奖励模型、多样化排序数据、引入额外的安全约束。

PPO 的实现复杂度高。需要同时运行四个模型：语言模型（策略）、参考语言模型（计算 KL 散度）、奖励模型（打分）、价值函数（估计未来奖励的折现和）。每个前向传播需要多次模型调用，计算开销巨大。实践中，通常使用开源库如 TRL、Ray RLlib 加速开发，但调优仍需要大量经验。

## DPO：简化的替代方案
DPO（Direct Preference Optimization，直接偏好优化）是 2024 年兴起的替代方案，彻底改变了 RLHF 的格局。DPO 的洞察是：奖励模型只是中间步骤，真正需要的是语言模型参数。通过数学推导，DPO 证明了可以直接使用偏好对（好答案 vs 坏答案）优化语言模型，无需显式训练奖励模型。

从技术角度看，DPO 将 RLHF 的两阶段流程（先训练奖励模型，再用 RL 优化）简化为单阶段优化。损失函数直接作用于偏好对，数学上等价于 RLHF 的静态版本（奖励模型固定不变）。这个简化的好处巨大——实现简单、训练稳定、无需调优 KL 散度、不需要价值函数。

DPO 的优势在工程实践中明显。训练速度更快（无需四个模型同时运行）、显存占用更低（单个模型）、超参数更少（学习率是主要需要调的参数）、效果接近甚至超过传统 RLHF。对于资源受限的团队，DPO 使得对齐训练变得可行——单张消费级显卡就可以进行 DPO 训练。

当然，DPO 并非万能。对于需要长期规划的任务（如多轮对话、复杂推理），传统 RLHF 的价值函数估计仍有价值。但对于大多数文本生成任务，DPO 已经成为 2025 年的首选方案。

## 工程实践
RLHF/DPO 的数据质量决定效果。排序/偏好数据要覆盖多样化的场景，安全性、帮助性、诚实性三个维度都要有足够的样例。一个常见误区是只关注安全（避免有害内容），而忽略帮助性（模型变得过于谨慎，回答过于简略）。平衡是关键——既要安全，又要有帮助。

标注成本是 RLHF 的主要障碍。人工排序每条成本 0.5-2 美元，10 万条数据的成本就是 5-20 万美元。2025 年的趋势是使用 AI 辅助标注——用强模型（如 GPT-4）为弱模型的回答打分或排序，人工只校验部分样本。RLOF（Reinforcement Learning from AI Feedback）更进一步，完全使用 AI 模型的反馈替代人类反馈，能降低 90% 的标注成本。

训练稳定性需要持续监控。除了奖励曲线，还要监控 KL 散度（是否偏离原始模型太远）、回答长度（是否学会用长回答欺骗奖励模型）、多样性（是否总是生成相似的回答）。设置自动化脚本，在异常情况时暂停训练，能避免浪费计算资源。

评估对齐效果时，人工评估是必需的。自动指标（如奖励分数）只能作为参考，真正重要的是人类对模型输出的主观感受。建立评估集，定期评估模型在安全性、帮助性、诚实性上的表现，防止训练过程中的退化。

对于大多数团队，建议从 DPO 开始。实现简单、训练稳定、成本低，效果接近传统 RLHF。只有在 DPO 无法满足需求（如需要复杂的多步决策）时，才考虑完整的 PPO 流程。开源库如 TRL、llama-factory 都提供了 DPO 的实现，可以直接使用。

最后，对齐不是一次性的任务，而是持续的过程。模型部署后，用户反馈是宝贵的数据来源——收集用户的点赞/点踩、问题报告，用这些数据持续优化模型。建立反馈-重训-部署的闭环，让模型能够持续改进。
