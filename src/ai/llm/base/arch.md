---
title: 架构
---

# 大模型架构

## 模型架构基础
在深入训练流程之前，需要理解两大主流架构的本质差异。GPT 系列采用 Decoder-only 架构，仅使用 Transformer 的解码器部分，配合因果注意力机制进行自回归训练——模型只能看到当前位置之前的 token，通过预测下一个 token 逐字生成文本。这种架构天然适合文本生成任务，能够保持上下文连贯性，从 GPT-3 到 GPT-4、Claude、LLaMA 都采用了这一设计。

BERT 系列则采用 Encoder-only 架构，使用双向注意力机制，模型能够同时看到上下文的所有 token。训练方式是掩码语言模型（MLM），随机遮盖部分 token 让模型预测，类似于"完形填空"。这种架构擅长理解类任务如文本分类、情感分析、问答匹配，但随着生成任务的重要性提升，GPT 架构已成为主流。值得注意的是，T5、BART 等模型采用 Encoder-Decoder 架构，结合了双向理解和生成能力，在机器翻译等序列到序列任务上仍有价值。

从工程实践看，Decoder-only 架构的优势不仅在于生成能力，更在于工程实现的简洁性——单一的前向传播路径，更容易进行大规模分布式训练和推理优化。这也是为什么现代大语言模型几乎都采用这一架构。
