---
title: GPT
order: 0
---

# GPT 模型
GPT 模型的训练是当今大语言模型的开端，其训练不是一蹴而就的，而是通过**预训练、监督微调、强化学习**三个递进阶段，将一个通用的语言模型逐步打造成能够理解指令、遵循规则、与人类价值观对齐的智能助手。这一范式被当作后来 LLM 的"标准训练流程"。

## 模型架构基础
在深入训练流程之前，需要理解两大主流架构的本质差异。GPT 系列采用 Decoder-only 架构，仅使用 Transformer 的解码器部分，配合因果注意力机制进行自回归训练——模型只能看到当前位置之前的 token，通过预测下一个 token 逐字生成文本。这种架构天然适合文本生成任务，能够保持上下文连贯性，从 GPT-3 到 GPT-4、Claude、LLaMA 都采用了这一设计。

BERT 系列则采用 Encoder-only 架构，使用双向注意力机制，模型能够同时看到上下文的所有 token。训练方式是掩码语言模型（MLM），随机遮盖部分 token 让模型预测，类似于"完形填空"。这种架构擅长理解类任务如文本分类、情感分析、问答匹配，但随着生成任务的重要性提升，GPT 架构已成为主流。值得注意的是，T5、BART 等模型采用 Encoder-Decoder 架构，结合了双向理解和生成能力，在机器翻译等序列到序列任务上仍有价值。

从工程实践看，Decoder-only 架构的优势不仅在于生成能力，更在于工程实现的简洁性——单一的前向传播路径，更容易进行大规模分布式训练和推理优化。这也是为什么现代大语言模型几乎都采用这一架构。

## 第一阶段：预训练
预训练的目标是让模型学会"如何说话"，即掌握语言的统计规律、世界知识和推理能力。这是模型能力的基础建设阶段，也是最消耗资源阶段。训练方法采用自监督学习，无需人工标注，直接从海量文本中学习预测下一个 token。

训练数据规模庞大，通常包含整个互联网的文本（网页、书籍、论文、代码），规模可达数十 TB。模型通过数万亿次的预测尝试，逐渐学会了语言规律（语法、语义）、常识知识（物理、历史、数学）和逻辑推理能力。GPT-3 使用了 3000+ 张 A100 GPU 运行数周，成本约 500 万美元，而 GPT-4 的训练成本估计在 5000 万到 1 亿美元之间。

数据工程是预训练成功的关键。原始数据需要经过严格清洗——去除低质量文本（广告、重复内容、乱码）、数据去重（防止模型"背答案"）、数据配比（平衡不同领域数据）。代码数据能提升模型的推理能力，论文数据增强专业知识，网页数据提供通识能力。实践中，一个常见的数据配比是代码占 10%、论文占 15%、网页占 75%，具体比例会根据目标任务调整。

当模型规模突破临界点（约 100 亿参数）后，会突然展现小模型不具备的能力，如零样本学习、链式推理等。这种现象被称为"涌现能力"，也是大语言模型区别于传统模型的核心特征。2025 年的技术趋势包括合成数据（使用 AI 生成高质量训练数据）、长上下文（上下文窗口从 4K 扩展到 1M+ token）、多模态预训练（从纯文本扩展到图像、视频、音频的联合训练）。

## 第二阶段：监督微调
监督微调（SFT）的目标是让模型学会"如何听话"，即理解人类指令、遵循任务要求、生成符合期望的输出格式。这是模型能力的"职业培训"阶段，将预训练获得的通用能力转化为实际任务能力。

训练方法采用监督学习，使用人工标注的高质量问答对进行训练。数据准备是关键环节——指令构造需要覆盖多样化任务（写作、编程、翻译、数学、问答），答案撰写由人类专家编写标准答案确保质量，数据规模通常为 1 万到 10 万条。与预训练的海量数据相比，SFT 数据量虽小但质量要求极高，一条高质量标注数据的价值远超一千条原始文本。

SFT 的核心作用是将"续写文本"能力转化为"回答问题"能力。预训练模型看到"今天天气"会继续生成"真不错"这样的续写，而经过 SFT 的模型能理解"今天天气怎么样"是一个需要回答的问题，并生成"今天天气晴朗，温度 25 度"这样的响应。此外，SFT 还让模型学习遵循格式要求（如 JSON 输出、代码块标记）、理解角色设定（如"你是一个专业的程序员助手"）、学习拒绝回答有害问题（如制造武器的方法）。

2024 年兴起的 DPO（Direct Preference Optimization，直接偏好优化）技术，通过比较"好答案"和"坏答案"直接优化模型，取代了部分复杂的 RLHF 流程，大幅降低训练成本。这已成为 2025 年的主流对齐方式。自动化标注也成为趋势——使用强模型（如 GPT-4）为弱模型生成训练数据，既降低成本又能保证质量。

## 第三阶段：强化学习
强化学习（RLHF）的目标是让模型学会"如何令人满意"，即对齐人类价值观、偏好和伦理标准。这是模型能力的"品格培养"阶段，也是让 ChatGPT 从"能用"到"好用"的关键。

预训练和 SFT 阶段的模型存在明显问题：可能生成有害内容（偏见、歧视或危险信息）、会一本正经地胡说八道（幻觉问题）、回答过于简略或偏离用户需求、无法理解人类的道德标准和社会规范。RLHF 通过让人类对模型输出进行评价，引导模型产生更符合人类期望的回复。

训练流程分为两个步骤。第一步是收集人类反馈训练奖励模型——让模型对同一问题生成多个不同回答（如 4 个），人类标注员对这些回答进行排序，然后训练一个"奖励模型"来模拟人类偏好。第二步是使用 PPO 算法（Proximal Policy Optimization）进行强化学习优化——奖励模型给 LLM 的输出打分，通过强化学习调整 LLM 参数使高奖励回答的出现概率增加，重复这一过程数千次逐步优化模型行为。

RLHF 对 ChatGPT 的成功至关重要。预训练模型只会"续写文本"，不会"回答问题"；SFT 让模型学会基本的对话格式；RLHF 让模型真正学会"对话艺术"——理解上下文、保持连贯、适时追问。更重要的是，RLHF 使模型学会拒绝不当请求（如"我无法回答这个问题"），减少了 80% 以上的有害输出，并引导模型提供详细、有用的回答。

2025 年的技术趋势包括合成数据反馈（使用强模型自动评价弱模型的输出，降低 90% 的人类标注成本）、多维度对齐（不仅对齐内容质量，还考虑安全性、公平性、透明度）、RLOF（Reinforcement Learning from AI Feedback，使用其他 AI 模型的反馈替代人类反馈）。

## 训练成本与挑战
大语言模型训练的成本结构需要认真考虑。算力成本方面，GPT-3 训练约 500 万美元，GPT-4 训练估计 5000 万到 1 亿美元，需要数千张 GPU 运行数月。数据成本方面，SFT 数据标注每小时 20 到 50 美元（专业标注员），RLHF 排序数据每条 0.5 到 2 美元，总数据成本达数十万到数百万美元。技术门槛同样不容忽视——需要大规模分布式训练框架（如 DeepSpeed、Megatron-LM）、复杂的超参数调优（学习率调度、批量大小、温度参数）、处理稳定性问题（训练崩溃、梯度爆炸）。

实践中常见的挑战包括灾难性遗忘（学习新任务时会忘记旧知识，解决方案是经验回放、弹性权重巩固）、奖励黑客（模型学会"欺骗"奖励模型输出看起来好但无意义的内容，解决方案是多样化奖励信号、定期重新训练奖励模型）、对齐税（对齐后模型性能可能下降，解决方案是迭代式对齐在保持能力的同时优化行为）、数据偏见（训练数据反映互联网的偏见和歧视，解决方案是数据清洗、公平性约束、偏见检测）。

## 工程实践
从工程角度，训练大语言模型需要系统性规划。模型架构选择时，Decoder-only 是当前主流，除非有明确的序列到序列任务需求（如机器翻译）才考虑 Encoder-Decoder。预训练阶段，数据质量比数量更重要——高质量的专业数据（论文、代码、教科书）的价值远超低质量的网页文本。建议从小规模实验开始（如 1B 参数模型），验证数据配比和训练流程，再扩展到大规模训练。

SFT 阶段，聚焦高质量指令数据。一个常见误区是认为 SFT 是教知识，实际上是教"怎么回答"——模型的知识来自预训练，SFT 只是教会它如何运用这些知识回答问题。数据标注时，要确保答案的多样性和准确性，避免模式坍塌（模型总是生成相似的回答）。DPO 是 2025 年的首选方案，相比传统 RLHF 更稳定、成本更低、效果接近。

RLHF 阶段，奖励模型的质量决定对齐效果。排序数据要覆盖多样化的场景（安全、帮助性、诚实性），标注员培训至关重要——不同标注员的标准差异会导致奖励模型困惑。训练时要注意 KL 散度约束，防止模型过度优化奖励函数而偏离原始语言模型。PPO 的实现复杂，可以考虑使用开源库如 TRL、Ray RLlib 加速开发。

训练基础设施方面，分布式训练是必备技能。数据并行适合大规模数据训练，模型并行适合超大模型，流水线并行能提高 GPU 利用率。混合精度训练（FP16/BF16）能显著加速，但需要处理数值稳定性问题。FlashAttention 等优化技巧通过优化内存访问模式，在不改变计算结果的前提下显著加速训练。

对于资源受限的团队，基于开源模型进行微调是更现实的选择。LLaMA、Mistral、Qwen 等开源模型提供了强大的基础能力，通过 SFT 和 DPO 进行领域适配，成本远低于从零训练。训练平台如 Hugging Face、Anyscale、Modal 提供了训练基础设施，降低了技术门槛。
