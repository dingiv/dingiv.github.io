# Transformer
在深度学习序列建模的发展历程中，Transformer 的出现是一个转折点。2017 年 Google 发表《Attention is All You Need》时，很多人并未意识到这将成为现代大语言模型（LLM）的基石架构。回看 RNN/LSTM 时代的痛点——串行计算导致的训练效率低下、长距离依赖的梯度消失问题，Transformer 用一个优雅的设计彻底解决了这些难题：完全抛弃循环结构，让序列中每个位置都能直接"看到"其他所有位置。

## 从 seq2seq 到注意力机制

Transformer 的思想渊源可以追溯到 seq2seq 模型。早期的序列到序列任务（如机器翻译）使用编码器-解码器架构：编码器将输入序列压缩成一个固定长度的向量，解码器从中生成输出序列。但这种设计存在明显问题——无论输入多长，最终都被压缩成一个向量，信息瓶颈严重。

注意力机制的引入缓解了这个问题。解码器在生成每个词时，不再只依赖固定的上下文向量，而是可以"关注"输入序列的不同位置。这让人联想到人类阅读时的做法：理解一个复杂句子时，我们会反复回看前后相关的词，而不是一次性记下所有信息。

但那时的注意力机制仍建立在 RNN 之上，计算效率问题并未解决。Transformer 的核心洞察是：既然注意力机制如此有效，为什么不直接用它来建模序列关系，而保留 RNN 的递归结构呢？

## 整体架构设计

Transformer 采用编码器-解码器架构，输入序列经编码器处理为语义表示，再由解码器生成输出。编码器和解码器各由 6 层（原论文配置）相同的模块堆叠而成，每层包含三个核心组件：多头自注意力层、前馈全连接层（FFN）、残差连接与层归一化。

编码器的任务是理解输入序列。每层的多头自注意力让序列中每个位置都能与其他所有位置交互，捕捉词之间的依赖关系；前馈网络则负责对每个位置的表示进行非线性变换，增强表达能力。解码器除了自注意力外，还有一个编码器-解码器注意力层（Cross-Attention），用于在生成时关注输入序列的相关部分。

训练时，解码器需要使用掩码机制防止"偷看"未来内容。这种因果性约束使得模型在实际推理时能够自回归地生成序列。

![](./transformer_arch.png)

## 自注意力机制

### 数学原理

自注意力是 Transformer 的核心创新。给定输入词向量矩阵 X（n 个词，每个词 d 维），通过三个可学习的投影矩阵生成 Query、Key、Value：

$$Q = XW^Q,\quad K = XW^K,\quad V = XW^V$$

注意力计算的核心公式是：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

这个公式的直观理解是：QK^T 计算每对词之间的相似度，得到注意力分数矩阵；除以 $\sqrt{d_k}$ 进行缩放，防止点积过大导致 softmax 进入饱和区；softmax 将分数归一化为概率分布；最后用这个分布对 V 加权求和。

### Q、K、V 的类比

可以把 Q、K、V 理解为检索系统中的三个角色：Query 是"我想找什么"，Key 是"有什么标签可以匹配"，Value 是"实际内容"。每个词同时扮演三种角色——既发出查询，也作为被查询的目标，最终得到的是所有词对其的"关注程度"加权后的信息聚合。

举例来说，处理句子 "The cat sat on the mat" 时，"sat" 这个词的 Query 会与所有词的 Key 计算相似度。由于主谓关系，"cat" 会得到较高权重；由于动宾关系，"mat" 也会有显著权重。这样 "sat" 的最终表示就融合了主语和宾语的信息。

### 缩放因子的必要性

缩放因子 $\sqrt{d_k}$ 经常被忽略，但对训练稳定性至关重要。当 d_k 较大（如 512）时，点积的方差会达到 512，导致 softmax 函数的输入值过大，梯度接近零。除以 $\sqrt{d_k}$ 将方差归一化为 1，确保梯度不会消失。在实践中，这个细节决定了模型能否正常训练。

## 多头注意力

单个注意力头只能捕捉一种类型的关系。自然语言中的依赖是多样的：语法关系（主谓一致）、语义关系（近义、反义）、共指关系（代词指代实体）等。多头注意力通过多组独立的 Q、K、V 投影，让不同的头专注于不同的模式。

原论文使用 8 个头，每头维度 64（总维度 512）。各头计算完成后，拼接起来再经过一个线性变换融合。这种设计类似于计算机视觉中的多通道卷积核，每个通道学习不同的特征模式。

从工程实践来看，头的数量和维度之间存在权衡。头数过多会增加参数量和计算开销，但能捕捉更细粒度的模式；头数过少则可能损失表达能力。现代模型通常将头数设为 8-32，具体取决于模型规模。

## 位置编码

自注意力机制本身是顺序不变的——打乱词序后，注意力计算的统计特性不变。这对 NLP 来说是个问题，因为语序显然包含重要信息。Transformer 通过位置编码注入位置信息。

原论文采用正弦-余弦编码方案：偶数维用 $\sin(pos/10000^{2i/d})$，奇数维用 $\cos(pos/10000^{2i/d})$。这种设计的巧妙之处在于不同频率对应不同粒度的位置信息（高频关注相邻位置，低频关注全局位置），且可以通过线性变换表示相对位置关系。更重要的是，这种固定编码可以外推到训练时未见过的序列长度。

后来的工作提出了多种改进方案。BERT 使用可学习的位置嵌入，简单直接但缺乏外推能力；T5 使用相对位置编码，显式建模位置间的相对距离；RoPE（旋转位置编码）通过旋转变换将位置信息注入 Q 和 K，在长文本场景下表现优异，被 LLaMA 等模型采用。

从工程角度看，位置编码的选择影响训练稳定性和长文本处理能力。固定编码收敛更快，可学习编码更灵活但需要更多调参；相对位置编码在处理超长文本时更有优势。

## 前馈网络与残差连接
每个 Transformer 层除了注意力，还有一个位置无关的前馈网络：两层全连接，中间用 ReLU 或 GELU 激活，维度通常从 512 扩展到 2048 再压缩回 512。这种"扩展-压缩"结构增加了模型的非线性表达能力。

注意力负责"信息交互"（跨位置聚合），FFN 负责"信息变换"（逐位置非线性映射）。两者配合，使得 Transformer 既能捕捉长距离依赖，又能学习复杂的特征变换。

每个子层后都有残差连接和层归一化：$\text{LayerNorm}(x + \text{Sublayer}(x))$。残差连接缓解梯度消失，允许信息直接流动；层归一化稳定训练、加速收敛。关于 LayerNorm 的位置（Pre-Norm vs Post-Norm），后来的研究发现 Pre-Norm（先归一化再进入子层）更适合深层网络，这成为现代 LLM 的标准配置。

## 架构变体与应用

Transformer 的灵活性催生了三种主要架构范式。仅编码器架构以 BERT 为代表，使用掩码语言模型预训练（随机遮盖 15% 的词让其预测），擅长理解类任务如文本分类、命名实体识别、问答。仅解码器架构以 GPT 系列为代表，采用自回归语言模型训练（预测下一个词），适合生成类任务如文本生成、对话、代码创作。编码器-解码器架构如 T5、BART，完整保留了原始结构，在机器翻译、文本摘要等序列到序列任务上表现出色。

这个分类在实践中很重要。选择架构时要考虑任务特性：需要双向上下文的选 BERT 类，需要生成能力的选 GPT 类，输入输出不对称的选 Encoder-Decoder。值得注意的是，现代大语言模型（GPT-4、Claude、LLaMA）几乎都采用 Decoder-Only 架构——这种简化在大规模预训练下反而效果更好，且工程实现更简洁。

## 工程实践中的考量

Transformer 的 $O(n^2)$ 复杂度（n 是序列长度）是主要瓶颈。处理长文本时，注意力矩阵的内存占用和计算量迅速增长。工程上常用的优化包括：稀疏注意力（只关注部分位置，如 Longformer）、线性近似（Performer 使用核方法降低复杂度）、分层处理（先局部注意力再全局聚合）。此外，FlashAttention 等工程技巧通过优化内存访问模式，在不改变计算结果的前提下显著加速训练。

位置编码的选择也影响工程实践。固定编码无需额外参数，但可学习编码在某些任务上效果更好。相对位置编码处理变长序列更鲁棒，但实现复杂度更高。RoPE 在长文本场景下表现优异，是目前的主流选择。

训练稳定性是另一个关键点。Transformer 对超参数较敏感，学习率 warm-up、梯度裁剪、LayerNorm epsilon 等细节都需要精心调优。混合精度训练（FP16/BF16）能显著加速，但需要处理数值稳定性问题；分布式训练时，张量并行、流水线并行、数据并行的组合使用是大规模训练的必备技能。

## 与 RNN/LSTM 的对比

从实际应用角度总结三种架构的差异：RNN 早已被淘汰，其串行计算和梯度消失问题在工程上无法接受；LSTM 在某些对延迟敏感、序列长度可控的场景下仍有价值（如实时语音识别），但新项目基本不会采用；Transformer 已成为标准架构，尽管有 $O(n^2)$ 的复杂度缺陷，但其并行化能力和可扩展性使其在 GPU/TPU 上的实际速度远快于 RNN 类模型。

特别值得注意的是，Transformer 的可扩展性催生了"规模即能力"的发现——通过增加参数量、数据量、计算量，模型会涌现出预训练时未明确教给它的能力（如上下文学习、思维链推理）。这既是技术突破，也带来了新的工程挑战：如何高效训练超大规模模型、如何评估模型能力、如何确保安全性。

## 实践建议

学习 Transformer 时，建议从简化版的单头注意力开始，理解 Q、K、V 的计算流程和形状变换（batch、seq_len、head_dim 维度的 permute 和 reshape）。然后实现多头注意力、位置编码、前馈网络，组装成完整的 Transformer Block。在此基础上，可以尝试复现简化版的 GPT-2：仅解码器架构，因果掩码，语言模型训练。

调试时，注意力权重可视化是重要工具。通过观察模型学到的注意力模式，可以判断训练是否正常（如是否学到语法关系）。梯度检查（gradient checking）在实现自定义层时很有帮助，能及早发现数值计算错误。

从研究趋势看，Transformer 仍在演进。Mixture-of-Experts（MoE）架构通过稀疏激活降低计算量（如 Mixtral 8x7B），状态空间模型（如 Mamba）试图在保持线性复杂度的同时逼近 Transformer 的表达能力，长上下文技术（如 Ring Attention、滑动窗口）将有效上下文扩展到百万 token 级别。理解 Transformer 的设计思想，是跟进这些前沿工作的基础。
