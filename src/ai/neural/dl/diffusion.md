---
title: Diffusion
---

# Diffusion
扩散模型（Diffusion Model）是近年来生成式 AI 领域最重要的突破之一。从 2020 年 DDPM 的提出，到 Stable Diffusion 的开源，再到 DALL-E 3、Sora 的惊艳表现，扩散模型已经彻底改变了图像和视频生成的方式。

对于 IT 开发者来说，理解扩散模型的关键在于掌握其核心思想：通过学习逐步去噪的过程来生成数据，这本质上是对数据分布的一种逆向建模。

## 核心原理概述

扩散模型灵感来源于非平衡热力学。前向扩散过程逐步向数据中添加高斯噪声，直到数据变成纯随机噪声；逆向去噪过程学习从噪声中逐步恢复原始数据。模型只需要学习每一步的去噪预测，而不需要直接建模整个数据分布。

数学上，前向过程通过 q(x_t|x_{t-1}) = N(x_t; √(1-β_t)x_{t-1}, β_t I) 逐步添加噪声。经过 T 步（通常 1000 步）后，数据近似服从标准高斯分布。逆向过程 p_θ(x_{t-1}|x_t) 由神经网络参数化，学习预测添加的噪声或直接预测去噪后的图像。

在实际推理中，从一个随机噪声 x_T 开始，通过 T 步去噪逐步得到 x_0。这个过程可以用加速采样技术（如 DDIM）大幅减少步数，从 1000 步压缩到 50 步甚至更少。

## 早期架构：CNN 为主导

### DDPM 与 U-Net 骨干

DDPM（Denoising Diffusion Probabilistic Models）于 2020 年提出，奠定了现代扩散模型的基础。其核心网络架构是 U-Net，这是一种经典的 encoder-decoder 架构，最初用于医学图像分割。U-Net 的名称来源于其对称的 U 形结构：左侧编码器逐步下采样提取特征，右侧解码器逐步上采样恢复分辨率，中间通过跳跃连接（skip connection）融合浅层和深层特征。

在扩散模型中，U-Net 的输入是带有噪声的图像 x_t 和时间步 t，输出是预测的噪声或去噪后的图像。时间步通过 sinusoidal position embedding 编码后，注入到网络的每一层。这种设计使得同一个网络能够处理不同噪声水平的图像，学习每一步的去噪策略。

从工程实现角度看，U-Net 中的卷积层大量使用 ResNet 块，每个块包含两个卷积层和残差连接。这种设计保证了深层网络的梯度传播稳定性。注意力机制被嵌入到网络的中间层，通常是瓶颈层，用于捕获全局依赖关系。

### 条件生成与 Cross-Attention

无条件扩散模型只能生成随机样本，无法控制生成内容。条件扩散模型通过引入额外的条件信息（如文本描述、类别标签、参考图像）来控制生成过程。最关键的机制是 cross-attention，它将条件信息注入到去噪网络中。

Cross-attention 来源于 Transformer 架构，但其应用场景不仅限于序列到序列任务。在扩散模型中，图像特征作为 query，条件编码（如 CLIP 文本特征）作为 key 和 value，通过注意力机制实现图像与条件的交互。具体来说，从 U-Net 的特征图中生成 query，从条件编码器中提取 key 和 value，计算注意力后加权得到条件化特征。

这种设计的优势在于灵活性。条件信息可以是文本、图像、深度图、边缘图等各种形式，只要能编码为特征向量就能通过 cross-attention 注入。GLIDE（DALL-E 2 的前身）是第一个成功应用 cross-attention 的文本到图像扩散模型，它证明了这种机制在复杂条件生成任务中的有效性。

### Latent Diffusion 的突破

Stable Diffusion 的核心创新是 Latent Diffusion。直接在像素空间（如 512×512×3）运行扩散模型计算开销巨大，Latent Diffusion 首先使用编码器将图像压缩到低维潜在空间（如 64×64×4），在潜在空间进行扩散过程，然后用解码器恢复图像。

这种设计的优势是显著的。潜在空间的维度是像素空间的 1/8（每边压缩 4 倍），计算量大幅降低，使得在消费级 GPU 上运行成为可能。更重要的是，潜在空间保留了图像的语义信息，去除了高频冗余，模型更容易学习有效的特征表示。

Stable Diffusion 的去噪网络仍然是基于 U-Net 的，但全部使用 cross-attention 来注入文本条件。它使用预训练的 CLIP 文本编码器将文本描述转换为特征序列，然后通过多层 cross-attention 将这些特征注入到 U-Net 的不同分辨率层。这种多尺度条件注入使得模型能够同时理解局部细节和全局语义。

从工程部署角度看，Latent Diffusion 的推理流程包括三个阶段：文本编码（CLIP）、扩散去噪（U-Net）、图像解码（VAE decoder）。其中去噪阶段是计算瓶颈，需要迭代多次。实际部署时可以通过量化、模型剪枝、算子融合等优化技术来加速推理。

## Transformer 化浪潮

### 架构演进的必然性

扩散模型的 Transformer 化与 ViT 的发展路径类似。CNN 架构虽然有效，但存在感受野受限、长距离依赖捕获能力弱的问题。在图像生成任务中，全局一致性非常重要（如保持人物的视觉连贯性），而 CNN 的局部感受野可能无法充分捕获这种全局语义。

Transformer 的全局注意力机制天然适合这种场景。DiT（Diffusion Transformer）将 U-Net 替换为标准的 Transformer 架构，首先通过 patch embedding 将潜在特征图切分为 patch 序列，然后通过标准的 Transformer Block 进行处理，最后通过 patch merging 恢复特征图形状。这种设计与 ViT 高度相似，证明了好架构的通用性。

### DiT 架构详解

DiT 的核心创新是将扩散过程的时间步和条件信息通过 adaptive layer norm（或类似机制）注入到 Transformer Block 中。在每个 Block 内部，时间步和条件首先被编码为尺度参数 γ 和偏移参数 β，然后对 patch 特征进行归一化：x = (x - mean(x)) / std(x) * γ + β。这种机制被称为 conditioner injection，比 cross-attention 更加高效。

DiT 的另一个关键设计是 block-wise 处理。不同分辨率（如 32×32、16×16、8×8）的特征图被独立处理，然后通过上采样或下采样连接。这种分块处理使得模型能够同时捕获多尺度信息，类似于 CNN 的金字塔结构，但每个块内部使用的是全局注意力。

从参数效率看，DiT 在相同参数量下优于基于 CNN 的扩散模型。实验表明，DiT-XL/2 模型在 ImageNet 生成任务上达到了当时的最优 FID 分数，证明了 Transformer 架构在生成任务上的潜力。

### 视频生成的 Transformer 化

视频生成是扩散模型 Transformer 化的重要驱动力。视频不仅有空间维度（图像），还有时间维度（帧间运动）。CNN 难以有效建模长距离时序依赖，而 Transformer 的注意力机制可以自然地处理时空关系。

Sora 是 OpenAI 推出的视频生成模型，采用了 DiT 架构的扩展版本。它将视频帧沿时间和空间维度切分为 patch（如 16×16×16 的时空块），然后通过 Transformer 处理 patch 序列。时空注意力机制使得模型能够同时捕获空间纹理和时序运动，生成高质量、高一致性的视频片段。

视频生成的挑战在于计算复杂度。一个 1 分钟、30 fps、512×512 的视频包含 1080 个帧，如果每个帧被切成 256 个 patch，序列长度将达到 27 万以上，远超标准 Transformer 的处理能力。实际的解决方案包括使用 patch embedding 降维、限制注意力范围、使用稀疏注意力模式等。

## 工程实践考量

### 训练稳定性与技巧

扩散模型的训练相比判别式模型更加困难。首先需要仔细设计噪声调度（noise schedule），控制每一步添加的噪声量。DDPM 使用线性调度，后续工作发现余弦调度（cosine schedule）效果更好。调度策略直接影响生成质量和采样速度。

另一个关键是损失函数的选择。最常用的是预测噪声的 MSE loss：L = ||ε - ε_θ(√ᾱ_t x_0 + √(1-ᾱ_t)ε, t)||²。但也可以预测 x_0 或速度 v（v-parameterization）。不同的预测目标会影响训练稳定性和生成效果，实践中需要根据任务选择。

条件生成的训练需要注意条件泄露问题。如果模型在训练时能够"偷看"目标条件，可能学不到有效的条件-图像映射。解决方案包括使用 classifier-free guidance，在训练时随机丢弃条件信息，推理时通过插值控制条件强度。

### 推理优化与加速

扩散模型的推理速度是主要瓶颈。标准的 DDPM 需要 1000 步去噪，即使使用 DDIM 加速到 50 步，仍然比 GAN 慢一个数量级。实际部署时需要综合考虑采样质量和速度。

常见的加速方法包括：使用更少步数的采样器（如 DPM-Solver）、知识蒸馏将教师模型的高质量采样迁移到学生模型的快速采样、使用 LoRA 或 ControlNet 等适配器重用预训练权重。对于实时生成任务，可以考虑单步生成模型（如 consistency distillation）。

模型量化是另一个重要方向。扩散模型的去噪网络对精度不敏感，可以安全地将 float32 量化为 int8 甚至 int4，在几乎不损失生成质量的情况下大幅减少显存占用和提高推理速度。Stable Diffusion 的社区版本广泛使用了 8-bit 量化来在消费级 GPU 上运行。

### 多模态应用

扩散模型的成功不仅体现在图像生成，还拓展到视频生成、3D 生成、音频生成等多个领域。这些应用的核心思想一致：将数据表示为序列（图像 patch、视频帧、3D 体素、音频波形），然后通过去噪学习生成过程。

文本到视频生成是最活跃的方向之一。模型首先将文本描述编码为条件特征，然后生成符合描述的视频帧序列。关键挑战是保持帧间一致性，避免闪烁或跳跃。解决方案包括使用时序注意力、参考帧注入、运动模块等技术。

3D 生成通常采用 2D 先验提升的方法。先用扩散模型生成多视角图像，然后通过神经渲染或显式重建（如 NeRF、3D Gaussian Splatting）恢复 3D 结构。这种方法绕过了直接在 3D 空间训练的高昂计算成本，同时保证了生成质量。

## 学习路径

入门建议先理解扩散模型的数学基础，特别是前向-逆向过程的推导和变分下界（ELBO）的概念。然后可以使用 PyTorch 实现一个简单的 DDPM 模型，在 MNIST 或 CIFAR-10 数据集上验证效果。

论文推荐阅读《Denoising Diffusion Probabilistic Models》（DDPM）、《High-Resolution Image Synthesis with Latent Diffusion Models》、《Scalable Diffusion Models with Transformers》（DiT）。如果关注条件生成，可以研究 GLIDE、DALL-E 2、Stable Diffusion 的架构细节。

从工程角度，建议深入研究 Stable Diffusion 的开源实现，理解其 pipeline 设计、模型结构、推理优化。对于实际应用，可以探索 ControlNet、LoRA、IP-Adapter 等微调方法，在预训练模型基础上适配特定任务。

扩散模型代表了生成式 AI 的新范式。与 GAN 的对抗训练相比，扩散模型的训练更加稳定；与 VAE 相比，生成质量更高。更重要的是，扩散模型的去噪框架具有极强的通用性，可以处理各种模态的数据生成任务，这是其生命力所在。
