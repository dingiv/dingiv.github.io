---
title: NLP
order: 20
---

# 自然语言处理（Natural Language Processing）

自然语言处理（NLP）是人工智能的重要分支，致力于让计算机理解、解释和生成人类语言。NLP 的目标是在计算机和人类语言之间架起桥梁，使机器能够处理和分析大量的自然语言数据。

## NLP 的核心挑战

人类语言具有高度的复杂性和歧义性，这给计算机处理带来了诸多挑战：

1. **歧义性**：同一个词或句子可能有多种含义
   - 词汇歧义："银行"可以指金融机构或河岸
   - 结构歧义："我看见了一个人用望远镜" - 谁用望远镜？

2. **上下文依赖**：词义和句意依赖于上下文
   - "这个苹果很好吃"（水果）vs "苹果发布了新产品"（公司）

3. **长距离依赖**：句子中相距较远的词之间存在语义关联
   - "我昨天在超市买的那个苹果，今天吃起来很甜"

4. **知识推理**：需要常识和背景知识
   - "他打开冰箱，发现牛奶过期了" - 需要理解冰箱用来存储食物

5. **语言多样性**：不同语言、方言、口语表达方式各异

## 常见 NLP 任务类型

### 基础任务

#### 1. 分词（Tokenization）

将连续的文本切分成有意义的单元（词、字符、子词）。

**中文分词特点**：
- 中文没有天然的词边界（不像英文有空格）
- 需要专门的分词算法（如结巴分词、HanLP）

**子词分词（Subword Tokenization）**：
- BPE（Byte Pair Encoding）
- WordPiece（BERT 使用）
- SentencePiece（T5、LLaMA 使用）

**示例**：
```
原文：自然语言处理很有趣
分词：自然 / 语言 / 处理 / 很 / 有趣

英文：Natural Language Processing is interesting
分词：Natural / Language / Processing / is / interesting
```

#### 2. 词性标注（Part-of-Speech Tagging）

为每个词标注其词性（名词、动词、形容词等）。

**示例**：
```
The/DT cat/NN sits/VBZ on/IN the/DT mat/NN
那/DT 只/M 猫/NN 坐/VV 在/P 垫子/NN 上/LC
```

#### 3. 命名实体识别（Named Entity Recognition, NER）

识别文本中的实体（人名、地名、组织名、时间等）。

**标注格式**：常用 BIO 标注
- B (Begin)：实体开始
- I (Inside)：实体内部
- O (Outside)：非实体

**示例**：
```
输入：苹果公司的CEO蒂姆·库克在北京发表演讲
输出：
  苹果公司 -> 组织
  蒂姆·库克 -> 人名
  北京 -> 地名
```

**应用**：信息抽取、知识图谱构建、智能客服

### 理解任务

#### 4. 文本分类（Text Classification）

将文本分配到预定义的类别。

**常见子任务**：
- **情感分析**：判断文本的情感倾向（正面、负面、中性）
- **主题分类**：将文档归类到不同主题（体育、科技、娱乐等）
- **垃圾邮件检测**：识别垃圾邮件
- **意图识别**：理解用户意图（对话系统）

**示例**：
```
文本："这家餐厅的菜品非常美味，服务也很周到！"
分类：正面情感（Positive）
```

**方法演变**：
- 传统：TF-IDF + 朴素贝叶斯/SVM
- 深度学习：CNN、LSTM、BERT

#### 5. 文本相似度计算

衡量两段文本的语义相似程度。

**应用场景**：
- 搜索引擎：查询和文档的匹配
- 问答系统：找到相似的历史问题
- 去重：检测重复内容
- 推荐系统：基于内容的推荐

**方法**：
- 余弦相似度（基于词向量）
- 编辑距离（Levenshtein Distance）
- 语义相似度（BERT、Sentence-BERT）

#### 6. 阅读理解与问答（Question Answering）

给定文本（上下文）和问题，从文本中找出答案或生成答案。

**类型**：
- **抽取式问答**：答案是原文中的片段（SQuAD）
- **生成式问答**：生成新的答案文本
- **多跳推理**：需要综合多个信息片段

**示例**：
```
上下文：苹果公司成立于1976年，总部位于加利福尼亚州库比蒂诺。
问题：苹果公司的总部在哪里？
答案：加利福尼亚州库比蒂诺
```

#### 7. 自然语言推理（Natural Language Inference, NLI）

判断两个句子之间的逻辑关系。

**关系类型**：
- **蕴含（Entailment）**：前提可以推出假设
- **矛盾（Contradiction）**：前提与假设相矛盾
- **中性（Neutral）**：无法判断

**示例**：
```
前提：一个女人在喝咖啡
假设：一个人在喝饮料
关系：蕴含（Entailment）
```

### 生成任务

#### 8. 机器翻译（Machine Translation）

将一种语言的文本翻译成另一种语言。

**发展历程**：
- **基于规则**：手工编写翻译规则
- **统计机器翻译（SMT）**：基于大规模平行语料
- **神经机器翻译（NMT）**：Seq2Seq + Attention（2014-2017）
- **Transformer 翻译**：Google Translate、DeepL（2017-至今）

**挑战**：
- 歧义处理
- 文化差异
- 罕见词翻译
- 保持语序和语法的流畅性

#### 9. 文本摘要（Text Summarization）

生成文本的简洁摘要。

**类型**：
- **抽取式摘要**：选取原文中的关键句子
- **生成式摘要**：生成新的总结性文本

**应用**：新闻摘要、会议纪要、文献综述

#### 10. 对话系统（Dialogue Systems）

与用户进行自然语言交互。

**类型**：
- **任务导向型**：完成特定任务（订票、客服）
- **闲聊型**：开放域对话（聊天机器人）
- **问答型**：回答用户问题

**关键模块**：
- 自然语言理解（NLU）
- 对话管理（DM）
- 自然语言生成（NLG）

#### 11. 文本生成（Text Generation）

根据给定的提示或条件生成连贯的文本。

**应用**：
- 创意写作：诗歌、故事生成
- 代码生成：GitHub Copilot
- 数据增强：生成训练样本
- 对话回复生成

**评估指标**：
- BLEU：机器翻译评估
- ROUGE：摘要评估
- Perplexity：语言模型困惑度

## NLP 神经网络架构演进

### 传统方法的局限

在深度学习之前，NLP 主要依赖：
- **词袋模型（Bag of Words）**：丢失词序信息
- **TF-IDF**：无法捕捉语义
- **N-gram**：维度爆炸，稀疏性问题

**词向量革命**（Word2Vec, 2013）：
- Skip-gram 和 CBOW 模型
- 将词映射到稠密的低维向量空间
- 捕捉词的语义关系："king - man + woman ≈ queen"

