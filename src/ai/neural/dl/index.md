---
title: 深度学习
order: 1
---

# 深度学习

限制机器学习的一大难题在于**训练数据的获取**，得到大量的优质训练数据是训练成功的前提。如何能够做到优质？那就要对数据做充足的特征工程。深度学习通过**多层神经网络**架构，极大地简化了特征工程的工作，让机器能够**自动学习特征表示**，极大推动了连接主义 AI 的发展。

从2012年 AlexNet 在 ImageNet 竞赛中的突破性表现开始，深度学习迅速成为人工智能领域的主流范式。它在计算机视觉、自然语言处理、语音识别等众多领域都取得了革命性的进展，甚至在围棋、蛋白质折叠等复杂问题上超越了人类专家。

## 核心思想

深度学习的本质是通过**端到端学习**（End-to-End Learning）的方式，用数据驱动模型自动提取层次化的特征表示：

- **浅层网络**学习低级特征（边缘、纹理、音素）
- **深层网络**学习高级特征（物体部件、语义概念）
- **输出层**完成最终任务（分类、检测、生成）

这种分层抽象的方式与人类感知系统高度相似，也是深度学习强大表达能力的来源。

## 不可解释性

深度学习模型通常包含数百万甚至数十亿个参数，这些参数通过复杂的非线性变换组合在一起，形成了一个"黑盒"系统。我们很难直观理解模型为什么做出某个预测，哪些特征起了关键作用。

这种不可解释性带来了一些挑战：

- **调试困难**：模型出错时难以定位问题
- **信任问题**：在医疗、金融等关键领域难以直接应用
- **安全隐患**：容易受到对抗样本攻击
- **偏见放大**：可能学习并放大训练数据中的偏见

近年来，可解释 AI（XAI）成为重要研究方向，尝试通过注意力可视化、特征归因、概念激活向量等技术来揭示模型的决策过程。


## 感知机
感知机（Perceptron）是机器学习中最古老、最简单的神经网络模型，它是单层神经网络的原型，主要用于二分类任务。感知机试图找到一个线性超平面，将两类样本完全分开（假设数据线性可分）。感知机是现代深度学习的前身。

![](./perceptron.webp)

- 任务：二分类。最简单神经网络，线性分类器。
- 模型：$w \cdot x + b > 0$ → 正类。
- 更新：误差驱动$w \leftarrow w + \eta (y - \hat{y}) x$
- 优点：在线学习、简单。
- 缺点：仅线性可分（XOR问题）、收敛需线性可分数据。
- 历史：Rosenblatt 1958，奠基神经网络。

## 激活函数

激活函数（Activation Function）是神经网络中的关键组件，它为网络引入**非线性变换能力**。如果没有激活函数，多层神经网络无论有多少层，本质上都只是线性变换的叠加，等价于单层网络，无法解决复杂的非线性问题。

### 常见激活函数

**Sigmoid**: $\sigma(x) = \frac{1}{1 + e^{-x}}$
- 输出范围 $(0, 1)$，适合概率输出
- 问题：梯度消失、计算开销大、输出非零中心

**Tanh**: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
- 输出范围 $(-1, 1)$，零中心化
- 问题：仍存在梯度消失

**ReLU**: $\text{ReLU}(x) = \max(0, x)$
- 计算简单、缓解梯度消失、稀疏激活
- 问题：神经元"死亡"（负值区域梯度为0）
- 目前最常用的激活函数

**Leaky ReLU**: $\text{LeakyReLU}(x) = \max(0.01x, x)$
- 解决 ReLU 的"死亡"问题
- 负值区域保留小梯度

**GELU**: $\text{GELU}(x) = x \cdot \Phi(x)$
- Transformer 模型中常用
- 平滑的非线性，性能更好

选择激活函数需要考虑任务特性、网络深度和训练稳定性。现代深度学习中，隐藏层多用 ReLU 或其变体，输出层根据任务选择（分类用 Softmax，回归用线性）。

## 梯度下降法

梯度下降（Gradient Descent）是训练神经网络的核心优化算法。它的基本思想是：沿着损失函数梯度的**负方向**迭代更新参数，逐步找到使损失函数最小的参数值。

### 基本公式

$$w_{t+1} = w_t - \eta \nabla L(w_t)$$

其中 $\eta$ 是学习率，$\nabla L(w_t)$ 是损失函数对参数的梯度。

### 三种变体

**批量梯度下降（Batch GD）**
- 每次使用全部训练数据计算梯度
- 优点：收敛稳定
- 缺点：大数据集计算开销巨大

**随机梯度下降（SGD）**
- 每次使用单个样本计算梯度
- 优点：更新快速，可逃离局部最优
- 缺点：收敛不稳定，震荡严重

**小批量梯度下降（Mini-batch GD）**
- 每次使用一小批样本（如32、64、128个）
- 平衡了计算效率和收敛稳定性
- 深度学习中的标准做法

### 现代优化器

为了加速收敛和提高训练稳定性，研究者们提出了许多改进算法：

- **Momentum**：引入动量项，加速收敛并减少震荡
- **AdaGrad**：自适应调整每个参数的学习率
- **RMSProp**：改进 AdaGrad，避免学习率过快衰减
- **Adam**：结合 Momentum 和 RMSProp，目前最常用的优化器
- **AdamW**：改进的 Adam，更好的权重衰减

实践中，Adam 及其变体因为对学习率不敏感、收敛快速而成为首选。

