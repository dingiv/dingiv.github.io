---
title: CNN
---

# 卷积神经网络
卷积神经网络（Convolutional Neural Network，CNN）是计算机视觉领域最经典的深度学习架构。从 1998 年 LeNet-5 用于手写数字识别，到 2012 年 AlexNet 在 ImageNet 竞赛中的惊人表现，再到 ResNet、EfficientNet 等现代架构，CNN 统治计算机视觉领域长达二十余年。

对于 IT 开发者来说，理解 CNN 的关键在于掌握其核心设计哲学：利用图像的局部相关性和平移不变性，通过权重共享大幅减少模型参数量。

## 网络架构总览
经典的 CNN 架构由三种类型的层交替堆叠而成：卷积层负责特征提取，池化层负责降维和抽象，全连接层负责最终分类。这种结构设计源于对人类视觉皮层的研究：简单细胞检测边缘，复杂细胞检测形状，高级细胞识别物体。

### 卷积层
卷积层是 CNN 的核心组件，它通过**卷积核**在图像上滑动来提取局部特征。卷积核是一个小的 2 维滑动窗口（通常是 3×3 或 5×5），其权重在整个图像上共享。这种权重共享机制是 CNN 的精髓所在：一个边缘检测器在图像左上角和右下角同样有效，因为图像具有平移不变性。

从数学角度看，二维卷积操作可以表示为：(f * g)(i, j) = Σₘ Σₙ f(i-m, j-n)g(m, n)。但在深度学习框架中实际使用的是互相关（cross-correlation）操作，省去了卷积核的翻转步骤，两者在学习等价的权重矩阵上没有本质区别。

#### 步长与填充

步长（stride）决定了卷积核在图像上滑动的步幅。默认步长为 1，每次移动一个像素；步长为 2 时每次移动两个像素，输出特征图尺寸减半。大步长是一种下采样方式，可以替代池化层的作用，但会丢失部分空间信息。

填充（padding）用于控制输出特征图的尺寸。常见的填充策略包括 valid 填充（不填充，输出尺寸缩小）和 same 填充（填充使得输出尺寸等于输入尺寸除以步长）。填充值为 (kernel_size - 1) / 2 时可以保持特征图尺寸不变（步长为 1 的情况下）。填充还有另一个重要作用：保留边界信息，避免图像边缘像素参与卷积计算的次数少于中心像素。

在代码实现中，假设输入图像为 [Batch, Channels, Height, Width]，卷积核大小为 kernel_size，输出通道数为 out_channels，步长为 stride，填充为 padding。输出特征图的尺寸可以通过公式计算：output_size = (input_size - kernel_size + 2×padding) / stride + 1。

#### 多通道卷积与感受野

实际图像通常是三通道的 RGB 图像，卷积操作需要同时处理多个通道。多通道卷积的卷积核深度必须与输入通道数匹配，例如输入是 3 通道，卷积核大小实际上是 [kernel_size, kernel_size, 3]。每个输出通道由一个完整的卷积核组计算得到，对输入的所有通道进行加权求和后合并。因此如果输入有 C_in 个通道，输出需要 C_out 个通道，卷积核权重矩阵的维度为 [C_out, C_in, kernel_size, kernel_size]。

感受野（receptive field）是指输出特征图中某个元素对应的输入图像中的区域大小。单个 3×3 卷积的感受野是 3×3，两个 3×3 卷积堆叠后的感受野是 5×5。感受野的计算公式为：RF_l = RF_{l-1} + (kernel_size - 1) × stride_{l-1}。深层网络通过堆叠多层卷积获得更大的感受野，最终能够"看到"整张图像。

从物理意义看，每个卷积核实际上是在学习一种特定的特征检测器：浅层卷积核倾向于检测简单的边缘和纹理，深层卷积核则能够识别复杂的模式和物体部件。这种层次化的特征提取使得 CNN 能够从原始像素中逐步构建出高级语义表示。

#### 特殊卷积变体

1×1 卷积是一种特殊的卷积操作，它不改变特征图的空间尺寸，只改变通道数。从数学上看，1×1 卷积等价于对每个像素位置独立地进行全连接计算。它在网络设计中常被用于通道降维或升维、增加非线性（通过后续的激活函数），以及实现跨通道的信息交互。

转置卷积（Transposed Convolution，也称反卷积）用于上采样操作，将特征图尺寸放大。它通过在输入特征图之间插入零值（步长大于 1 时）然后进行普通卷积来实现。转置卷积在语义分割、图像生成等需要恢复分辨率的任务中非常重要。但需要注意它可能产生棋盘格效应，可以通过调整步长和卷积核大小来缓解。

空洞卷积（Dilated Convolution）在卷积核元素之间插入空洞（即跳过某些像素），能够在不增加参数量的情况下扩大感受野。空洞率为 2 的 3×3 卷积相当于 5×5 的感受野，但参数量仍然是 9。这使得空洞卷积在需要大感受野的场景下（如语义分割）非常有用，但过大的空洞率可能导致网格效应（gridding artifact），即某些像素无法参与计算。

分组卷积（Grouped Convolution）将输入通道和输出通道分别分成若干组，每组独立进行卷积。极端情况下，组数等于输入通道数时就是深度可分离卷积的深度卷积部分。分组卷积可以减少参数量和计算量，但限制了跨组的信息流动。ResNeXt 通过分组卷积在相同计算量下获得了比 ResNet 更好的性能。

### 池化层

池化层的作用是对特征图进行降维，减少参数量和计算量，同时引入一定程度的平移不变性。最常用的是最大池化（Max Pooling），它在每个局部窗口内取最大值作为输出。相比平均池化，最大池化能够保留最显著的特征，在实践中效果更好。

从信息处理的角度看，池化层是一种下采样操作。它丢弃了精确的空间位置信息，保留了特征的显著性和大致位置。这种损失在某些任务中是可以接受的，比如图像分类只需要知道图像中"有什么"，而不需要知道"精确在哪里"。但对于目标检测或语义分割等需要精确定位的任务，现代架构倾向于使用步长卷积替代池化层，以保留更多的空间信息。

### 全连接层与激活函数

全连接层位于网络的末端，将提取到的特征映射到类别空间。在传统 CNN 中，经过多次卷积和池化后，特征图被展平为一维向量，然后通过一个或多个全连接层进行分类。现代架构中，全局平均池化常被用来替代展平操作，它对每个通道求平均后直接连接到输出层，大大减少了参数量。

激活函数引入非线性，使得网络能够学习复杂的函数映射。ReLU 及其变体是最常用的选择，相比于传统的 Sigmoid 和 Tanh，ReLU 计算简单且缓解了梯度消失问题。在批归一化（BatchNorm）出现后，深层网络的训练变得更加稳定，ReLU 的优势进一步凸显。

## 经典架构演进

### LeNet 与 AlexNet

LeNet-5 由 Yann LeCun 在 1998 年提出，用于识别手写数字。它奠定了 CNN 的基本结构：卷积层、池化层、全连接层交替堆叠，使用 Sigmoid 激活函数。虽然结构简单，但包含了现代 CNN 的所有核心思想。

AlexNet 在 2012 年 ImageNet 竞赛中以 15.3% 的 top-5 错误率夺得冠军，远超第二名的 26.2%。它的成功归功于三个关键因素：使用 ReLU 激活函数加速训练，采用 Dropout 防止过拟合，利用 GPU 进行并行计算。AlexNet 还引入了分块训练技术，将模型分布在两个 GPU 上，突破了显存限制。

### VGG 与 ResNet

VGGNet 展示了网络深度的重要性。它全部使用 3×3 卷积核，通过堆叠多个 3×3 卷积来获得更大的感受野。两个 3×3 卷积的堆叠相当于一个 5×5 卷积的感受野，但参数量更少（2×3×3=18 vs 5×5=25）。VGG-16 有 16 层，VGG-19 有 19 层，这种深度使得它能够学习更复杂的特征表示。

但单纯增加深度带来了梯度消失问题。ResNet 通过残差连接解决了这个问题：每个学习层学习的是残差映射 F(x) 而不是目标映射 H(x)，即 H(x) = F(x) + x。这种设计使得梯度能够直接通过网络传播，训练数百层的网络成为可能。ResNet-50、ResNet-101、ResNet-152 至今仍是许多应用的基准模型。

### 现代高效架构

EfficientNet 通过复合缩放方法同时优化网络的深度、宽度和分辨率。它证明了均匀缩放这三个维度比单独缩放其中一个更有效。EfficientNet-B0 到 B7 在参数量和准确率之间取得了很好的平衡，是移动端部署的热门选择。

ConvNeXt 重新审视了纯卷积网络的潜力。它借鉴了 Vision Transformer 的设计理念，如更大的卷积核（7×7）、层归一化（LayerNorm）、更少的激活函数等，使得纯卷积网络在 ImageNet 上首次超越了同级 ViT。这证明了经典的卷积架构经过改进仍有强大生命力。

## 工程实践考量

### 训练技巧与优化

CNN 的训练相比 ViT 更简单直接。标准的 SGD 优化器配合动量（momentum）就能取得不错效果，学习率通常从 0.1 开始，每若干个 epoch 衰减一次。数据增强是提高泛化能力的关键，常用的方法包括随机裁剪、水平翻转、颜色抖动等。对于小数据集，还可以使用 MixUp 和 CutMix 等高级增强技术。

批归一化是现代 CNN 的标配。它对每个 batch 的特征进行归一化，使得层与层之间的输入分布更加稳定，加速了训练并提高了泛化能力。但推理时的 batch size 可能很小，这时需要使用训练期间统计的移动平均值进行归一化。

### 推理优化与部署

CNN 的推理优化在工程实践中非常重要。首先可以通过剪枝（pruning）移除不重要的连接，然后通过量化（quantization）将 32 位浮点数转换为 8 位整数，在精度损失很小的情况下大幅减少模型大小和提高推理速度。对于特定的硬件平台，还可以使用 TensorRT、OpenVINO 等推理框架进行算子融合和内核优化。

在端侧设备上部署时，模型的大小和推理速度是关键约束。MobileNet 系列通过深度可分离卷积（Depthwise Separable Convolution）大幅减少参数量。标准卷积同时对空间和通道进行卷积，而深度可分离卷积将其分解为深度卷积（每个通道独立卷积）和逐点卷积（1×1 卷积混合通道），参数量减少到原来的约 1/8。

### 常见问题与调试

CNN 训练中最常见的问题是过拟合。除了使用 Dropout 和数据增强外，还可以通过早停（early stopping）来监控验证集性能，在模型开始过拟合时停止训练。另一个问题是梯度消失或爆炸，可以通过批归一化、残差连接和适当的初始化方法（如 He 初始化）来缓解。

在调试 CNN 时，可视化是一个强大的工具。可以可视化卷积核来查看网络学到了什么样的特征，可视化特征图来理解网络如何处理输入，可视化激活值分布来诊断网络的健康状况。Grad-CAM 等技术可以生成类激活映射，帮助理解模型的决策依据。

## 架构对比与选择

CNN 相比 ViT 的核心优势在于归纳偏置强。CNN 假设相邻像素相关、局部模式可平移，这使得它在中小规模数据集上训练效率高，模型参数量相对较少。CNN 的计算复杂度与图像分辨率呈线性关系，在处理高分辨率图像时更具优势。而且 CNN 的生态系统非常成熟，有大量预训练权重和部署工具可供使用。

但 CNN 的感受野是逐步扩大的，需要到深层才能获得全局信息。这在处理需要全局推理的任务时可能成为限制。另外，CNN 与语言模型的架构差异较大，在多模态应用中需要额外的对齐工作。

对于实际应用，如果数据量在百万级别以下，或者需要在资源受限的设备上部署，CNN 仍然是更实用的选择。ResNet-50 及其变体在精度和效率之间取得了良好平衡，是许多应用的首选。如果关注模型大小和推理速度，可以考虑 MobileNet 或 EfficientNet 的轻量级版本。

## 学习路径

入门建议先理解卷积操作的数学原理和反向传播推导，然后使用 PyTorch 或 TensorFlow 从零实现一个简单的 CNN。接着可以研究经典架构的演进过程，从 LeNet 到 AlexNet、VGG、ResNet，理解每个改进背后的动机。

论文推荐阅读《Gradient-Based Learning Applied to Document Recognition》（LeNet）、《ImageNet Classification with Deep Convolutional Neural Networks》（AlexNet）、《Deep Residual Learning for Image Recognition》（ResNet）。如果关注模型压缩和部署，可以研究 pruning、quantization、knowledge distillation 等技术。

在现代 CV 任务中，CNN 和 ViT 各有优势。理解 CNN 的工作原理不仅有助于选择合适的模型架构，更能为理解 ViT 等新兴架构提供对比和参照。毕竟，CNN 凝结了人类对视觉理解的深刻智慧，这些智慧至今仍有启发意义。
