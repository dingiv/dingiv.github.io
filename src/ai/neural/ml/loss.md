# 损失函数
损失函数，也称为**代价函数（Cost Function）**或**目标函数（Objective Function）**，是机器学习模型训练的核心组成部分。它用于**量化模型预测值与真实值之间的误差**。

## 核心作用
- 在模型训练阶段，优化算法（如梯度下降）通过不断最小化损失函数来调整模型参数。
- 损失函数的值直接指导参数更新方向。

我们如何评价模型已经训练的比较优秀了？通过比较训练之后的预测值 $\hat{y_i}$ 和真实值 $y_i$ 之间的关系。

$$
Loss = y_i - \hat{y_i}
$$
$y_i$ 是真实的标签值，常数；$\hat{y_i}$ 是关于模型中的参数值 $w_i$ 和 $b_i$ 等的函数
$$
\hat{y_i} = f(w_1, w_2, ..., w_n, b_1, b_2, ..., b_n)
$$

因此，整个损失函数本质上可以视为一个关于这些模型参数的多变量函数，而模型训练的过程归结为求解使该损失函数取得最小值的参数组合，很自然地，我们可以通过对损失函数关于各个参数求偏导数，并令导数为零或利用数值优化方法来找到损失函数的极小值点，此时的参数值即为训练完成的模型参数。

## 常见损失函数类型

| 任务类型 | 损失函数名称                | 公式示例                                             | 适用场景               | 特点                     |
| -------- | --------------------------- | ---------------------------------------------------- | ---------------------- | ------------------------ |
| 回归     | 均方误差（MSE）             | $J = \frac{1}{m} \sum (y - \hat{y})^2$               | 线性回归等连续值预测   | 对大误差惩罚更重，易优化 |
| 回归     | 平均绝对误差（MAE）         | $J = \frac{1}{m} \sum \|y - \hat{y}\|$               | 对异常值鲁棒性要求高   | 对异常值不敏感           |
| 分类     | 交叉熵损失（Cross-Entropy） | 二分类：$J = -[y\log\hat{y} + (1-y)\log(1-\hat{y})]$ | 逻辑回归、神经网络分类 | 与概率输出匹配，效果好   |
| 分类     | 合页损失（Hinge Loss）      | $J = \max(0, 1 - y \cdot \hat{y})$                   | SVM                    | 追求最大间隔             |

需要特别注意的是，不同的任务类型必须搭配合适的损失函数，否则模型将难以有效学习数据中的规律，例如在回归问题中使用交叉熵损失或在分类问题中使用均方误差都会导致优化方向严重偏离预期。

损失函数的计算通常基于训练集进行，但我们最终关心的始终是模型在独立测试集上的泛化表现，以避免仅仅在训练数据上过度拟合。在实际工程实践中，为了进一步抑制过拟合现象，常会在原始损失函数的基础上添加正则化项，例如 L1 正则（Lasso）通过参数绝对值之和惩罚稀疏性，而 L2 正则（Ridge）通过参数平方和惩罚权重幅度，从而在经验风险最小化的同时实现结构风险的最小化。

## 梯度下降法
梯度下降法是求解上述损失函数最小值的最基础且最常用的优化算法，其核心思想在于利用损失函数关于参数的梯度（即偏导数向量）来指导参数的迭代更新，因为梯度方向指向函数值上升最快的方向，所以沿着负梯度方向以一定步长（学习率）移动参数即可使损失函数值逐步下降，直至收敛到局部或全局最小点。

具体而言，对于参数向量 $\theta$，梯度下降的更新规则为 $\theta := \theta - \alpha \nabla J(\theta)$，其中 $\alpha$ 为学习率，$\nabla J(\theta)$ 为损失函数对 $\theta$ 的梯度，根据每次计算梯度所用样本数量的不同，梯度下降又可分为批量梯度下降（使用全部训练数据，精确但计算开销大）、随机梯度下降（每次仅用一个样本，噪声大但收敛快）以及小批量梯度下降（折中方案，实际中最常用）；此外，为了进一步提升优化效率与稳定性，现代实践中广泛采用诸如动量法、AdaGrad、RMSProp 以及 Adam 等自适应优化器，它们在基本梯度下降的基础上引入动量积累或自适应学习率机制，从而在复杂非凸损失景观中表现出更优的收敛性能。

总之，损失函数与梯度下降法的结合构成了参数化机器学习模型训练的基石，深刻理解二者的数学本质与实际选择原则，是掌握机器学习算法的关键一步。
