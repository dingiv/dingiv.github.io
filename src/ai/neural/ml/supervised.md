# 监督学习
监督学习的训练数据带有标签，监督学习的训练目标是学习特征 → 标签的映射。监督学习主要包括分类和回归两种子类。

回归使用函数来拟合因变量和自变量之间的关系，适合于标签是连续的数值；而有些标签并不是连续数值，例如，判断一个零件质量是否合格，该标签是一个典型的二值枚举，仅包含 "是" 和 "否" 两种结果，此外，还有多分类（如图像识别）。

| 项目     | 分类（Classification）          | 回归（Regression）           |
| -------- | ------------------------------- | ---------------------------- |
| 输出类型 | 离散类别（如“是/否”、1/2/3）    | 连续数值（如100.5、-3.2）    |
| 决策边界 | 分割类别边界                    | 拟合连续曲线/曲面            |
| 典型问题 | 邮件分类、疾病诊断、图像识别    | 房价预测、销量预测、温度预测 |
| 评估指标 | 准确率、精确率、召回率、F1、AUC | MSE、RMSE、MAE、$R^2$        |

不同的算法实现方式不同，但是有相同的核心任务，那就是在训练数据上学习一个函数 f，使得在新数据上预测尽可能准确（泛化能力强）。这些算法中，有一些从一开始便已经判断出数据之间具有的函数关系的形式，只是函数中的某些位置的上的参数不确定，典型的如线性回归，在一开始，我们便已经确定了某个参数就是类似于线性关系的模式，不确定的是这个线性函数中的 w 和 b 参数。为此，可以将模型分为**有参模型和无参模型**。有参模型往往需要大量的数据统计进行训练，从而拟合出模型的中的可变参数；无参模型，仅需进行少量的训练或者无需训练，即可通过现场归纳数据进行预测。

模型哲学：有参 vs 无参
+ 有参模型 (Parametric)：预设了函数形式（如线性关系），通过数据估计有限数量的参数（如 $w, b$）。优点是速度快，缺点是模型容量受限。典型代表：线性回归、逻辑回归。
+ 无参模型 (Non-parametric)：不预设固定的函数形式，模型复杂度随数据量增长。优点是灵活，缺点是计算开销大。典型代表：KNN、决策树。

## 有参算法

### 线性回归
线性回归是最基本的数据分析模型，它假设自变量和因变量之间具有一个简单的函数关系，用直线/超平面拟合变量之间规律。这种方法在高中的课本上就已经教授过。

- 任务：回归。
- 模型：$y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b$
- 损失：均方误差
- 优缺点：简单、可解释、计算快。线性假设，对异常值敏感。可以通过岭回归（L2正则）、Lasso（L1正则）防过拟合。

线性回归的求解方式基于[损失函数](./loss)的概念，主要有两种方式：最小二乘法（公式法）或者梯度下降法，其中，最小二乘法是纯数学的实现方式，而梯度下降法是我们软件或计算机行业具体落地的时候的实践方式。

### 逻辑回归
逻辑回归是一种仿照通过线性回归改造的用于分类的算法，尽管它叫回归。
- 任务：分类（常二分类）。基于线性回归 + Sigmoid，将输出转为 `[0,1]` 概率。
- 模型：$z = w \cdot x + b, \quad p = \sigma(z) = \frac{1}{1 + e^{-z}}$，预测：$p \ge 0.5$ → 正类。
- 损失：交叉熵
- 优缺点：输出概率、可解释、快、适合高维。线性决策边界、多分类需One-vs-Rest。
- 适用：垃圾邮件、疾病诊断。


## 无参算法
### KNN
K近邻算法是一种简单、直观且经典的机器学习算法，属于懒惰学习（lazy learning），即训练阶段几乎不做任何计算，只存储数据；预测时才进行计算。

根据测试样本在特征空间中最接近的 K 个训练样本的类别（或值），来预测测试样本的类别（分类）或数值（回归）。该算法体现了一个思想：近朱者赤，近墨者黑。我们会通过观察一个“人”周围的朋友来判断这个“人”的好坏。

- 任务：分类或者回归均可以。分类：少数服从多数（投票）；回归：取 K 个邻居的平均值（或加权平均）；
- 模型：欧氏距离 $d(x,y) = \sqrt{\sum (x_i - y_i)^2}$。
- 超参数：K值（小→过拟合，大→欠拟合，用交叉验证调优）。
- 优缺点：简单、非线性、对分布无假设。计算慢（O(n)）、存储全数据、高维差、需标准化。
- 适用：小数据集、基线模型。

### 决策树
决策树（Decision Tree）是一种经典的监督学习算法，它通过递归地将特征空间划分成多个区域，每个区域对应一个预测值，用于决策。

女生相亲决策算法。
![决策树](./dtree.png)

- 任务：分类/回归。通过树结构递归分割特征空间。
- 分裂准则：
  - 分类：信息增益（ID3）、信息增益比（C4.5）、基尼不纯度（CART）。
  - 回归：方差减少。
- 优点：可解释（可视化树）、非线性、无需缩放、处理混合数据。
- 缺点：易过拟合（深树记训练数据）、不稳定（数据微变树大变）。
- 剪枝：预剪枝（限深度）、后剪枝（最小化验证误差）。

### SVM
支持向量机（Support Vector Machine）
- 任务：分类/回归。找最大间隔超平面分割类别。
- 硬间隔：线性可分，最大化$\frac{2}{||w||}$。
- 软间隔：引入松弛变量+铰链损失，容忍噪声。
- 核技巧：RBF核$K(x_i,x_j) = \exp(-\gamma ||x_i - x_j||^2)$ 处理非线性。
- 优点：泛化强、少样本有效、高维好。
- 缺点：训练慢（O(n^2/n^3)）、参数调优难、不可解释。
- 适用：文本分类、图像。

## 集成学习
集成学习（Ensemble Learning）是监督学习中经常采用的模型训练增强策略，其基本思想就是“三个臭皮匠，顶个诸葛亮”，通过三个弱模型合作，组成一个强模型，以此提升模型效果。

集成学习一般有三种常见的思路：
- Bagging（装袋法）：并行训练，思维碰撞。从原始数据中有放回地随机采样，构造出多个不同的训练集，分别分给多个独立的决策树，每棵树看到的样本和特征**都不同但有交集**，它们各有各的偏见，但把结果平均一下（回归）或投票（分类），就能消除个体的噪声，使模型非常稳定，能够防止过拟合。
- Boosting（提升法）：串行训练，减少偏差。多个模型进行串联，每个模型看到的训练集相同，但后一个模型专门攻克前一个模型做错的题，关注前轮错误。第一个模型预测完，发现有些样本预测错了（残差大），第二个模型就重点去拟合这些残差，通过不断“打补丁”，最后把所有模型加权组合
- Stacking（堆叠法）：多模型加元模型，裁判裁决。先训练第一层的几个不同模型（比如 SVM、KNN、随机森林），然后把它们的预测结果作为新的特征输入给第二层的模型（通常是简单的逻辑回归），由第二层模型给出最终答案。

不同的思路之间，体现了多个模型的合作方式不同。

### 随机森林
随机森林是一种通过集成多棵决策树来提高预测精度和稳定性的算法，结合 Bagging 集成学习思想。它通过引入“随机性”来解决单棵决策树容易过拟合的问题。

+ 样本随机抽样 (Bootstrap Aggregating)：从原始训练集中，通过有放回抽样（Bootstrap）选出一部分样本。这意味着某些样本可能在一棵树中出现多次，而有些则从未出现（这些未被选中的约 36.8% 的数据称为 袋外数据 OOB，可用于自我验证）。
+ 特征随机抽样：在决策树的每个节点分裂时，算法并不从所有特征中寻找最优解，而是随机选取特征子集（通常是 $\sqrt{p}$ 个特征），从中选出最佳分裂特征。

### XGBoost
XGBoost （Extreme Gradient Boosting）核心思想是串行地训练树，每棵新树都在拟合前一棵树的残差。属于**梯度提升树（Gradient Boosting Decision Tree, GBDT）**的优化实现

```
最终预测 = 所有树预测之和（加性模型）。
```

目标函数：
$$
Obj = \sum_{i=1}^m l(y_i, \hat{y}_i) + \sum_{k=1}^K \Omega(f_k)
$$
- l：损失函数（如MSE、LogLoss）。
- Ω：正则项，惩罚树复杂度。

每轮添加新树 f_t，使目标最小化。

在损失函数中加入正则项控制复杂度；对损失函数进行二阶泰勒展开以获得更准的梯度信息；支持特征粒度的并行计算

与传统 GBDT 相比，XGBoost 的关键改进：
1. 二阶梯度优化：不仅用一阶梯度（残差），还用二阶导数（Hessian）加速收敛，提高精度。
2. 正则化：在损失函数中加入树复杂度惩罚（叶子数、叶子权重平方），有效防止过拟合。
3. 稀疏感知：高效处理缺失值（自动学习缺失方向）。
4. 直方图加速：近似分裂点查找，显著降低计算量。
5. 并行与缓存优化：特征列排序、块并行，提升训练速度。
6. 支持多种目标函数：分类、回归、排序、计数等。

优点:精度极高，常优于随机森林、LightGBM（视任务）；训练速度快（直方图+并行）；原生支持缺失值、类别特征（较新版本）；内置交叉验证、早停机制；特征重要性可解释。

缺点：参数多，需要仔细调优；内存占用较高（尤其大数据集）；对噪声敏感（需特征工程）；不适合超高维稀疏数据（如文本，需结合embedding）。
