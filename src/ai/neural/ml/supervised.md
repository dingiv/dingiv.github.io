# 监督学习
训练数据带有标签，目标是学习特征 → 标签的映射。监督学习主要包括分类和回归两种子类。

回归使用函数来拟合因变量和自变量之间的关系，适合于标签是连续的数值；而有些标签并不是连续数值，例如，判断一个零件质量是否合格，该标签是一个典型的二值枚举，仅包含 "是" 和 "否" 两种结果，此外，还有多分类（如图像识别）。

| 项目     | 分类（Classification）          | 回归（Regression）           |
| -------- | ------------------------------- | ---------------------------- |
| 输出类型 | 离散类别（如“是/否”、1/2/3）    | 连续数值（如100.5、-3.2）    |
| 决策边界 | 分割类别边界                    | 拟合连续曲线/曲面            |
| 典型问题 | 邮件分类、疾病诊断、图像识别    | 房价预测、销量预测、温度预测 |
| 评估指标 | 准确率、精确率、召回率、F1、AUC | MSE、RMSE、MAE、$R^2$        |

不同的算法实现方式不同，但是有相同的核心任务，那就是在训练数据上学习一个函数 f，使得在新数据上预测尽可能准确（泛化能力强）。这些算法中，有一些从一开始便已经判断出数据之间具有的函数关系的形式，只是函数中的某些位置的上的参数不确定，典型的如线性回归，在一开始，我们便已经确定了某个参数就是类似于线性关系的模式，不确定的是这个线性函数中的 w 和 b 参数。为此，可以将模型分为**有参模型和无参模型**。有参模型往往需要大量的数据统计进行训练，从而拟合出模型的中的可变参数；无参模型，仅需进行少量的训练或者无需训练，即可通过现场归纳数据进行预测。

## 线性回归
线性回归是最基本的数据分析模型，它假设自变量和因变量之间具有一个简单的函数关系，用直线/超平面拟合变量之间规律。这种方法在高中的课本上就已经教授过。

- 任务：回归。
- 模型：$y = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b$
- 损失：均方误差
- 优缺点：简单、可解释、计算快。线性假设，对异常值敏感。可以通过岭回归（L2正则）、Lasso（L1正则）防过拟合。

线性回归的求解方式主要有两种方式：最小二乘法（闭式）或者[梯度下降法](./loss)，其中，最小二乘法是纯数学的实现方式，而梯度下降法是我们软件或计算机行业具体落地的时候的实践方式。

## 逻辑回归
逻辑回归是一种仿照通过线性回归改造的用于分类的算法，尽管它叫回归。
- 任务：分类（常二分类）。基于线性回归 + Sigmoid，将输出转为 `[0,1]` 概率。
- 模型：$z = w \cdot x + b, \quad p = \sigma(z) = \frac{1}{1 + e^{-z}}$，预测：$p \ge 0.5$ → 正类。
- 损失：交叉熵
- 优缺点：输出概率、可解释、快、适合高维。线性决策边界、多分类需One-vs-Rest。
- 适用：垃圾邮件、疾病诊断。

## KNN
K近邻算法是一种简单、直观且经典的机器学习算法，属于懒惰学习（lazy learning），即训练阶段几乎不做任何计算，只存储数据；预测时才进行计算。

根据测试样本在特征空间中最接近的 K 个训练样本的类别（或值），来预测测试样本的类别（分类）或数值（回归）。该算法体现了一个思想：近朱者赤，近墨者黑。我们会通过观察一个“人”周围的朋友来判断这个“人”的好坏。

- 任务：分类或者回归均可以。分类：少数服从多数（投票）；回归：取 K 个邻居的平均值（或加权平均）；
- 模型：欧氏距离 $d(x,y) = \sqrt{\sum (x_i - y_i)^2}$。
- 超参数：K值（小→过拟合，大→欠拟合，用交叉验证调优）。
- 优缺点：简单、非线性、对分布无假设。计算慢（O(n)）、存储全数据、高维差、需标准化。
- 适用：小数据集、基线模型。

## 决策树
决策树（Decision Tree）是一种经典的监督学习算法，它通过递归地将特征空间划分成多个区域，每个区域对应一个预测值，用于决策。

女生相亲决策算法。
![决策树](./dtree.png)

- 任务：分类/回归。通过树结构递归分割特征空间。
- 分裂准则：
  - 分类：信息增益（ID3）、信息增益比（C4.5）、基尼不纯度（CART）。
  - 回归：方差减少。
- 优点：可解释（可视化树）、非线性、无需缩放、处理混合数据。
- 缺点：易过拟合（深树记训练数据）、不稳定（数据微变树大变）。
- 剪枝：预剪枝（限深度）、后剪枝（最小化验证误差）。

## SVM
支持向量机（Support Vector Machine）
- 任务：分类/回归。找最大间隔超平面分割类别。
- 硬间隔：线性可分，最大化$\frac{2}{||w||}$。
- 软间隔：引入松弛变量+铰链损失，容忍噪声。
- 核技巧：RBF核$K(x_i,x_j) = \exp(-\gamma ||x_i - x_j||^2)$ 处理非线性。
- 优点：泛化强、少样本有效、高维好。
- 缺点：训练慢（O(n^2/n^3)）、参数调优难、不可解释。
- 适用：文本分类、图像。

## 感知机
感知机（Perceptron）是机器学习中最古老、最简单的神经网络模型，它是单层神经网络的原型，主要用于二分类任务。感知机试图找到一个线性超平面，将两类样本完全分开（假设数据线性可分）。感知机是现代深度学习的前身。

![](./perceptron.webp)

- 任务：二分类。最简单神经网络，线性分类器。
- 模型：$w \cdot x + b > 0$ → 正类。
- 更新：误差驱动$w \leftarrow w + \eta (y - \hat{y}) x$
- 优点：在线学习、简单。
- 缺点：仅线性可分（XOR问题）、收敛需线性可分数据。
- 历史：Rosenblatt 1958，奠基神经网络。


## 集成学习
组合多个弱模型 → 强模型，提升性能。
- **Bagging**：并行，有放回采样。**随机森林**：决策树+Bagging+特征随机。
- **Boosting**：顺序，关注前轮错误。**AdaBoost**（加权投票）、**GBDT**（梯度提升）。
- **Stacking**：多模型+元模型。
- **优点**：高精度、防过拟合。
- **代表**：XGBoost、LightGBM（表格数据SOTA）。

### 随机森林


### XGBoost
