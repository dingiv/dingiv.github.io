---
title: 集群
order: 60
---

# 集群

AI 的计算需要消耗大量的计算资源，AI 引擎需要基于分布式集群为前提进行设计和实现。单张 GPU 的算力和显存有限，训练大模型（如 GPT-3 175B）需要数千张 GPU 协同工作，推理高并发请求也需要多 GPU 甚至多机集群。分布式集群涉及通信拓扑、硬件互联、集合通信等多个层面，需要在算法、系统和工程三个层面进行协同优化。

## 硬件互联

集群内部的通信带宽直接影响分布式训练和推理的性能。从 GPU 到 GPU，从节点到节点，不同层次的通信技术带宽差异巨大。

| 互联技术 | 带宽 | 延迟 | 覆盖范围 | 适用场景 |
|----------|------|------|----------|----------|
| NVLink | 450-900 GB/s | <1 μs | 节点内（8 卡） | 张量并行、高频通信 |
| PCIe 5.0 | 64 GB/s | ~1 μs | 节点内（CPU-GPU） | 数据传输 |
| InfiniBand NDR | 400 Gbps (50 GB/s) | ~1 μs | 节点间 | 数据并行、跨节点通信 |
| RoCE v2 | 100-200 Gbps (12.5-25 GB/s) | ~2 μs | 节点间 | 以太网 RDMA |
| 以太网 | 25-100 Gbps (3-12 GB/s) | ~10 μs | 节点间 | 成本敏感场景 |

NVLink 是 NVIDIA 的 GPU 间高速互联技术，带宽远超 PCIe，适用于节点内的高频通信（如张量并行）。InfiniBand 是数据中心级的高性能网络，带宽接近 PCIe、延迟低，适用于跨节点通信。RoCE（RDMA over Converged Ethernet）允许在以太网上进行 RDMA，成本低于 InfiniBand 但性能也略低。

## 通信层次

分布式训练和推理的通信可分为三个层次：机内通信（GPU-GPU）、机间通信（节点-节点）、跨机房通信（数据中心-数据中心）。

机内通信通过 NVLink 或 PCIe 完成，带宽高延迟低，适合张量并行等高频通信场景。机间通信通过 InfiniBand 或以太网完成，带宽较低，适合数据并行等低频通信场景。跨机房通信用于跨数据中心训练（如联邦学习），带宽最低且延迟最高，需要专门的优化算法（如梯度压缩、异步训练）。

## 通信优化

通信与计算重叠（overlap）是将通信时间隐藏在计算时间中的关键技术。DeepSpeed 的梯度预取在前向传播计算层 i 的梯度时，预取层 i+1 的参数到显存，同时同步层 i-1 的梯度，实现三级流水。梯度累积是简单示例：在前向传播计算 mini-batch 1 的同时，同步 mini-batch 0 的梯度。

拓扑感知通信根据网络拓扑优化通信路径。8 张 GPU 组成的单机，ring allreduce 比树状通信效率更高；64 台机器组成的集群，hierarchical allreduce（节点内用 ring，节点间用 tree）最优。NCCL 会自动检测拓扑，选择最优通信算法。

## 通信库

NCCL（NVIDIA Collective Communications Library）是 NVIDIA 提供的集合通信库，针对 NVIDIA GPU 和网络优化，性能远高于开源实现（如 Gloo、MPI）。RCCL 是 AMD GPU 的对应实现。Gloo 是 PyTorch 的通用通信后端，支持 CPU 和 GPU，适合开发测试环境。
