---
title: 主流引擎
order: 10
---

# 推理引擎

推理引擎关注如何在给定模型权重后，高效地生成输出。相比于训练，推理的特点是只做前向传播、对延迟敏感、需要处理变长序列、并发请求多样。这些差异催生了与训练引擎截然不同的优化技术栈。

## 核心挑战

推理的首要挑战是**延迟**，而非吞吐量。用户等待 ChatGPT 返回首个 Token 的时间（TTFT，Time To First Token）直接决定体验，这要求推理引擎优化单请求的端到端延迟，而非简单地提升 batch size。vLLM 通过 PagedAttention 引入细粒度 KV Cache 管理，将 TTFT 降低了 2-3 倍。

显存占用是另一大瓶颈。训练完成后，模型权重和优化器状态可丢弃，但推理时 KV Cache 会随序列长度线性增长。一个 7B 模型在 2048 长度、batch size 32 时，KV Cache 约需 8GB（FP16），超过权重本身。TGI（Text Generation Inference）通过 FlashAttention 减少内存碎片，vLLM 通过 PagedKV 支持显存不足时的部分 offload 到 CPU。

并发请求的调度也很复杂。不同请求的序列长度差异巨大，短序列完成后释放的显存需要分配给新请求，而长序列的 KV Cache 需要持续保留。连续批处理（continuous batching）是 2023 年的关键技术——当 batch 中的某个序列生成结束时，立即插入新序列，而非等待整个 batch 完成。这可将吞吐量提升 3-10 倍。

## 推理优化技术

KV Cache 优化是推理性能的核心。标准 Attention 需要缓存所有历史位置的 Key 和 Value 张量，每次生成新 Token 时与历史 K V 做矩阵乘法，计算复杂度为 $O(n^2)$。FlashAttention 通过分块计算减少内存访问次数，将显存带宽利用率从 20% 提升到 80% 以上，推理速度提升 2-3 倍。PagedAttention 进一步将 KV Cache 分页管理，支持显存不足时的动态换页和共享。

算子融合通过将多个连续算子合并为一个 CUDA kernel 来减少显存访问。例如 LayerNorm 后接 Residual 可融合为一个 kernel，只需读写一次显存，而非两次。TensorRT-LLM 通过图优化自动识别可融合算子，推理延迟降低 15-30%。

量化是降低显存占用和计算量的利器。将 FP16 权重量化为 INT8 可将显存减半，INT4 减为四分之一，配合 INT8/INT4 算子可实现接近原始精度的性能。GPTQ、AWQ、SpQR 等量化算法通过最小化权重误差或激活误差，在 4-bit 量化下仍保持 PPL 接近 FP16。llama.cpp 是量化推理的代表，支持在 CPU 上运行量化后的 LLaMA 模型。

投机采样（speculative decoding）通过一个小模型（如 1B 参数）快速生成多个 Token，然后由大模型并行验证，若验证通过则保留，否则回退重新生成。这在大模型的慢速生成和小模型的快速生成间取得了平衡，可将生成速度提升 2-3 倍。前提是需要训练一个与主模型分布一致的投机模型。

## 推理框架

| 框架 | 核心技术 | 适用场景 |
|------|----------|----------|
| vLLM | PagedAttention、连续批处理 | 高吞吐在线服务、长序列 |
| TGI | FlashAttention、量化基准 | HuggingFace 模型部署、生产环境 |
| TensorRT-LLM | 算子融合、INT4/INT8 量化 | NVIDIA GPU、极致性能 |
| llama.cpp | CPU/GPU 混合推理、GGUF 量化 | 本地部署、资源受限环境 |
| SGLang | RadixAttention、结构化并发 | 多轮对话、复杂 prompt |

选择推理框架需要考虑硬件平台（NVIDIA GPU vs CPU vs Apple Silicon）、部署规模（单机 vs 集群）、延迟要求（在线服务 vs 离线批处理）。vLLM 和 TGI 在 NVIDIA GPU 上性能最强，llama.cpp 适合本地部署，TensorRT-LLM 适合对性能极致优化的场景。

推理性能的关键指标包括：吞吐量（tokens/秒）、延迟（首 token 延迟、每 token 延迟）、显存占用（权重 + KV Cache）、并发能力（最大并发请求数）。benchmark 时需要综合考虑这些指标，而非只看吞吐量。


# 训练引擎

训练引擎关注如何高效地将大模型训练过程分布式化到多张 GPU 甚至多台机器上。相比于单卡训练，分布式训练引入了通信开销、显存压力、负载均衡等复杂问题，需要在算法、系统和工程三个层面进行协同优化。

## 核心挑战

大模型训练面临三大核心挑战：**显存容量**、**计算效率**和**通信带宽**。

显存容量是首要瓶颈。一个 7B 参数的模型，仅权重就需要约 14GB 显存（FP16），加上激活值、梯度、优化器状态，实际需求可能达到 60GB 以上，远超单卡容量。这催生了 ZeRO、FSDP 等显存优化技术，通过分片存储优化器状态、梯度和参数，将显存占用从 $O(2)$ 降至 $O(1)$。

计算效率方面，大模型训练的浮点运算量巨大，GPT-3 175B 的训练需要约 3.14 × 10^23 FLOPs。单纯增加 GPU 数量会面临 Amdahl 定律的通信瓶颈，因此需要算子融合（如 FlashAttention）、混合精度训练（BF16/FP16）、编译优化（torch.compile）等技术提升单卡计算效率。

通信带宽是分布式训练的阿喀琉斯之踵。数据并行需要在每次迭代后同步梯度，模型并行则需要在前向/反向传播中频繁通信张量。NVLink 带宽约 400GB/s，而 PCIe 5.0 仅 32GB/s，跨节点 InfiniBand 通常低于 100GB/s。DeepSpeed、Megatron 通过通信与计算重叠（overlap）、梯度压缩、拓扑感知通信等手段缓解这一问题。

## 分布式策略

从数据并行的朴素同步 SGD，到 3D 并行的复杂张量切分，不同策略在通信频率、显存占用、工程复杂度上各有取舍。

数据并行是最直观的方案：每个 GPU 持有完整模型副本，处理不同数据分片，通过 AllReduce 同步梯度。PyTorch DDP 封装了这一模式，但在大模型场景下，多副本的显存开销无法接受。ZeRO 进一步优化了数据并行，将优化器状态、梯度、参数分片到不同 GPU，仅在需要时通过 AllGather 重建，将显存占用从 $2n$ 倍降至 1 倍（$n$ 为 GPU 数量）。

张量并行将模型的单个算子在多个 GPU 上切分。例如矩阵乘法 $Y = XW$，可将权重 $W$ 按列切分到 4 张卡，每张卡计算 $Y_i = XW_i$，最后通过 AllConcat 拼接结果。这种方式无需复制模型，但每个算子完成后都需要通信，延迟敏感。Megatron-LM 首创了这一技术，用于训练 GPT-3 175B。

流水线并行将模型的不同层分配到不同 GPU，形成流水线。GPU 1 计算第 1-12 层，GPU 2 计算 13-24 层，两者可并行处理不同样本。但流水线存在气泡（bubble）空转，需要通过微批次（micro-batch）调度和 1F1B 策略填充。PipeDream、PipeDream-2BW 是早期探索者。

3D 并行是上述三者的组合：在同一集群内同时使用数据、张量、流水线并行。通常在节点内使用张量并行（高带宽 NVLink），节点间使用流水线并行（跨节点通信少），最外层使用数据并行。Megatron-DeepSpeed 成功用此策略在 3072 张 A100 上训练了 1T 参数的模型。

## 混合专家架构

MoE（Mixture of Experts）通过稀疏激活打破参数量与计算量的耦合。Switch Transformer、GLaM 等模型将 FFN 层替换为多个专家网络，每个 Token 只路由到少数专家，大幅增加模型容量而不增加计算量。但这引入了负载均衡问题——热门专家过载、冷门专家空闲，需要通过专家容量限制（expert capacity factor）、损失惩罚（load balance loss）、动态路由（Top-k gating）等机制调节。

## 训练框架

| 框架 | 核心能力 | 适用场景 |
|------|----------|----------|
| DeepSpeed | ZeRO 显存优化、CPU 卸载 | 大规模预训练、显存受限环境 |
| FSDP | PyTorch 原生分片训练 | 与 PyTorch 生态深度集成 |
| Megatron-LM | 张量并行、3D 并行 | 超大规模模型（100B+） |
| Colossal-AI | Gemini 显存管理、序列并行 | 长序列模型、多样化并行策略 |
| Ray Train | 弹性训练、容错恢复 | 云原生环境、故障容忍场景 |

选择框架需要考虑团队技术栈、模型规模、硬件拓扑。DeepSpeed 易上手且文档丰富，适合快速实验；FSDP 与 PyTorch 无缝集成，适合已有 PyTorch 代码迁移；Megatron-LM 性能最强但工程复杂度高，适合训练千亿级以上参数模型。
