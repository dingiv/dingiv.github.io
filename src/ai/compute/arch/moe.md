---
title: 混合专家
order: 35
---

# 混合专家

MoE（Mixture of Experts，混合专家）是一种通过稀疏激活打破模型参数量与计算量耦合的架构设计。传统的密集模型（dense model）在推理时激活所有参数，而 MoE 模型每个输入只激活部分专家网络，大幅增加模型容量而不增加计算量。

## 原理

Transformer 的 FFN（Feed-Forward Network）层占据了大模型参数量的 2/3。MoE 将 FFN 替换为多个专家网络（experts），每个 expert 是一个独立的 FFN。对于每个 token，一个路由网络（gating network）会选择 Top-k 个专家（通常 k=2），只有这些专家会被激活，其余专家保持休眠。

MoE 的核心优势在于**稀疏激活**。虽然模型参数量增加（有 8 个专家的 MoE 模型参数量是 FFN 的 8 倍），但每个输入只激活 k 个专家，计算量与原 FFN 相当。这使得模型可以大幅增加容量（从 7B 到 50B+）而不增加推理成本。

## 路由机制

路由网络是 MoE 的关键组件，负责决定每个 token 应该发送到哪些专家。简单的路由基于输入的 hidden state 计算每个 expert 的分数，然后选择 Top-k。但这可能导致负载不均衡：某些专家被过度使用（热门专家），某些专家很少被使用（冷门专家）。

Switch Transformer 引入的负载均衡损失（load balance loss）解决这一问题。它鼓励专家被均匀使用：如果某个专家的使用频率偏离平均，模型会受到惩罚。具体来说，负载均衡损失 = $k \times$ expert 使用频率的变异系数，其中 $k$ 是超参数控制损失权重。

稀疏路由（sparse routing）是另一种优化。它不使用 Softmax 选择 Top-k，而是使用稀疏门控（sparse gating），每个 token 只路由到一个专家。这简化了实现，但可能损失模型容量（无法组合多个专家的知识）。

## 负载均衡

MoE 的挑战之一是**负载均衡**。如果路由网络总是选择相同的几个专家，这些专家会成为瓶颈，计算效率下降。更严重的是，如果某些专家很少被训练，它们可能无法学到有用的表示（专家坍塌）。

负载均衡有多种解决方案。Switch Transformer 的专家容量限制（expert capacity factor）限制每个 expert 处理的 token 数量，如果超出容量则丢弃 token 或路由到其他专家。负载均衡损失通过惩罚项鼓励均匀使用专家。

GLaM（GShard MoE）使用专家分组和动态路由来提高负载均衡。所有专家被分为多个组（如每组 4 个），token 先路由到组，组内再路由到具体专家。这降低了路由复杂度（从 $N$ 个专家降低到 $\sqrt{N}$），也提高了负载均衡。

## 专家并行

MoE 的训练需要特殊的并行策略——专家并行（Expert Parallelism）。不同专家可以分布在不同的 GPU 上，每个 GPU 只持有部分专家的参数。当 token 被路由到某个专家时，需要跨 GPU 通信（AllGather）将 token 发送到持有该专家的 GPU。

DeepSpeed-MoE 和 Megatron-MoE 是两大 MoE 训练框架。DeepSpeed-MoE 通过 expert parallelism 结合数据并行，将专家分布到多个 GPU，同时支持 CPU 卸载专家参数（当专家数量超过 GPU 容量时）。Megatron-MoE 结合张量并行和专家并行，在 MoE 模型上实现了 1T 参数的训练。

## 代表模型

| 模型                      | 参数量 | 专家数 | Top-k | 特点                           |
| ------------------------- | ------ | ------ | ----- | ------------------------------ |
| Switch Transformer (2017) | 1.6T   | 64     | 1     | 首次大规模 MoE，负载均衡损失   |
| GLaM (2021)               | 1.2T   | 64     | 1     | GShard 动态路由，专家分组      |
| Mixtral 8x7B (2023)       | 47B    | 8      | 2     | 开源 MoE，性能接近 Llama-2-70B |
| Grok-1 (2023)             | 314B   | 64     | -     | xAI 的超大规模 MoE             |

Mixtral 8x7B 是 Mistral AI 发布的开源 MoE 模型，由 8 个 7B 专家组成。虽然参数量为 47B，但推理时只激活 2 个专家，计算量与 14B 密集模型相当。Mixtral 在多个基准测试中性能接近 Llama-2-70B，证明了 MoE 的有效性。

## 局限性

MoE 的局限性在于训练和推理的复杂性。训练 MoE 需要专门的框架（DeepSpeed-MoE、Megatron-MoE），且需要仔细调优负载均衡和路由策略。推理时需要调度多个专家，增加工程复杂度。

另一个局限是**长序列训练**。MoE 的 KV Cache 需要为每个专家分别存储，显存占用更大。对于长序列（如 128K 上下文），这会成为瓶颈。序列并行的 MoE（Sequence Parallel MoE）是一个研究方向，但目前尚未成熟。

MoE 也可能存在专家塌陷问题：如果某些专家学习到通用的表示，它们会被频繁调用；而某些专家学习到特殊的表示，它们很少被调用。这导致模型容量利用不充分，实际性能可能不如同参数量的密集模型。
