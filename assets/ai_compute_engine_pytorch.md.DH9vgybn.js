import{_ as T,o,c as l,ah as r,j as e,a}from"./chunks/framework.BvDvRtye.js";const A=JSON.parse('{"title":"PyTorch","description":"","frontmatter":{"title":"PyTorch","order":0},"headers":[],"relativePath":"ai/compute/engine/pytorch.md","filePath":"ai/compute/engine/pytorch.md"}'),Q={name:"ai/compute/engine/pytorch.md"},d={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},s={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"0.781ex",height:"1.52ex",role:"img",focusable:"false",viewBox:"0 -661 345 672","aria-hidden":"true"},i={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},n={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.186ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.677ex",height:"1.692ex",role:"img",focusable:"false",viewBox:"0 -666 2067.4 748","aria-hidden":"true"},h={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},p={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.832ex",height:"2.452ex",role:"img",focusable:"false",viewBox:"0 -833.9 2577.6 1083.9","aria-hidden":"true"},c={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},m={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"8.101ex",height:"2.452ex",role:"img",focusable:"false",viewBox:"0 -833.9 3580.6 1083.9","aria-hidden":"true"},x={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},u={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.138ex",height:"1.439ex",role:"img",focusable:"false",viewBox:"0 -442 503 636","aria-hidden":"true"},g={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},w={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.186ex"},xmlns:"http://www.w3.org/2000/svg",width:"9.042ex",height:"1.731ex",role:"img",focusable:"false",viewBox:"0 -683 3996.6 765","aria-hidden":"true"},b={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},y={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.05ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.371ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -683 1048 705","aria-hidden":"true"},P={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},f={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"9.874ex",height:"1.902ex",role:"img",focusable:"false",viewBox:"0 -683 4364.5 840.8","aria-hidden":"true"};function k(_,t,D,L,H,M){return o(),l("div",null,[t[27]||(t[27]=r('<h1 id="pytorch" tabindex="-1">PyTorch <a class="header-anchor" href="#pytorch" aria-label="Permalink to “PyTorch”">​</a></h1><p>从 AI infra 工程师的视角来看，PyTorch 不仅是一个深度学习框架，更是一个复杂的分布式计算系统。它的实现涉及硬件后端抽象、动态计算图的执行、自动微分系统、内存管理、分布式训练等多个子系统。理解这些实现原理，对于开发 AI 引擎、优化模型性能、调试底层问题至关重要。</p><h2 id="硬件后端抽象" tabindex="-1">硬件后端抽象 <a class="header-anchor" href="#硬件后端抽象" aria-label="Permalink to “硬件后端抽象”">​</a></h2><p>PyTorch 的一个精妙设计是 Dispatcher 机制，它实现了多后端的统一抽象。上层 API（如 <code>torch.add</code>）不直接调用 CUDA kernel，而是通过 Dispatcher 分发到具体后端。Dispatcher 根据张量的 device 类型查找对应的 kernel 实现，然后将调用转发过去。</p><p>这种设计使得 PyTorch 可以无缝支持多种硬件平台，开发者无需修改上层代码，只需安装对应的后端包即可。</p><table tabindex="0"><thead><tr><th>平台</th><th>后端</th><th>底层调用</th></tr></thead><tbody><tr><td>NVIDIA GPU</td><td>CUDA</td><td>cuBLAS、cuDNN、TensorRT</td></tr><tr><td>AMD GPU</td><td>ROCm</td><td>hipBLAS、MIOpen</td></tr><tr><td>Apple M 芯片</td><td>MPS</td><td>Metal Compute</td></tr><tr><td>Intel GPU / CPU</td><td>XPU (oneAPI)</td><td>oneDNN</td></tr><tr><td>Huawei Ascend NPU</td><td>Ascend C</td><td>CANN</td></tr><tr><td>Google TPU</td><td>XLA</td><td>HLO / MLIR</td></tr><tr><td>CPU</td><td>Native</td><td>OpenMP / MKL / BLAS</td></tr></tbody></table><h3 id="dispatcher-实现原理" tabindex="-1">Dispatcher 实现原理 <a class="header-anchor" href="#dispatcher-实现原理" aria-label="Permalink to “Dispatcher 实现原理”">​</a></h3><p>Dispatcher 的核心是一个基于类型注册的分发表。每个张量操作（如 <code>torch.add</code>）在初始化时会注册不同设备类型的 kernel 实现。运行时，Dispatcher 根据输入张量的 device 属性查找对应的 kernel，然后通过函数指针直接调用。这种间接调用层的设计使得添加新后端变得简单——只需为新设备注册 kernel 实现即可，无需修改上层 API。</p><p>对于 AI infra 开发者来说，这意味着可以为定制硬件（如国产 AI 芯片）开发 PyTorch 后端。实现一个后端需要定义 DeviceGuard、Allocator、Stream 等接口，然后为每个算子编写 kernel 实现。PyTorch 的 native_functions.yaml 文件定义了所有算子的接口规范，是实现后端的重要参考。</p><h2 id="自动微分系统" tabindex="-1">自动微分系统 <a class="header-anchor" href="#自动微分系统" aria-label="Permalink to “自动微分系统”">​</a></h2><p>PyTorch 的自动微分系统是其最精巧的设计之一。Autograd 的核心思想是：在前向传播时记录操作的依赖关系，在反向传播时自动应用链式法则计算梯度。</p><h3 id="计算图构建" tabindex="-1">计算图构建 <a class="header-anchor" href="#计算图构建" aria-label="Permalink to “计算图构建”">​</a></h3><p>从实现角度看，每个张量都有一个 <code>grad_fn</code> 属性指向创建它的函数。例如 <code>y = x + 2</code> 会创建一个 <code>AddBackward</code> 节点，记录输入 <code>x</code> 和常量 <code>2</code>。当调用 <code>y.backward()</code> 时，PyTorch 会从 <code>y</code> 开始反向遍历计算图，对每个 <code>grad_fn</code> 调用其 <code>backward()</code> 方法，计算梯度并累积到输入张量的 <code>.grad</code> 属性。</p><p>这种设计有几个关键点。梯度累积意味着多次调用 <code>backward()</code> 会让梯度累加，需要手动 <code>zero_grad()</code> 清零，这是实现梯度累积训练的基础。计算图保留机制默认在前向传播后释放图，需要设置 <code>create_graph=True</code> 才能进行高阶微分。原位操作（如 <code>x += 1</code>）会修改原值导致梯度计算错误，Autograd 通过版本检测机制标记这类操作的梯度为无效。</p><h3 id="autograd-引擎" tabindex="-1">Autograd 引擎 <a class="header-anchor" href="#autograd-引擎" aria-label="Permalink to “Autograd 引擎”">​</a></h3><p>Autograd 引擎是一个基于拓扑排序的执行引擎。反向传播时，引擎首先对计算图进行拓扑排序，确保节点按照依赖关系正确执行。然后为每个节点分配计算资源，通过线程池并行执行独立的梯度计算。引擎还支持梯度检查点（gradient checkpointing），通过牺牲计算换显存，只保存部分中间结果，反向传播时重新计算被丢弃的激活值。</p><h2 id="内存管理" tabindex="-1">内存管理 <a class="header-anchor" href="#内存管理" aria-label="Permalink to “内存管理”">​</a></h2><p>PyTorch 的内存管理是性能优化的关键。理解内存分配器的行为，对于解决 OOM 问题、优化显存利用率至关重要。</p><h3 id="缓存分配器" tabindex="-1">缓存分配器 <a class="header-anchor" href="#缓存分配器" aria-label="Permalink to “缓存分配器”">​</a></h3><p>PyTorch 使用缓存分配器（Caching Allocator）来管理 GPU 显存，避免频繁的 malloc/free 开销。当请求分配显存时，分配器优先从缓存中查找合适大小的空闲块；如果没有，则从 CUDA 申请新块。释放时不立即归还给 CUDA，而是放入缓存供后续复用。</p><p>缓存分配器的配置对性能影响很大。<code>max_split_size_mb</code> 限制单个缓存块的最大大小，避免内存碎片；<code>garbage collection threshold</code> 控制何时触发缓存整理；<code>expandable_segments</code> 允许缓存段动态扩展。显存不足时（OOM），可以尝试增加 <code>max_split_size_mb</code> 或启用 <code>expandable_segments</code>。</p><h3 id="内存碎片" tabindex="-1">内存碎片 <a class="header-anchor" href="#内存碎片" aria-label="Permalink to “内存碎片”">​</a></h3><p>内存碎片是导致 OOM 的常见原因。外部碎片由大量小块显存分配导致，缓存分配器通过合并相邻空闲块来缓解。内部碎片由请求大小与实际分配大小不匹配导致，可以通过调整缓存块大小策略来优化。</p><p>PyTorch 提供了 <code>torch.cuda.memory_summary()</code> API 来分析显存使用情况，包括缓存大小、碎片率、分配统计等。对于调试显存问题，这个 API 比 nvidia-smi 更精准。</p><h2 id="分布式训练" tabindex="-1">分布式训练 <a class="header-anchor" href="#分布式训练" aria-label="Permalink to “分布式训练”">​</a></h2><p>PyTorch 提供了多种分布式训练策略，从简单到复杂依次是 DataParallel、DistributedDataParallel (DDP)、FullyShardedDataParallel (FSDP)。</p><h3 id="ddp-实现" tabindex="-1">DDP 实现 <a class="header-anchor" href="#ddp-实现" aria-label="Permalink to “DDP 实现”">​</a></h3><p>DistributedDataParallel 是生产级的分布式方案，每张卡运行独立的进程，通过 AllReduce 同步梯度。DDP 使用高效的通信后端（NCCL、Gloo），支持多机多卡。</p>',28)),e("p",null,[t[4]||(t[4]=a("DDP 的实现涉及两个关键机制。bucket 机制将多个参数的梯度打包成一个 bucket，减少通信次数。梯度累积在反向传播时异步通信，通过通信与计算的重叠（overlap）隐藏通信延迟。具体来说，DDP 在反向传播计算层 ",-1)),e("mjx-container",d,[(o(),l("svg",s,[...t[0]||(t[0]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D456",d:"M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)])])),t[1]||(t[1]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"i")])],-1))]),t[5]||(t[5]=a(" 的梯度时，同时同步层 ",-1)),e("mjx-container",i,[(o(),l("svg",n,[...t[2]||(t[2]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(567.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(1567.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[3]||(t[3]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"i"),e("mo",null,"−"),e("mn",null,"1")])],-1))]),t[6]||(t[6]=a(" 的梯度，将通信开销从串行的 30% 降至并行的 10% 以下。",-1))]),t[28]||(t[28]=r('<h3 id="fsdp-实现" tabindex="-1">FSDP 实现 <a class="header-anchor" href="#fsdp-实现" aria-label="Permalink to “FSDP 实现”">​</a></h3><p>FullyShardedDataParallel 是 PyTorch 2.0 引入的大模型训练方案，借鉴了 DeepSpeed ZeRO-3 的设计。它将参数、梯度、优化器状态全部分片到多张卡，前向传播时通过 AllGather 重建参数，反向传播后立即释放。</p><p>FSDP 的设计更贴近 PyTorch 的模块化哲学。通过 <code>torch.distributed.fsdp.FullyShardedDataParallel</code> 包装模块，自动处理前向传播时的 AllGather 和反向传播后的 ReduceScatter。迁移现有代码非常简单——只需将 <code>nn.DataParallel</code> 替换为 <code>FSDP</code>，无需重构模型定义。</p><p>FSDP 与 <code>torch.compile</code> 深度集成，通过算子融合和通信计算重叠，在保持易用性的同时实现了与 DeepSpeed 相当的性能。FSDP 原生支持 BF16 混合精度训练，BF16 与 FP16 的指数位相同（8 位），但尾数位减少到 7 位，数值范围更大，不需要 loss scaling，对于训练稳定性敏感的大模型是更安全的选择。</p><h2 id="编译优化" tabindex="-1">编译优化 <a class="header-anchor" href="#编译优化" aria-label="Permalink to “编译优化”">​</a></h2><p>PyTorch 2.0 引入的 <code>torch.compile</code> 是近年来最重要的更新之一。它通过编译优化将动态图的性能提升到接近静态图的水平。</p><h3 id="编译流水线" tabindex="-1">编译流水线 <a class="header-anchor" href="#编译流水线" aria-label="Permalink to “编译流水线”">​</a></h3><p><code>torch.compile</code> 的核心组件包括 TorchDynamo、AOTAutograd、PrimTorch、Inductor。TorchDynamo 负责捕获 Python bytecode，将其转换为 FX Graph。它使用字节码分析和 guard 机制，在输入变化时自动重新编译。AOTAutograd 提前执行 autograd，生成前向和反向的计算图，允许跨前向反向的优化。PrimTorch 定义了算子的原语集合，将不同后端的算子统一到一组原语。Inductor 是编译后端，将 FX Graph 编译为高效的 Triton kernel 或 C++ 代码。</p><h3 id="优化技术" tabindex="-1">优化技术 <a class="header-anchor" href="#优化技术" aria-label="Permalink to “优化技术”">​</a></h3><p><code>torch.compile</code> 的收益来自多方面。算子融合减少 kernel 启动开销和显存访问，例如 LayerNorm 后接 Residual 可融合为一个 kernel，只需读写一次显存。内存规划减少中间结果的存储，通过活跃度分析确定变量的生命周期，复用存储空间。常量折叠在编译时计算常量表达式。死代码消除删除无用计算。对于 Transformer 等模型，<code>torch.compile</code> 可带来 30% 以上的性能提升。</p><h2 id="模型并行" tabindex="-1">模型并行 <a class="header-anchor" href="#模型并行" aria-label="Permalink to “模型并行”">​</a></h2><p>当模型太大无法放入单卡显存时，需要使用模型并行。PyTorch 提供了多种模型并行策略。</p><h3 id="序列并行" tabindex="-1">序列并行 <a class="header-anchor" href="#序列并行" aria-label="Permalink to “序列并行”">​</a></h3>',13)),e("p",null,[t[13]||(t[13]=a("对于 Transformer 模型，序列并行是高效的方案。序列并行将序列维度切分到多张卡，每张卡只计算部分序列的 Attention，然后通过 Ring Attention 在环状拓扑上通信。这避免了完整序列的 KV Cache 存储和计算，将显存占用从 ",-1)),e("mjx-container",h,[(o(),l("svg",p,[...t[7]||(t[7]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[8]||(t[8]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"O"),e("mo",{stretchy:"false"},"("),e("msup",null,[e("mi",null,"n"),e("mn",null,"2")]),e("mo",{stretchy:"false"},")")])],-1))]),t[14]||(t[14]=a(" 降至 ",-1)),e("mjx-container",c,[(o(),l("svg",m,[...t[9]||(t[9]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2188.6,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mi" transform="translate(2688.6,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3191.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[10]||(t[10]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"O"),e("mo",{stretchy:"false"},"("),e("msup",null,[e("mi",null,"n"),e("mn",null,"2")]),e("mrow",{"data-mjx-texclass":"ORD"},[e("mo",null,"/")]),e("mi",null,"p"),e("mo",{stretchy:"false"},")")])],-1))]),t[15]||(t[15]=a("（",-1)),e("mjx-container",x,[(o(),l("svg",u,[...t[11]||(t[11]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D45D",d:"M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z",style:{"stroke-width":"3"}})])])],-1)])])),t[12]||(t[12]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"p")])],-1))]),t[16]||(t[16]=a(" 为卡数）。",-1))]),t[29]||(t[29]=e("h3",{id:"张量并行",tabindex:"-1"},[a("张量并行 "),e("a",{class:"header-anchor",href:"#张量并行","aria-label":"Permalink to “张量并行”"},"​")],-1)),e("p",null,[t[23]||(t[23]=a("张量并行是另一种模型并行策略，将单个算子的张量切分到多张卡。例如矩阵乘法 ",-1)),e("mjx-container",g,[(o(),l("svg",w,[...t[17]||(t[17]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1040.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2096.6,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2948.6,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[18]||(t[18]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"Y"),e("mo",null,"="),e("mi",null,"X"),e("mi",null,"W")])],-1))]),t[24]||(t[24]=a("，可将权重 ",-1)),e("mjx-container",b,[(o(),l("svg",y,[...t[19]||(t[19]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D44A",d:"M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z",style:{"stroke-width":"3"}})])])],-1)])])),t[20]||(t[20]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"W")])],-1))]),t[25]||(t[25]=a(" 按列切分，每张卡计算 ",-1)),e("mjx-container",P,[(o(),l("svg",f,[...t[21]||(t[21]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(614,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(1185.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(2241.5,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z" style="stroke-width:3;"></path></g><g data-mml-node="msub" transform="translate(3093.5,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(977,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g></g></g></g>',1)])])),t[22]||(t[22]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("msub",null,[e("mi",null,"Y"),e("mi",null,"i")]),e("mo",null,"="),e("mi",null,"X"),e("msub",null,[e("mi",null,"W"),e("mi",null,"i")])])],-1))]),t[26]||(t[26]=a("，最后拼接结果。张量并行的通信频率高（每个算子都需要通信），因此需要高带宽互联（如 NVLink）才能发挥性能。",-1))]),t[30]||(t[30]=r('<p>PyTorch 的 <code>torch.distributed.tensor</code> 模块提供了原生的张量并行支持，通过 <code>DTensor</code> 抽象简化了张量并行的实现。DTensor 自动处理张量的分片、通信、聚合，使得编写张量并行代码像编写单卡代码一样简单。</p><h2 id="通信后端" tabindex="-1">通信后端 <a class="header-anchor" href="#通信后端" aria-label="Permalink to “通信后端”">​</a></h2><p>PyTorch 的分布式训练依赖高效的通信后端。NCCL (NVIDIA Collective Communications Library) 是 NVIDIA GPU 上的默认后端，针对 NVLink 和 InfiniBand 优化，性能最佳。Gloo 是通用的后端，支持 TCP 和 RDMA，适合非 NVIDIA 场景。MPI 后端适合已有 MPI 集群的 HPC 环境。</p><h3 id="通信原语" tabindex="-1">通信原语 <a class="header-anchor" href="#通信原语" aria-label="Permalink to “通信原语”">​</a></h3><p>通信原语包括点对点通信和集合通信。AllReduce 将所有卡的梯度聚合并分发，是数据并行的核心。AllGather 将分片的参数聚合到每张卡，是 FSDP 前向传播的关键。ReduceScatter 将梯度聚合并分片到不同卡，是 FSDP 反向传播的关键。</p><p>理解这些通信原语的特性，对于优化分布式训练性能至关重要。例如，AllReduce 的带宽需求高但延迟敏感度低，适合通过增加 batch size 来摊薄通信开销；AllGather 的数据量随模型大小线性增长，在超大模型场景下会成为瓶颈。</p><h3 id="processgroup" tabindex="-1">ProcessGroup <a class="header-anchor" href="#processgroup" aria-label="Permalink to “ProcessGroup”">​</a></h3><p>ProcessGroup 是 PyTorch 分布式通信的抽象层，屏蔽了不同通信后端的差异。实现自定义通信后端需要实现 ProcessGroup 接口，包括 <code>broadcast</code>、<code>allreduce</code>、<code>allgather</code> 等方法。这对于适配国产通信库（如华为的 HCCS）或优化特定拓扑（如树形网络）非常有用。</p><h2 id="模型格式与序列化" tabindex="-1">模型格式与序列化 <a class="header-anchor" href="#模型格式与序列化" aria-label="Permalink to “模型格式与序列化”">​</a></h2><p>PyTorch 的原生模型格式是基于 pickle 的序列化格式，文件扩展名通常为 <code>.pt</code>、<code>.pth</code> 或 <code>.pkl</code>。这种格式将 Python 对象（包括张量、模块、字典等）序列化为二进制文件，加载时通过反序列化重建对象。</p><h3 id="state-dict-机制" tabindex="-1">state_dict 机制 <a class="header-anchor" href="#state-dict-机制" aria-label="Permalink to “state_dict 机制”">​</a></h3><p>PyTorch 推荐通过 <code>state_dict()</code> 保存模型参数，而非直接保存整个模型对象。<code>state_dict()</code> 返回一个有序字典，包含所有可学习参数（权重和偏置）以及持久化缓冲区（如 BatchNorm 的 running mean）。这种设计的优势是：只保存参数不保存代码，避免了版本兼容性问题；加载时可以灵活处理参数映射（如加载预训练权重到自定义模型）。</p><h3 id="自定义算子扩展" tabindex="-1">自定义算子扩展 <a class="header-anchor" href="#自定义算子扩展" aria-label="Permalink to “自定义算子扩展”">​</a></h3><p>PyTorch 提供了多层自定义算子扩展机制。最简单的方式是通过 <code>torch.autograd.Function</code> 定义自定义的前向和反向传播，适合快速原型开发。生产级性能需要使用 C++/CUDA 扩展，通过 <code>torch.utils.cpp_extension</code> 加载编译好的共享库。</p><p>PyTorch 2.0 引入了 <code>torch.library</code> API，简化了自定义算子的注册流程。开发者只需定义算子的内核实现和元数据（如类型推断、别名分析），PyTorch 会自动生成对应的 Python 绑定、Autograd 函数、Dispatch Key 分发。这使得为国产 AI 芯片开发算子后端变得更加规范。</p><p>对于需要跨平台部署的场景，TorchScript 是另一种选择。它将 Python 代码编译为中间表示，可以在无 Python 环境的 C++ 程序中加载执行。但 TorchScript 的类型系统较为严格，动态特性支持有限，代码迁移成本较高。</p>',16))])}const C=T(Q,[["render",k]]);export{A as __pageData,C as default};
