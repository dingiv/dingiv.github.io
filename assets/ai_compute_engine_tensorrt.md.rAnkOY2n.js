import{_ as i,o as s,c as e,ah as n}from"./chunks/framework.BwbIerCg.js";const T=JSON.parse('{"title":"TensorRT-LLM","description":"","frontmatter":{"title":"TensorRT-LLM"},"headers":[],"relativePath":"ai/compute/engine/tensorrt.md","filePath":"ai/compute/engine/tensorrt.md"}'),t={name:"ai/compute/engine/tensorrt.md"};function r(l,a,h,p,o,k){return s(),e("div",null,[...a[0]||(a[0]=[n(`<h1 id="tensorrt-llm" tabindex="-1">TensorRT-LLM <a class="header-anchor" href="#tensorrt-llm" aria-label="Permalink to “TensorRT-LLM”">​</a></h1><p>TensorRT-LLM 是 NVIDIA 开发的推理优化框架，通过 TensorRT 的图优化和算子融合能力，将 LLM 推理性能推向极致。它是 NVIDIA 在推理领域对抗 vLLM/TGI 的王牌。</p><h2 id="核心技术" tabindex="-1">核心技术 <a class="header-anchor" href="#核心技术" aria-label="Permalink to “核心技术”">​</a></h2><p>TensorRT-LLM 的核心是 TensorRT，一个深度学习推理优化器。TensorRT 解析 PyTorch/ONNX 模型后，构建计算图，然后进行一系列优化：层融合（LayerNorm + Residual → 单个 kernel）、精度校准（FP32 → FP16/INT8）、内核自动调优（针对不同 GPU 架构选择最优 CUDA kernel）。这些优化将推理延迟降低 15-30%。</p><p>对于 LLM，TensorRT-LLM 额外优化了 Attention 算子。通过 in-place 更新 KV Cache（减少内存分配）、masked softmax 融合（减少 kernel 启动开销）、多头 attention 并行（增加 GPU 占用率），将 Attention 的计算效率提升到接近理论峰值。</p><h2 id="int4-量化" tabindex="-1">INT4 量化 <a class="header-anchor" href="#int4-量化" aria-label="Permalink to “INT4 量化”">​</a></h2><p>TensorRT-LLM 支持 AWQ（Activation-aware Weight Quantization）INT4 量化。AWQ 的核心洞察是：只有 1% 的权重对量化敏感，这些权重保留高精度（FP16），其余 99% 量化为 INT4。这保持了模型精度的同时，将显存占用降低 75%，计算速度提升 2-3 倍（INT4 矩阵乘法比 FP16 快得多）。</p><p>量化过程高度自动化。给定 FP16 模型，TensorRT-LLM 自动计算每层的量化 scale、校准激活值范围、生成量化后的 engine 文件（<code>.engine</code>）。engine 文件是针对特定 GPU 架构（如 A100、H100）编译的二进制，加载后直接执行，无需 JIT 编译，启动速度快。</p><h2 id="in-flight-batching" tabindex="-1">In-flight Batching <a class="header-anchor" href="#in-flight-batching" aria-label="Permalink to “In-flight Batching”">​</a></h2><p>In-flight Batching 是 TensorRT-LLM 的独特优化。当某个序列生成结束时，立即插入新序列，无需等待当前 batch 完成。这与 vLLM 的连续批处理类似，但 TensorRT-LLM 的实现在 CUDA 层面完成，调度开销更低，适合极高并发场景（1000+ 并发请求）。</p><h2 id="使用方式" tabindex="-1">使用方式 <a class="header-anchor" href="#使用方式" aria-label="Permalink to “使用方式”">​</a></h2><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 构建 INT4 量化引擎</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> build.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --model_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama-2-7b</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --quantization</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> int4_awq</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --output_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama-2-7b-int4</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 运行推理</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run.py</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --engine_dir</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> llama-2-7b-int4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --max_output_len</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 512</span></span></code></pre></div><p>TensorRT-LLM 的 API 偏底层，需要手动构建 engine、配置 tokenizer、管理 CUDA stream。这比 vLLM/TGI 的易用性差，但换来的是极致的性能控制力。对于追求极致性能的商业场景，TensorRT-LLM 是不二之选。</p><h2 id="适用场景" tabindex="-1">适用场景 <a class="header-anchor" href="#适用场景" aria-label="Permalink to “适用场景”">​</a></h2><p>TensorRT-LLM 最适合 NVIDIA GPU 架构（A100、H100、L40S）上的高性能推理。对于非 NVIDIA GPU（如 AMD ROCm），TensorRT-LLM 不支持，需要考虑 vLLM 或 TGI。对于 CPU 推理，llama.cpp 是更合适的选择。</p><p>TensorRT-LLM 的性能优势在 H100 上尤为明显，因为 NVIDIA 针对自家的 Transformer Engine（FP8 算子）做了深度优化。在 A100 上，TensorRT-LLM 与 vLLM 性能接近；在 H100 上，TensorRT-LLM 可领先 20-30%。</p>`,16)])])}const c=i(t,[["render",r]]);export{T as __pageData,c as default};
