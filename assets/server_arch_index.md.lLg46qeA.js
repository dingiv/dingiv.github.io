import{_ as r,o as e,c as t,ah as s,bs as n,bt as i,bu as o}from"./chunks/framework.BvDvRtye.js";const m=JSON.parse('{"title":"集群架构","description":"","frontmatter":{"title":"集群架构","order":40},"headers":[],"relativePath":"server/arch/index.md","filePath":"server/arch/index.md"}'),l={name:"server/arch/index.md"};function h(p,a,d,c,g,_){return e(),t("div",null,[...a[0]||(a[0]=[s('<h1 id="web-集群架构" tabindex="-1">Web 集群架构 <a class="header-anchor" href="#web-集群架构" aria-label="Permalink to “Web 集群架构”">​</a></h1><p>服务端面临着巨大的流量压力，而自摩尔定律失效后，单机性能增长缓慢，多线程和分布式技术兴起，架构者转而通过横向扩展的方式增加系统的吞吐能力。</p><p><img src="'+n+'" alt=""></p><h2 id="常见架构" tabindex="-1">常见架构 <a class="header-anchor" href="#常见架构" aria-label="Permalink to “常见架构”">​</a></h2><h3 id="单机架构" tabindex="-1">单机架构 <a class="header-anchor" href="#单机架构" aria-label="Permalink to “单机架构”">​</a></h3><p>基于一个主服务进程的服务器，显著受限于单机的性能和软件的算法实现。而后出现了前后端分离的技术，将客户端的页面渲染和服务端的接口处理进行隔离，初步将一部分的服务器压力转移到了客户端的设备上。典型特征就是，使用 Vue 等前端框架单独编写前端页面，并且托管到静态服务器（Nginx）或者渲染服务器（SSR server）上；客户端下载后，由客户的浏览器等设备进行渲染操作，极大地卸载了服务器的压力。</p><p><img src="'+i+'" alt=""></p><h3 id="多机架构" tabindex="-1">多机架构 <a class="header-anchor" href="#多机架构" aria-label="Permalink to “多机架构”">​</a></h3><p>随着访问量的增加，单机依旧架构无法满足吞吐量的需求，可以开启多个服务器进程，形成多机架构；面向前端时，多机架构引入了负载均衡的策略，使用分发服务器（例如 nginx）对请求进行分发。</p><p><img src="'+o+'" alt=""></p><p>要想实现从单机架构向多机架构的重构，必须做到<strong>主体服务程序的无状态化</strong>。总体思路就是，将主体服务程序中的状态抽离出来，托管到一个独立的外部服务中去，其中，状态分为两种，一种控制面的状态，一种是数据面的状态。控制面状态管理服务被称为<strong>配置中心</strong>，数据面状态服务被称为<strong>数据库</strong>。集群中的服务实例不应该在进程中保存状态，应当通过发送 http 请求或者 RPC 调用向数据管理服务进行获取。</p><p>配置中心负责管理和分发集群中主体服务程序的控制面配置，同时可能还会负责<strong>心跳检测</strong>，集群中的每个服务程序实例必须定期向配置中心发送心跳，或者配置中心向服务实例发送心跳，以此来检测服务实例的可用性。</p><p>数据库负责存储和管理数据面的数据，并实现数据读写的并发控制。数据是整个系统的最上游和交会处，承担着极大的性能压力和并发要求，在此时，数据库成为了系统中新的瓶颈。为此，我们将采用多种手段减少数据库的压力，包括但不限于：缓存技术、分库分表、数据库集群……</p><h3 id="微服务架构" tabindex="-1">微服务架构 <a class="header-anchor" href="#微服务架构" aria-label="Permalink to “微服务架构”">​</a></h3><p>多机架构解决了服务程序的吞吐能力，但是整个集群的瓶颈转移到了数据库上，同时，随着业务的复杂度升高，多机架构将所有的业务逻辑放在同一个程序中，每个业务实例均需要处理全部的业务。为此，提出了微服务架构和分库分表方案。将一个大型的互联网项目拆分成一个个相对独立内聚的小模块，各自管理自己的一个数据库，提供单一的服务功能——由此称之为<strong>微服务</strong>。</p><p>微服务之间可能需要相互依赖，那么在一个服务需要调用另一个服务的功能的时候，它会通过 RPC 调用去访问上游服务，从而获得数据。那么，一个服务如何获知集群中它需要的上游的服务的 IP 呢？它如何确认它所要访问的服务是否还依然存活呢？此时，集群中需要引入一个新的组件用于执行<strong>服务发现</strong>的功能。</p><h4 id="服务发现" tabindex="-1">服务发现 <a class="header-anchor" href="#服务发现" aria-label="Permalink to “服务发现”">​</a></h4><p>在微服务系统中，某套业务代码可以存在多个服务实例，对于服务调用者，这些服务的地址和 IP 是动态的，如果一个下游的服务希望去调用上游服务的 RPC 接口，那么，它需要先到服务发现中心去询问（每个组件一开始只知道一个服务发现中心的地址），某个业务的 IP 地址是多少，在得到了上游接口的地址之后，它才能够发起请求。这个过程类似于 DNS 服务。</p><p>然后是，服务中心如何得知服务的地址？这需要微服务集群中的所有服务实例均遵守微服务的服务发现协议，大家在启动的时候，都要去和服务中心注册自己，让服务中心托管自己的地址信息。并且，服务中心需要每个已注册的服务每过一段时间向服务中心发送心跳，以此来确定某个服务依然可用，否则，就将其可用状态移除。</p><h4 id="配置中心" tabindex="-1">配置中心 <a class="header-anchor" href="#配置中心" aria-label="Permalink to “配置中心”">​</a></h4><p>微服务需要有很多的应用参数配置，微服务在集群中的位置不确定，为了能够方便各个微服务的配置管理，降低运维的难度，同时，实现配置热更新，需要将各个微服务的配置进行集中化管理。其中，负责管理所有的微服务配置的组件就是配置中心。各个组件在启动的时候，可以向配置中心查询配置，并且，订阅服务变更事件，在配置变更之后，配置中心通知每个订阅者，订阅的微服务实现配置热更新。</p><p>一般地，服务发现和配置中心可以合成一个——<strong>服务配置中心</strong>，负责的事情叫做<strong>微服务治理</strong>，如国内最著名的 nacos。服务配置中心是微服务集群架构的最核心的组件。</p><h3 id="云原生架构" tabindex="-1">云原生架构 <a class="header-anchor" href="#云原生架构" aria-label="Permalink to “云原生架构”">​</a></h3><p>Docker 容器的提出无疑是云时代的基石技术，而随后的 k8s <strong>容器编排系统</strong>更是在此基础上的妙笔，Google 将其多年的互联网架构经验凝结成一套可复用的软件基础设施。云原生技术是对于微服务技术的深刻总结和抽象复用，并通过 Linux 容器技术，增加了更多的特性，达到了一石多鸟的效果，包括但不限于：</p><ul><li>容器化部署。使用容器部署，简化了部署流程，让环境配置不再头疼，让依赖管理变得轻松，结合容器镜像能力，让部署流程变得可自动化。</li><li>弹性和可恢复。使用容器编排，批量拉起服务，并在服务宕机时，自动重启。结合互联网的微服务架构，简单拉起同一服务的多个实例。</li><li>分布式下沉。将微服务中的运维管理和分布式协议的功能下沉到云原生管理平台去做，从而让业务逻辑和集群的管理逻辑分离，减轻了开发人员的负担，同时屏蔽了不同语言之间的交互。微服务的服务发现和配置中心组件，直接被云原生平台提供的 <strong>service</strong> 和 <strong>config set</strong> 概念给支持。</li><li>自动化运维。将互联网后台运维进行规范化，并使用 k8s 进行实现，通过 k8s 配置文件进行集群管理。</li><li>新模式开发。开发了函数计算和 Serverless 服务等新型云计算服务。</li></ul>',25)])])}const x=r(l,[["render",h]]);export{m as __pageData,x as default};
