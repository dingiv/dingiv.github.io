import{_ as i,o as a,c as n,ah as e}from"./chunks/framework.BwbIerCg.js";const o=JSON.parse('{"title":"TGI","description":"","frontmatter":{"title":"TGI"},"headers":[],"relativePath":"ai/compute/engine/tgi.md","filePath":"ai/compute/engine/tgi.md"}'),t={name:"ai/compute/engine/tgi.md"};function h(l,s,p,k,r,d){return a(),n("div",null,[...s[0]||(s[0]=[e(`<h1 id="tgi" tabindex="-1">TGI <a class="header-anchor" href="#tgi" aria-label="Permalink to “TGI”">​</a></h1><p>Text Generation Inference (TGI) 是 HuggingFace 开发的生产级推理框架，专为 Transformers 生态模型优化设计。它强调稳定性和可观测性，是 HuggingFace Inference Endpoints 的底层引擎。</p><h2 id="核心特性" tabindex="-1">核心特性 <a class="header-anchor" href="#核心特性" aria-label="Permalink to “核心特性”">​</a></h2><p>TGI 内置了 FlashAttention 实现，通过分块计算减少显存访问，推理速度比标准 Attention 快 2-3 倍。对于长序列（8K+），性能提升更为显著。</p><p>量化支持是 TGI 的强项。它原生支持 BNB（bitsandbytes）、GPTQ、AWQ 等量化格式，加载量化模型只需一行配置。TGI 还提供了详细的量化精度 benchmark，帮助开发者选择合适的量化位宽（INT8、INT4 甚至 NF4）。</p><p>TGI 的可观测性非常完善。内置 Prometheus metrics（请求延迟、吞吐量、显存使用）、日志结构化输出、健康检查接口，这些都是生产环境必需的功能。相比 vLLM 的研究导向，TGI 更注重工程实践。</p><h2 id="使用方式" tabindex="-1">使用方式 <a class="header-anchor" href="#使用方式" aria-label="Permalink to “使用方式”">​</a></h2><div class="language-bash"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">meta-llama/Llama-2-7b</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">volume</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">$PWD</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">/data</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # share a volume with the Docker container to avoid downloading weights every run</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --gpus</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> all</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --shm-size</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 1g</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 8080:80</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -v</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> $volume</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">:/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  ghcr.io/huggingface/text-generation-inference:latest</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --model-id</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> $model </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --quantize</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> bitsandbytes-nf4</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  --max-total-tokens</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 4096</span></span></code></pre></div><p>TGI 以 Docker 容器形式部署，通过参数控制量化、并发、显存限制。这比 vLLM 的 Python API 更适合生产环境，因为容器化部署易于管理和扩展。</p><h2 id="与-vllm-的选择" tabindex="-1">与 vLLM 的选择 <a class="header-anchor" href="#与-vllm-的选择" aria-label="Permalink to “与 vLLM 的选择”">​</a></h2><p>TGI 和 vLLM 是当前最流行的两大推理引擎。TGI 的优势在于稳定性和可观测性，适合企业级部署；vLLM 的优势在于性能和吞吐量，适合高并发场景。HuggingFace 的模型生态系统与 TGI 深度集成，加载 Hub 上的模型、 tokenizer、配置文件都开箱即用。对于非 HuggingFace 格式的模型（如 PyTorch <code>.pt</code> 文件），vLLM 的兼容性更好。</p>`,11)])])}const F=i(t,[["render",h]]);export{o as __pageData,F as default};
