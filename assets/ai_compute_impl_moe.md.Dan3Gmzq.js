import{_ as s,o as a,c as l,ah as r,j as e,a as o}from"./chunks/framework.BvDvRtye.js";const b=JSON.parse('{"title":"混合专家","description":"","frontmatter":{"title":"混合专家","order":35},"headers":[],"relativePath":"ai/compute/impl/moe.md","filePath":"ai/compute/impl/moe.md"}'),n={name:"ai/compute/impl/moe.md"},i={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},d={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"2.939ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 1299 705","aria-hidden":"true"},p={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},Q={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.179ex",height:"1.595ex",role:"img",focusable:"false",viewBox:"0 -694 521 705","aria-hidden":"true"},T={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},m={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"0"},xmlns:"http://www.w3.org/2000/svg",width:"2.009ex",height:"1.545ex",role:"img",focusable:"false",viewBox:"0 -683 888 683","aria-hidden":"true"},h={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},x={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.206ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.939ex",height:"2.398ex",role:"img",focusable:"false",viewBox:"0 -969 1741 1060","aria-hidden":"true"};function w(u,t,g,M,k,c){return a(),l("div",null,[t[14]||(t[14]=r('<h1 id="混合专家" tabindex="-1">混合专家 <a class="header-anchor" href="#混合专家" aria-label="Permalink to “混合专家”">​</a></h1><p>MoE（Mixture of Experts，混合专家）是一种通过稀疏激活打破模型参数量与计算量耦合的架构设计。传统的密集模型（dense model）在推理时激活所有参数，而 MoE 模型每个输入只激活部分专家网络，大幅增加模型容量而不增加计算量。</p><h2 id="原理" tabindex="-1">原理 <a class="header-anchor" href="#原理" aria-label="Permalink to “原理”">​</a></h2><p>Transformer 的 FFN（Feed-Forward Network）层占据了大模型参数量的 2/3。MoE 将 FFN 替换为多个专家网络（experts），每个 expert 是一个独立的 FFN。对于每个 token，一个路由网络（gating network）会选择 Top-k 个专家（通常 k=2），只有这些专家会被激活，其余专家保持休眠。</p><p>MoE 的核心优势在于<strong>稀疏激活</strong>。虽然模型参数量增加（有 8 个专家的 MoE 模型参数量是 FFN 的 8 倍），但每个输入只激活 k 个专家，计算量与原 FFN 相当。这使得模型可以大幅增加容量（从 7B 到 50B+）而不增加推理成本。</p><h2 id="路由机制" tabindex="-1">路由机制 <a class="header-anchor" href="#路由机制" aria-label="Permalink to “路由机制”">​</a></h2><p>路由网络是 MoE 的关键组件，负责决定每个 token 应该发送到哪些专家。简单的路由基于输入的 hidden state 计算每个 expert 的分数，然后选择 Top-k。但这可能导致负载不均衡：某些专家被过度使用（热门专家），某些专家很少被使用（冷门专家）。</p>',7)),e("p",null,[t[4]||(t[4]=o("Switch Transformer 引入的负载均衡损失（load balance loss）解决这一问题。它鼓励专家被均匀使用：如果某个专家的使用频率偏离平均，模型会受到惩罚。具体来说，负载均衡损失 = ",-1)),e("mjx-container",i,[(a(),l("svg",d,[...t[0]||(t[0]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(521,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[1]||(t[1]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"k"),e("mo",null,"×")])],-1))]),t[5]||(t[5]=o(" expert 使用频率的变异系数，其中 ",-1)),e("mjx-container",p,[(a(),l("svg",Q,[...t[2]||(t[2]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D458",d:"M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z",style:{"stroke-width":"3"}})])])],-1)])])),t[3]||(t[3]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"k")])],-1))]),t[6]||(t[6]=o(" 是超参数控制损失权重。",-1))]),t[15]||(t[15]=e("p",null,"稀疏路由（sparse routing）是另一种优化。它不使用 Softmax 选择 Top-k，而是使用稀疏门控（sparse gating），每个 token 只路由到一个专家。这简化了实现，但可能损失模型容量（无法组合多个专家的知识）。",-1)),t[16]||(t[16]=e("h2",{id:"负载均衡",tabindex:"-1"},[o("负载均衡 "),e("a",{class:"header-anchor",href:"#负载均衡","aria-label":"Permalink to “负载均衡”"},"​")],-1)),t[17]||(t[17]=e("p",null,[o("MoE 的挑战之一是"),e("strong",null,"负载均衡"),o("。如果路由网络总是选择相同的几个专家，这些专家会成为瓶颈，计算效率下降。更严重的是，如果某些专家很少被训练，它们可能无法学到有用的表示（专家坍塌）。")],-1)),t[18]||(t[18]=e("p",null,"负载均衡有多种解决方案。Switch Transformer 的专家容量限制（expert capacity factor）限制每个 expert 处理的 token 数量，如果超出容量则丢弃 token 或路由到其他专家。负载均衡损失通过惩罚项鼓励均匀使用专家。",-1)),e("p",null,[t[11]||(t[11]=o("GLaM（GShard MoE）使用专家分组和动态路由来提高负载均衡。所有专家被分为多个组（如每组 4 个），token 先路由到组，组内再路由到具体专家。这降低了路由复杂度（从 ",-1)),e("mjx-container",T,[(a(),l("svg",m,[...t[7]||(t[7]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D441",d:"M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z",style:{"stroke-width":"3"}})])])],-1)])])),t[8]||(t[8]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"N")])],-1))]),t[12]||(t[12]=o(" 个专家降低到 ",-1)),e("mjx-container",h,[(a(),l("svg",x,[...t[9]||(t[9]=[r('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z" style="stroke-width:3;"></path></g></g><g data-mml-node="mo" transform="translate(0,109)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z" style="stroke-width:3;"></path></g><rect width="888" height="60" x="853" y="849"></rect></g></g></g>',1)])])),t[10]||(t[10]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("msqrt",null,[e("mi",null,"N")])])],-1))]),t[13]||(t[13]=o("），也提高了负载均衡。",-1))]),t[19]||(t[19]=r('<h2 id="专家并行" tabindex="-1">专家并行 <a class="header-anchor" href="#专家并行" aria-label="Permalink to “专家并行”">​</a></h2><p>MoE 的训练需要特殊的并行策略——专家并行（Expert Parallelism）。不同专家可以分布在不同的 GPU 上，每个 GPU 只持有部分专家的参数。当 token 被路由到某个专家时，需要跨 GPU 通信（AllGather）将 token 发送到持有该专家的 GPU。</p><p>DeepSpeed-MoE 和 Megatron-MoE 是两大 MoE 训练框架。DeepSpeed-MoE 通过 expert parallelism 结合数据并行，将专家分布到多个 GPU，同时支持 CPU 卸载专家参数（当专家数量超过 GPU 容量时）。Megatron-MoE 结合张量并行和专家并行，在 MoE 模型上实现了 1T 参数的训练。</p><h2 id="代表模型" tabindex="-1">代表模型 <a class="header-anchor" href="#代表模型" aria-label="Permalink to “代表模型”">​</a></h2><table tabindex="0"><thead><tr><th>模型</th><th>参数量</th><th>专家数</th><th>Top-k</th><th>特点</th></tr></thead><tbody><tr><td>Switch Transformer (2017)</td><td>1.6T</td><td>64</td><td>1</td><td>首次大规模 MoE，负载均衡损失</td></tr><tr><td>GLaM (2021)</td><td>1.2T</td><td>64</td><td>1</td><td>GShard 动态路由，专家分组</td></tr><tr><td>Mixtral 8x7B (2023)</td><td>47B</td><td>8</td><td>2</td><td>开源 MoE，性能接近 Llama-2-70B</td></tr><tr><td>Grok-1 (2023)</td><td>314B</td><td>64</td><td>-</td><td>xAI 的超大规模 MoE</td></tr></tbody></table><p>Mixtral 8x7B 是 Mistral AI 发布的开源 MoE 模型，由 8 个 7B 专家组成。虽然参数量为 47B，但推理时只激活 2 个专家，计算量与 14B 密集模型相当。Mixtral 在多个基准测试中性能接近 Llama-2-70B，证明了 MoE 的有效性。</p><h2 id="局限性" tabindex="-1">局限性 <a class="header-anchor" href="#局限性" aria-label="Permalink to “局限性”">​</a></h2><p>MoE 的局限性在于训练和推理的复杂性。训练 MoE 需要专门的框架（DeepSpeed-MoE、Megatron-MoE），且需要仔细调优负载均衡和路由策略。推理时需要调度多个专家，增加工程复杂度。</p><p>另一个局限是<strong>长序列训练</strong>。MoE 的 KV Cache 需要为每个专家分别存储，显存占用更大。对于长序列（如 128K 上下文），这会成为瓶颈。序列并行的 MoE（Sequence Parallel MoE）是一个研究方向，但目前尚未成熟。</p><p>MoE 也可能存在专家塌陷问题：如果某些专家学习到通用的表示，它们会被频繁调用；而某些专家学习到特殊的表示，它们很少被调用。这导致模型容量利用不充分，实际性能可能不如同参数量的密集模型。</p>',10))])}const v=s(n,[["render",w]]);export{b as __pageData,v as default};
