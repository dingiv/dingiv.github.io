import{_ as s,o as a,c as t,ah as e}from"./chunks/framework.BwbIerCg.js";const E=JSON.parse('{"title":"vLLM","description":"","frontmatter":{"title":"vLLM"},"headers":[],"relativePath":"ai/compute/engine/vllm.md","filePath":"ai/compute/engine/vllm.md"}'),n={name:"ai/compute/engine/vllm.md"};function h(l,i,p,k,r,d){return a(),t("div",null,[...i[0]||(i[0]=[e(`<h1 id="vllm" tabindex="-1">vLLM <a class="header-anchor" href="#vllm" aria-label="Permalink to “vLLM”">​</a></h1><p>vLLM 是 UC Berkeley 研发的开源推理引擎，通过 PagedAttention 创新性地解决了 KV Cache 管理问题，成为 2023-2024 年最流行的 LLM 推理框架之一。</p><h2 id="pagedattention" tabindex="-1">PagedAttention <a class="header-anchor" href="#pagedattention" aria-label="Permalink to “PagedAttention”">​</a></h2><p>传统推理引擎将每个请求的 KV Cache 作为连续显存块管理，这带来两个问题：一是预分配困难——无法预测序列长度，预分配过多浪费显存，过少则中断生成；二是内存碎片——不同请求的序列长度差异导致显存无法有效复用。</p><p>PagedAttention 借鉴操作系统的虚拟内存管理，将 KV Cache 分页（page），每页固定大小（如 16 个 token）。每个请求的 KV Cache 是一组页面的链表，页面可分散在显存任意位置。这消除了预分配问题——按需申请页面；也解决了内存碎片——页面大小统一，可自由复用。</p><p>更重要的是，PagedAttention 支持跨请求的 KV Cache 共享。系统 prompt（如&quot;你是一个有用的助手&quot;）在多个请求中完全相同，共享其 KV Cache 可节省大量显存。在多轮对话中，用户 prompt 重复出现时也可共享，这特别适合客服机器人等场景。</p><h2 id="连续批处理" tabindex="-1">连续批处理 <a class="header-anchor" href="#连续批处理" aria-label="Permalink to “连续批处理”">​</a></h2><p>vLLM 的调度器支持 continuous batching，即当 batch 中某个序列生成结束时（遇到 EOS token 或达到长度限制），立即插入新序列，而非等待整个 batch 完成。这充分利用了 GPU 的计算资源，避免了传统静态 batch 中&quot;短序列完成后 GPU 空转&quot;的问题。</p><p>调度器还支持前缀缓存（prefix caching）——重复的 prompt 前缀只计算一次 KV Cache，后续请求直接复用。对于常见的系统 prompt + 用户 prompt 组合，这可将首 token 延迟降低 50% 以上。</p><h2 id="使用方式" tabindex="-1">使用方式 <a class="header-anchor" href="#使用方式" aria-label="Permalink to “使用方式”">​</a></h2><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vllm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> LLM</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, SamplingParams</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">llm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> LLM(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;meta-llama/Llama-2-7b&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">tensor_parallel_size</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">prompts </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hello, my name is&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;The future of AI is&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sampling_params </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SamplingParams(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.8</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">top_p</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.95</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">max_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">outputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> llm.generate(prompts, sampling_params)</span></span></code></pre></div><p>vLLM 兼容 OpenAI API，可通过 <code>vllm serve</code> 命令启动兼容服务，直接替换 OpenAI API 的 base_url 即可。这种兼容性使得从 OpenAI 迁移到自托管模型变得非常简单。</p><h2 id="性能对比" tabindex="-1">性能对比 <a class="header-anchor" href="#性能对比" aria-label="Permalink to “性能对比”">​</a></h2><p>vLLM 官方 benchmark 显示，相比 TGI（Text Generation Inference），vLLM 在吞吐量上提升 2-4 倍，在首 token 延迟上降低 30-50%。这主要归功于 PagedAttention 的高效内存管理和连续批处理的调度优化。但 vLLM 的 GPU 显存占用略高，因为页面元数据（page table）需要额外维护。</p>`,14)])])}const g=s(n,[["render",h]]);export{E as __pageData,g as default};
