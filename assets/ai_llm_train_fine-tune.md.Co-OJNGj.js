import{_ as e,c as t,o as n,ae as a}from"./chunks/framework.CDjunVez.js";const u=JSON.parse('{"title":"微调","description":"","frontmatter":{"title":"微调","order":10},"headers":[],"relativePath":"ai/llm/train/fine-tune.md","filePath":"ai/llm/train/fine-tune.md"}'),i={name:"ai/llm/train/fine-tune.md"};function r(o,p,l,_,d,s){return n(),t("div",null,[...p[0]||(p[0]=[a('<h1 id="大模型微调" tabindex="-1">大模型微调 <a class="header-anchor" href="#大模型微调" aria-label="Permalink to &quot;大模型微调&quot;">​</a></h1><p>二、全参数微调（Full Fine-tuning） 是什么</p><p>更新模型全部参数</p><p>重新塑造模型分布</p><p>特点</p><p>表达能力最强</p><p>代价最大</p><p>灾难性遗忘风险高</p><p>现实使用情况</p><p>❌ 几乎没人对 &gt;30B 模型这么干</p><p>✔ 只在：</p><p>7B 以下</p><p>极其明确的新任务</p><p>数据质量极高</p><p>本质</p><p>重新长一个脑子</p><p>三、参数高效微调（PEFT）——当前主流</p><p>这是你最该关心的部分。</p><p>1️⃣ LoRA（Low-Rank Adaptation） 原理一句话</p><p>不改原权重，只在关键矩阵旁边加一个低秩“偏移量”。</p><p>数学上：</p><p>W&#39; = W + A·B</p><p>优点</p><p>显存友好</p><p>可插拔</p><p>可叠加（理论上）</p><p>典型用途</p><p>行业模型</p><p>风格模型</p><p>角色模型</p><p>工具使用能力增强</p><p>注意事项</p><p>LoRA 是能力记忆</p><p>不是事实记忆</p><p>LoRA 叠多了会冲突</p><p>2️⃣ Adapter / IA³</p><p>Adapter：在层间插小网络</p><p>IA³：只调缩放系数</p><p>特点：</p><p>比 LoRA 稳定</p><p>效果通常略弱</p><p>工程复杂度高</p><p>现实中：</p><p>LoRA 赢麻了</p><p>3️⃣ Prefix / Prompt Tuning</p><p>学一段“可训练的 prompt 向量”</p><p>不改模型结构</p><p>优点：</p><p>参数极少</p><p>快速试验</p><p>缺点：</p><p>表达能力有限</p><p>长期性能不如 LoRA</p><p>四、对齐型微调（Alignment Tuning）</p><p>这是改变“行为边界”的微调。</p><p>1️⃣ SFT（Supervised Fine-Tuning）</p><p>人工标注的「输入 → 理想输出」</p><p>奠定模型“基本人格”</p><p>📌 常见误区</p><p>以为 SFT 是教知识 实际上是教“怎么回答”</p><p>2️⃣ RLHF（Reinforcement Learning from Human Feedback）</p><p>奖励模型 + 强化学习</p><p>改善“主观质量”</p><p>代价：</p><p>贵</p><p>难复现</p><p>工程复杂</p><p>3️⃣ DPO / IPO（当前趋势）</p><p>不跑 RL</p><p>用偏好对直接优化</p><p>优点：</p><p>稳定</p><p>成本低</p><p>效果接近 RLHF</p><p>这是 2024–2025 的主流对齐方式。</p>',75)])])}const c=e(i,[["render",r]]);export{u as __pageData,c as default};
