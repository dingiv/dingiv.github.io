import{_ as t,o as a,c as d,ah as i}from"./chunks/framework.BwbIerCg.js";const s=JSON.parse('{"title":"PCI","description":"","frontmatter":{},"headers":[],"relativePath":"kernel/embed/bus/pci/index.md","filePath":"kernel/embed/bus/pci/index.md"}'),P={name:"kernel/embed/bus/pci/index.md"};function r(p,e,C,n,o,l){return a(),d("div",null,[...e[0]||(e[0]=[i('<h1 id="pci" tabindex="-1">PCI <a class="header-anchor" href="#pci" aria-label="Permalink to “PCI”">​</a></h1><p>PCI 总线协议是现代计算机系统中连接各种高速设备的核心标准。从早期显卡、网卡到现在的 NVMe SSD、GPU 加速卡，PCI 总线承载着系统中绝大部分高速数据传输任务。</p><h2 id="pcie-通道与带宽" tabindex="-1">PCIe 通道与带宽 <a class="header-anchor" href="#pcie-通道与带宽" aria-label="Permalink to “PCIe 通道与带宽”">​</a></h2><p>PCIe 采用点对点的全双工串行传输方式，每个基本的传输单元称为&quot;通道&quot;（Lane）。一条通道由一对发送线和一对接收线组成，可以同时进行双向数据传输。实际应用中，多个通道可以聚合成更高带宽的连接，常见的规格包括 x1（基础设备）、x4（网卡、NVMe SSD）、x8（存储加速卡）和 x16（显卡）。</p><p>每一代 PCIe 标准都在提升单通道的传输速率，下表展示了各版本的性能演进：</p><table tabindex="0"><thead><tr><th>版本</th><th>单通道单向速率</th><th>x16 单向带宽</th><th>主要技术特点</th></tr></thead><tbody><tr><td>PCIe 3.0</td><td>1 GB/s</td><td>16 GB/s</td><td>128b/130b 编码</td></tr><tr><td>PCIe 4.0</td><td>2 GB/s</td><td>32 GB/s</td><td>信号优化，频率翻倍</td></tr><tr><td>PCIe 5.0</td><td>4 GB/s</td><td>64 GB/s</td><td>信号补偿增强</td></tr><tr><td>PCIe 6.0</td><td>8 GB/s</td><td>128 GB/s</td><td>PAM4 调制，前向纠错</td></tr></tbody></table><p>从系统架构角度看，CPU 通过两种方式提供 PCIe 通道：一种是 CPU 直接提供的通道，用于连接显卡、NVMe 等高性能设备；另一种是通过主板芯片组（PCH）转接的通道，用于连接 USB 控制器、网卡、SATA 控制器等相对低速的外设。所有通道最终通过 Root Complex 或 PCIe Switch 进行统一管理和路由。</p><h2 id="pci-架构" tabindex="-1">PCI 架构 <a class="header-anchor" href="#pci-架构" aria-label="Permalink to “PCI 架构”">​</a></h2><p>物理层</p><p>链路层</p><p>实现一个 PCI 设备</p><h2 id="高速互联的演进" tabindex="-1">高速互联的演进 <a class="header-anchor" href="#高速互联的演进" aria-label="Permalink to “高速互联的演进”">​</a></h2><p>随着 AI 计算、大规模数据处理等场景的兴起，传统 PCIe 总线在带宽和延迟方面开始显现瓶颈。工业界因此发展出多种基于 PCIe 物理层或完全重构的高速互联协议。</p><ul><li><p>缓存一致性互联 CXL（Compute Express Link）建立在 PCIe 物理层之上，但重新定义了协议层以支持 CPU 与加速器设备之间的缓存一致性。传统 PCIe 架构下，CPU 和设备各自维护独立的缓存视图，数据共享需要通过软件显式同步，而 CXL 让设备能够直接访问 CPU 的内存地址空间并保持缓存一致性。这种机制在内存池化（Memory Pooling）场景下特别有价值，服务器可以将多个内存模块集中管理，动态分配给不同的计算节点。</p></li><li><p>专用加速互联 NVIDIA 的 NVLink 是为 GPU 间通信专门设计的互联协议，其带宽远超 PCIe。NVLink 4.0 的单 GPU 总带宽可达 900GB/s，是 PCIe 5.0 x16 的十倍以上。在多 GPU 训练场景中，模型参数和梯度需要在 GPU 之间频繁同步，如果走 PCIe 通道会成为明显的瓶颈。NVSwitch 则进一步提供了多 GPU 之间的全互联拓扑，让每个 GPU 都能以满带宽与其他 GPU 通信。</p><p>AMD 的 Infinity Fabric 和 NVIDIA 的 NVLink 定位类似，在 EPYC 服务器和 Instinct 加速卡中承担 CPU 之间、CPU 与 GPU 之间的高速互联任务。这类专用互联通常采用芯片间多维网格拓扑，而非 PCIe 的树形结构，从而降低多跳通信的延迟。</p></li><li><p>芯粒级互联 UCIe（Universal Chiplet Interconnect Express）是较新的标准，面向芯片内部模块（芯粒）之间的互联。随着制程工艺逼近物理极限，把所有功能集成在单块芯片上变得不再经济，芯粒架构允许将不同功能模块用不同工艺制造，然后通过高带宽互联封装在一起。AMD、Intel、TSMC 等厂商都在推动这一标准，未来 SoC 内部的模块化互联可能会部分取代外部 PCIe 的功能。</p></li></ul><h2 id="带宽与延迟的权衡" tabindex="-1">带宽与延迟的权衡 <a class="header-anchor" href="#带宽与延迟的权衡" aria-label="Permalink to “带宽与延迟的权衡”">​</a></h2><p>工程实践中容易过度关注带宽指标，但延迟在某些场景下更为关键。PCIe 采用包传输（packetized protocol），每次通信需要经历请求封装、链路传输、目标解析、响应返回等多个阶段，其典型延迟在几百纳秒到微秒量级，比内存总线高几个数量级。</p><p>对于 GPU 训练中的梯度同步、远程 DMA 操作、共享内存协作这类需要&quot;纳秒级响应&quot;的场景，PCIe 的延迟会直接拖累整体效率。这也是为什么高性能计算集群会采用 NVLink、Infinity Fabric 这类低延迟互联的原因。同样地，在 NVMe SSD 阵列、多网卡负载均衡等场景下，当多个设备同时向 PCIe Root Complex 发起请求时，带宽和延迟都会成为瓶颈，需要在系统设计阶段就做好拓扑规划和流量隔离。</p><p>在实际系统设计中，PCIe 设备的拓扑布局和资源分配会影响最终性能表现。CPU 提供的 PCIe 通道通常连接在集成的内存控制器上，访问系统内存的延迟更低，适合连接对延迟敏感的设备如显卡、NVMe SSD。而通过 PCH 转接的通道虽然延迟稍高，但数量更多，适合连接网卡、USB 控制器等设备。</p><p>多设备并行工作时需要考虑带宽竞争问题。例如同时使用多个 NVMe SSD 时，如果它们连接在同一个 PCIe Root Complex 下，带宽会被分摊。合理的做法是将高带宽设备分散到不同的 Root Complex 或使用 PCIe Switch 进行隔离。在 GPU 集群中，如果训练任务涉及大量的 GPU 间通信，应优先选择支持 NVLink 的 GPU 和拓扑，而不是依赖 PCIe 互联。</p><p>随着 CXL 等新标准的成熟，未来的服务器架构可能会更加灵活。内存可以作为一种可动态分配的资源池，计算节点可以根据需要申请或释放内存，而不受物理插槽数量的限制。这种架构对硬件和软件都提出了新的要求，操作系统需要支持内存的热插拔，应用层也需要适配新的编程模型。</p>',20)])])}const c=t(P,[["render",r]]);export{s as __pageData,c as default};
