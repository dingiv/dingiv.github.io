import{_ as e,c as t,o as P,ae as n}from"./chunks/framework.BQlYxExx.js";const l=JSON.parse('{"title":"PCI","description":"","frontmatter":{},"headers":[],"relativePath":"kernel/embed/bus/pci.md","filePath":"kernel/embed/bus/pci.md"}'),C={name:"kernel/embed/bus/pci.md"};function o(a,p,s,I,i,c){return P(),t("div",null,[...p[0]||(p[0]=[n('<h1 id="pci" tabindex="-1">PCI <a class="header-anchor" href="#pci" aria-label="Permalink to &quot;PCI&quot;">​</a></h1><p>PCI 总线协议是一种广泛用于现代机器上的高速硬件协议。</p><p>二、PCIe 通道（Lane）</p><p>每个 PCIe “lane” 是一对发送线和一对接收线组成的全双工链路。 一个设备可以用多个通道聚合成更高带宽的连接，比如：</p><p>x1：1 条通道（最小）</p><p>x4：4 条通道（常见于网卡、NVMe SSD）</p><p>x8：8 条通道（部分存储或网络加速卡）</p><p>x16：16 条通道（标准显卡插槽）</p><p>每一代 PCIe 都提高带宽：</p><p>版本 每通道单向速率 x16 理论带宽（单向） PCIe 3.0 1 GB/s 16 GB/s PCIe 4.0 2 GB/s 32 GB/s PCIe 5.0 4 GB/s 64 GB/s PCIe 6.0 8 GB/s（PAM4 信号） 128 GB/s</p><p>CPU 直接提供部分 PCIe 通道（直连显卡、NVMe 等高性能设备）。</p><p>主板芯片组（PCH） 提供额外通道，连接低速外设（USB 控制器、网卡、SATA 控制器等）。</p><p>所有通道都通过 PCIe Switch 或 Root Complex 管理。</p><p>CXL（Compute Express Link）</p><p>建立在 PCIe 物理层之上，但协议不同。</p><p>支持 CPU 与 GPU/加速器/内存之间的缓存一致性（cache coherence）。</p><p>Intel、AMD、ARM、NVIDIA、Google 都在推。</p><p>它让多设备共享内存空间，从而减少 PCIe 复制数据的延迟。</p><p>NVLink / NVSwitch（NVIDIA）</p><p>专为 GPU 间通信设计。</p><p>比 PCIe 快几个数量级：NVLink 4.0 单 GPU 总带宽可达 900GB/s。</p><p>用于大型 GPU 集群，比如 DGX、HGX 系列。</p><p>Infinity Fabric（AMD）</p><p>连接 CPU、GPU、I/O 节点的高速互联。</p><p>在 EPYC、Instinct 系列中承担与 NVLink 类似的角色。</p><p>UCIe（Universal Chiplet Interconnect Express）</p><p>新标准，目标是芯粒级（chiplet）互联。</p><p>AMD、Intel、TSMC、Samsung 都在支持，未来趋势是 SoC 内部模块化互联取代外部 PCIe。</p><p>主要因为现代负载的数据需求爆炸性增长：</p><p>AI 模型训练 / 推理 GPU 不仅需要算力，还需要巨量参数与样本在 CPU ↔ GPU 之间流动。 例如训练 GPT-4 级模型时，单轮梯度同步可能涉及数百 GB 数据。PCIe 就成了喉咙。 所以 NVIDIA 改用 NVLink / NVSwitch，让 GPU 间通信达到 TB/s 级。</p><p>高性能存储与缓存系统 NVMe SSD 读写性能已接近 PCIe x4 的极限。多个 SSD 同时工作时，PCIe Root Complex 会饱和。</p><p>多设备并行（多 GPU / 多 FPGA） PCIe 的拓扑通常是树形结构（CPU 根节点 → 多个下行设备），通信要“绕远路”，带宽被分摊，延迟增加。</p><p>内存分离架构（Memory Pooling） 当服务器尝试把内存资源集中管理（通过 CXL 等技术共享），传统 PCIe 无法提供足够低延迟和高一致性支持。</p><p>三、带宽不是唯一问题 —— 延迟也致命</p><p>PCIe 是包传输结构（packetized protocol），每次通信有握手、封装、路由，延迟比内存总线高几个数量级。 对于像 GPU 同步、远程 DMA、共享内存这类需要“纳秒级响应”的场景，这种延迟直接拖垮效率。</p>',35)])])}const G=e(C,[["render",o]]);export{l as __pageData,G as default};
