import{_ as e,o as d,c as a,ah as r,ai as l}from"./chunks/framework.CJUjh4G6.js";const c=JSON.parse('{"title":"算力","description":"","frontmatter":{"title":"算力","order":60},"headers":[],"relativePath":"ai/compute/index.md","filePath":"ai/compute/index.md"}'),i={name:"ai/compute/index.md"};function o(n,t,P,h,p,I){return d(),a("div",null,[...t[0]||(t[0]=[r('<h1 id="ai-算力" tabindex="-1">AI 算力 <a class="header-anchor" href="#ai-算力" aria-label="Permalink to “AI 算力”">​</a></h1><p>目前的 AI 训练需要大量的计算资源，是阻碍 AI 发展的重大绊脚石。</p><h2 id="算力软件栈" tabindex="-1">算力软件栈 <a class="header-anchor" href="#算力软件栈" aria-label="Permalink to “算力软件栈”">​</a></h2><p>从上层应用到硬件的计算资源调用链路如下：</p><ul><li>大模型应用层：大模型部署、调度与记忆管理。</li><li>模型算法层：实现具体的 AI 模型算法，如 Transformer、CNN 等。</li><li>深度学习框架层（PyTorch）：屏蔽底层硬件差异，定义统一的张量算子接口，由各厂商的后端实现具体调用逻辑。</li><li>算子层：封装 PyTorch 的张量操作，调用厂商提供的加速计算 API。算子通常基于特定 DSL 编写，如 NVIDIA 的 CUDA C++。</li><li>加速计算 API 接口层：用户态的硬件加速 DSL，如 CUDA C++，负责将高层语法编译为中间表示（PTX），供驱动程序处理。还有比较新的 OpenAI 的 Triton。</li><li>HAL 层：封装 <code>/dev/nvidia0</code> 等设备节点，提供 <code>libcuda.so</code>（计算）、<code>libnvidia-glcore.so</code>（图形）等闭源动态库，与内核驱动通信。</li><li>内核驱动层：将 PTX 中间码编译为 GPU 机器码（SASS），管理硬件资源。NVIDIA 驱动曾是数百 MB 的巨型二进制，后通过 GSP 架构重构。</li><li>硬件层：NVIDIA GPU、AMD GPU、Google TPU、华为昇腾 NPU 等物理设备。</li></ul><p><img src="'+l+'" alt=""></p><h2 id="硬件加速" tabindex="-1">硬件加速 <a class="header-anchor" href="#硬件加速" aria-label="Permalink to “硬件加速”">​</a></h2><p>使用 GPU 的加速可并行执行的计算任务，目前主要包括俩个领域：图形渲染和科学计算。人工智能领域主要使用科学计算 API 进行加速。</p><p>然而，硬件加速的现状并不乐观，各个硬件厂商纷纷使用自家独立的 GPU API，并且同是自家的 API 同样也被迭代和变更，导致不同的硬件设备的差异直接就被暴露到了应用层。应用层的软件编写者需要直面硬件差异。</p><p>图形加速计算使用的计算栈大体类似，不过略有不同，具体参考图形渲染章节<a href="/client/render/gpu">硬件加速</a>。</p><blockquote><p>OpenCL</p><p>曾经的 GPU 跨平台统一 API，但是随着各家的硬件生态不断割裂，分歧再次扩大，OpenCL 已逐渐退出历史舞台，但仍然被 AMD 和 Intel 所支持，不过性能往往不如各家的专用 API。</p></blockquote><h2 id="加速计算-api" tabindex="-1">加速计算 API <a class="header-anchor" href="#加速计算-api" aria-label="Permalink to “加速计算 API”">​</a></h2><table tabindex="0"><thead><tr><th>厂商</th><th>图形 API</th><th>通用计算 API</th></tr></thead><tbody><tr><td>Apple（苹果）</td><td>Metal Graphics</td><td>Metal Compute（+ Core ML / ANE）</td></tr><tr><td>NVIDIA（英伟达）</td><td>OpenGL / Vulkan / DirectX</td><td>CUDA</td></tr><tr><td>AMD（超微）</td><td>OpenGL / Vulkan / DirectX</td><td>ROCm（Radeon Open Compute）</td></tr><tr><td>Intel</td><td>OpenGL / Vulkan / DirectX</td><td>oneAPI（DPC++ / SYCL）</td></tr></tbody></table><h2 id="torch" tabindex="-1">Torch <a class="header-anchor" href="#torch" aria-label="Permalink to “Torch”">​</a></h2><p>Torch 框架为了使用硬件加速计算，规定各个 GPU 厂商的封装层，将各家的硬件 API 进行屏蔽，从而让上层的数据科学家无需触及糟心而混乱的 GPU 生态，专注于数据训练即可，在调用 torch 的 API 时，torch 将帮助识别当前的硬件环境，使用对应的硬件进行加速，常见的硬件平台包括：</p><table tabindex="0"><thead><tr><th>平台</th><th>后端</th><th>底层调用</th></tr></thead><tbody><tr><td>NVIDIA GPU</td><td>CUDA</td><td>cuBLAS、cuDNN、TensorRT</td></tr><tr><td>AMD GPU</td><td>ROCm</td><td>hipBLAS、MIOpen</td></tr><tr><td>Apple M 芯片</td><td>MPS （Metal Performance Shaders）</td><td>Metal Compute</td></tr><tr><td>Intel GPU / CPU</td><td>XPU （oneAPI）</td><td>oneDNN</td></tr><tr><td>Huawei Ascend NPU</td><td>Ascend C</td><td>CANN</td></tr><tr><td>Google TPU</td><td>XLA</td><td>HLO / MLIR</td></tr><tr><td>CPU</td><td>Native</td><td>OpenMP / MKL / BLAS</td></tr></tbody></table><p>包括国产的华为昇腾 NPU 生态。</p><h2 id="gsp-架构" tabindex="-1">GSP 架构 <a class="header-anchor" href="#gsp-架构" aria-label="Permalink to “GSP 架构”">​</a></h2><p>GSP（GPU System Processor）是 NVIDIA 从 Turing 架构开始在 GPU 芯片上集成的专用 RISC-V 处理器，将原本运行在内核驱动中的复杂逻辑下沉到 GPU 固件中执行，然后将计算语法编译的任务上移到用户态去做。</p><h3 id="背景" tabindex="-1">背景 <a class="header-anchor" href="#背景" aria-label="Permalink to “背景”">​</a></h3><p>早期 NVIDIA 驱动的 <code>.ko</code> 内核模块体积高达数百 MB，原因在于它承担了过多职责：接收 GLSL、HLSL、PTX 等多种上层代码，在内核态完成编译并生成机器码。这种设计带来两个严重问题：</p><ul><li>驱动膨胀：内核模块承载编译器、调度器等复杂逻辑，代码体积难以控制。</li><li>维护困境：闭源驱动与 Linux 内核演进冲突频发，社区无法介入修复。</li></ul><p>GSP 的引入改变了这一局面。内核驱动现在只负责透传命令到 GSP 固件，由固件完成 GPU 初始化、任务调度、PTX 编译等核心工作。这带来以下收益：</p><ul><li>驱动瘦身：内核模块仅保留必要的信令逻辑，体积大幅缩减。</li><li>固件可控：NVIDIA 可通过固件更新修复问题，无需重发驱动。</li><li>开源契机：内核态逻辑简化后，NVIDIA 得以发布开源内核模块（Open Kernel Modules），改善了与 Linux 社区的关系。</li></ul><h3 id="现状" tabindex="-1">现状 <a class="header-anchor" href="#现状" aria-label="Permalink to “现状”">​</a></h3><p>GSP 固件由内核驱动在初始化阶段加载到 GPU 的专用显存区域执行。它接管了显示引擎、电源管理、上下文切换等核心职能，内核驱动通过 RPC 机制与 GSP 通信，提交计算任务和查询状态。</p><p>在 GSP 架构下，编译职责被重新分配：</p><ul><li><strong>GSP 固件</strong>：负责 GPU 硬件初始化、任务调度、电源管理、显示输出控制等底层职能。</li><li><strong>内核驱动</strong>：仅保留设备枚举、内存映射、中断处理等薄层逻辑，透传命令到 GSP。</li><li><strong>用户态运行时</strong>：负责将上层计算语法（CUDA、HLSL、GLSL）编译为中间表示（PTX 或 SPIR-V），再通过 HAL 层接口提交给驱动。</li></ul><p>这种分层使得 NVIDIA 能够开放内核驱动源码（Open Kernel Modules），同时将编译器等复杂逻辑保留在用户态闭源库中。对于上层开发者，NVIDIA 推荐直接通过 HAL 层提交 PTX（计算）或 SPIR-V（图形），而算子语言（如 Triton）和着色器语言则可作为前端 DSL 自由定制，只要最终编译到这些中间表示即可。</p><h3 id="ptx-和-spir-v" tabindex="-1">PTX 和 SPIR-V <a class="header-anchor" href="#ptx-和-spir-v" aria-label="Permalink to “PTX 和 SPIR-V”">​</a></h3><p>PTX（Parallel Thread Execution）是 NVIDIA 的虚拟指令集架构，类似于 Java Bytecode 或 WebAssembly。它以可读的文本/汇编形式存在，核心价值是<strong>跨代兼容</strong>——从 Maxwell 到 Blackwell，GPU 硬件架构差异巨大，PTX 提供了一套稳定的带寄存器抽象的指令，让开发者无需为每代显卡重写代码。GSP 内置的 <code>ptxas</code> 编译器会在运行时将 PTX 即时编译为 GPU 真正执行的机器码 SASS（Streaming Assembly）。</p><p>SPIR-V（Standard Portable Intermediate Representation）则是 Khronos 制定的跨厂商中间表示，服务于 Vulkan 和 OpenCL 生态。与 PTX 的 NVIDIA 专有定位不同，SPIR-V 的目标是<strong>跨硬件兼容</strong>——同一份 SPIR-V 二进制可在 AMD、Intel、NVIDIA 等不同 GPU 上执行。SPIR-V 采用二进制格式而非文本格式，更紧凑但不可直接阅读。</p><table tabindex="0"><thead><tr><th>特性</th><th>PTX</th><th>SPIR-V</th></tr></thead><tbody><tr><td>制定方</td><td>NVIDIA</td><td>Khronos</td></tr><tr><td>格式</td><td>文本/汇编</td><td>二进制</td></tr><tr><td>兼容范围</td><td>仅 NVIDIA GPU</td><td>跨厂商（AMD/Intel 等）</td></tr><tr><td>主要用途</td><td>CUDA 计算</td><td>Vulkan 图形、OpenCL 计算</td></tr><tr><td>运行时编译</td><td>驱动内置 ptxas</td><td>驱动内置 SPIR-V 编译器</td></tr></tbody></table><p>在新架构下，NVIDIA 的 HAL 层同时支持接收 PTX 和 SPIR-V，开发者可根据目标平台选择合适的中间表示。</p>',34)])])}const A=e(i,[["render",o]]);export{c as __pageData,A as default};
