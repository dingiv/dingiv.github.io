import{_ as n,c as s,o as e,ae as t}from"./chunks/framework.CDjunVez.js";const u=JSON.parse('{"title":"NLP","description":"","frontmatter":{"title":"NLP","order":20},"headers":[],"relativePath":"ai/neural/dl/nlp.md","filePath":"ai/neural/dl/nlp.md"}'),i={name:"ai/neural/dl/nlp.md"};function p(l,a,o,r,c,d){return e(),s("div",null,[...a[0]||(a[0]=[t(`<h1 id="自然语言处理" tabindex="-1">自然语言处理 <a class="header-anchor" href="#自然语言处理" aria-label="Permalink to &quot;自然语言处理&quot;">​</a></h1><p>自然语言处理（Natural Language Processing）是人工智能的重要分支，致力于让计算机理解、解释和生成人类语言。NLP 的目标是在计算机和人类语言之间架起桥梁，使机器能够处理和分析大量的自然语言数据。在实际工程中，我们发现 NLP 系统的核心挑战在于如何处理语言的复杂性和歧义性，这与传统计算机视觉任务有着本质区别。</p><h2 id="nlp-的核心挑战" tabindex="-1">NLP 的核心挑战 <a class="header-anchor" href="#nlp-的核心挑战" aria-label="Permalink to &quot;NLP 的核心挑战&quot;">​</a></h2><p>人类语言具有高度的复杂性和歧义性，这给计算机处理带来了诸多挑战：</p><ol><li><p>歧义性：同一个词或句子可能有多种含义</p><ul><li>词汇歧义：&quot;银行&quot;可以指金融机构或河岸</li><li>结构歧义：&quot;我看见了一个人用望远镜&quot; - 谁用望远镜？</li></ul></li><li><p>上下文依赖：词义和句意依赖于上下文</p><ul><li>&quot;这个苹果很好吃&quot;（水果）vs &quot;苹果发布了新产品&quot;（公司）</li></ul></li><li><p>长距离依赖：句子中相距较远的词之间存在语义关联</p><ul><li>&quot;我昨天在超市买的那个苹果，今天吃起来很甜&quot;</li></ul></li><li><p>知识推理：需要常识和背景知识</p><ul><li>&quot;他打开冰箱，发现牛奶过期了&quot; - 需要理解冰箱用来存储食物</li></ul></li><li><p>语言多样性：不同语言、方言、口语表达方式各异</p></li></ol><h2 id="常见-nlp-任务类型" tabindex="-1">常见 NLP 任务类型 <a class="header-anchor" href="#常见-nlp-任务类型" aria-label="Permalink to &quot;常见 NLP 任务类型&quot;">​</a></h2><h3 id="分词" tabindex="-1">分词 <a class="header-anchor" href="#分词" aria-label="Permalink to &quot;分词&quot;">​</a></h3><p>分词（Tokenization）是将连续文本切分成有意义单元的基础步骤，看似简单却直接影响后续所有任务的效果。中文分词的难度在于没有天然的词边界，不像英文用空格分隔单词，需要专门的分词算法如结巴分词、HanLP 来处理。</p><p>工程实践中，子词分词（Subword Tokenization）已成为现代 NLP 的标配。BPE（Byte Pair Encoding）通过统计词频合并最常出现的字符对，WordPiece 被 BERT 采用，而 T5 和 LLaMA 则使用 SentencePiece。这些方法能够有效处理罕见词和未知词，同时保持词表大小的可控性。</p><p>示例：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>原文：自然语言处理很有趣</span></span>
<span class="line"><span>分词：自然 / 语言 / 处理 / 很 / 有趣</span></span>
<span class="line"><span></span></span>
<span class="line"><span>英文：Natural Language Processing is interesting</span></span>
<span class="line"><span>分词：Natural / Language / Processing / is / interesting</span></span></code></pre></div><h3 id="词性标注与命名实体识别" tabindex="-1">词性标注与命名实体识别 <a class="header-anchor" href="#词性标注与命名实体识别" aria-label="Permalink to &quot;词性标注与命名实体识别&quot;">​</a></h3><p>词性标注为每个词标注名词、动词、形容词等语法属性，是句法分析的基础。而命名实体识别（NER）则更具实用价值——它从文本中识别出人名、地名、组织名、时间等实体。工业界常用 BIO 标注格式：B (Begin) 表示实体开始，I (Inside) 表示实体内部，O (Outside) 表示非实体。NER 在信息抽取、知识图谱构建、智能客服等场景中有着广泛的应用。</p><p>示例：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>The/DT cat/NN sits/VBZ on/IN the/DT mat/NN</span></span>
<span class="line"><span>那/DT 只/M 猫/NN 坐/VV 在/P 垫子/NN 上/LC</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>输入：苹果公司的CEO蒂姆·库克在北京发表演讲</span></span>
<span class="line"><span>输出：</span></span>
<span class="line"><span>  苹果公司 -&gt; 组织</span></span>
<span class="line"><span>  蒂姆·库克 -&gt; 人名</span></span>
<span class="line"><span>  北京 -&gt; 地名</span></span></code></pre></div><h3 id="文本分类与相似度计算" tabindex="-1">文本分类与相似度计算 <a class="header-anchor" href="#文本分类与相似度计算" aria-label="Permalink to &quot;文本分类与相似度计算&quot;">​</a></h3><p>文本分类是 NLP 中最常见的任务之一，包括情感分析、主题分类、垃圾邮件检测、意图识别等子任务。从工程角度看，方法经历了从传统的 TF-IDF + 朴素贝叶斯/SVM，到深度学习时代的 CNN、LSTM，再到 BERT 等预训练模型的演进。每个阶段在准确率、推理速度、数据需求上都有不同的权衡。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>文本：&quot;这家餐厅的菜品非常美味，服务也很周到！&quot;</span></span>
<span class="line"><span>分类：正面情感（Positive）</span></span></code></pre></div><p>文本相似度计算则衡量两段文本的语义接近程度。搜索引擎用它来做查询和文档匹配，问答系统用它来找相似的历史问题，推荐系统用它做基于内容的推荐。具体方法包括基于词向量的余弦相似度、衡量字面差异的编辑距离（Levenshtein Distance），以及基于 BERT、Sentence-BERT 的语义相似度计算。</p><h3 id="阅读理解与问答" tabindex="-1">阅读理解与问答 <a class="header-anchor" href="#阅读理解与问答" aria-label="Permalink to &quot;阅读理解与问答&quot;">​</a></h3><p>问答系统可以分为抽取式、生成式和多跳推理三类。SQuAD 数据集推动的抽取式问答要求从原文中定位答案片段，而生成式问答则需要模型理解问题后生成新的答案文本。多跳推理更具挑战性，比如&quot;《哈利波特》的作者的国籍是什么&quot;这个问题，需要先识别作者（J.K.罗琳），再查询她的国籍（英国），这种推理能力在实际的知识问答场景中非常常见。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>上下文：苹果公司成立于1976年，总部位于加利福尼亚州库比蒂诺。</span></span>
<span class="line"><span>问题：苹果公司的总部在哪里？</span></span>
<span class="line"><span>答案：加利福尼亚州库比蒂诺</span></span></code></pre></div><h3 id="自然语言推理" tabindex="-1">自然语言推理 <a class="header-anchor" href="#自然语言推理" aria-label="Permalink to &quot;自然语言推理&quot;">​</a></h3><p>自然语言推理（NLI）判断两个句子之间的逻辑关系——蕴含、矛盾或中性。这个任务看似简单，实际上是衡量模型语言理解能力的重要基准。给定前提&quot;一个女人在喝咖啡&quot;和假设&quot;一个人在喝饮料&quot;，模型需要判断这是蕴含关系。NLI 能力直接影响对话系统、事实核查等下游应用的效果。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>前提：一个女人在喝咖啡</span></span>
<span class="line"><span>假设：一个人在喝饮料</span></span>
<span class="line"><span>关系：蕴含（Entailment）</span></span></code></pre></div><h3 id="机器翻译" tabindex="-1">机器翻译 <a class="header-anchor" href="#机器翻译" aria-label="Permalink to &quot;机器翻译&quot;">​</a></h3><p>机器翻译的发展是 NLP 进步的缩影。早期的基于规则的方法依赖人工编写的翻译规则，扩展性极差。统计机器翻译（SMT）时代基于大规模平行语料统计翻译概率，但需要复杂的特征工程。神经机器翻译（NMT）采用 Seq2Seq + Attention 架构显著提升了翻译质量，而 Transformer 的出现则彻底改变了这一领域——Google Translate 和 DeepL 等产品都基于此架构。实际应用中，歧义处理、文化差异、罕见词翻译和语序流畅性仍然是需要持续优化的方向。</p><h3 id="文本摘要与对话系统" tabindex="-1">文本摘要与对话系统 <a class="header-anchor" href="#文本摘要与对话系统" aria-label="Permalink to &quot;文本摘要与对话系统&quot;">​</a></h3><p>文本摘要分为抽取式和生成式两种。抽取式摘要从原文中选取关键句子组成摘要，实现简单但连贯性较差；生成式摘要能够生成新的总结性文本，流畅性更好但容易出现事实错误。新闻摘要、会议纪要、文献综述是典型的应用场景。</p><p>对话系统则更为复杂。任务导向型对话系统（订票、客服）需要精确理解用户意图并完成指定任务，闲聊型对话系统追求开放域的自然交互，而问答型对话系统则聚焦于准确回答用户问题。一个完整的对话系统包含自然语言理解（NLU）、对话管理（DM）和自然语言生成（NLG）三个核心模块，每个模块都有不同的技术路线和工程挑战。</p><h3 id="文本生成评估" tabindex="-1">文本生成评估 <a class="header-anchor" href="#文本生成评估" aria-label="Permalink to &quot;文本生成评估&quot;">​</a></h3><p>文本生成任务的评估一直是个难题。BLEU 用于机器翻译评估，通过计算 n-gram 匹配度来衡量生成文本与参考文本的相似性。ROUGE 则用于摘要评估，侧重召回率而非精确度。Perplexity（困惑度）是语言模型的传统评估指标，但这些指标与人类主观判断的相关性始终有限——生成&quot;这个苹果是红色的&quot;和&quot;这个苹果是红色的水果&quot;，BLEU 分数可能相同但信息量完全不同，这也是实际工程中需要结合人工评估的原因。</p><h2 id="nlp-神经网络架构演进" tabindex="-1">NLP 神经网络架构演进 <a class="header-anchor" href="#nlp-神经网络架构演进" aria-label="Permalink to &quot;NLP 神经网络架构演进&quot;">​</a></h2><p>深度学习时代之前，NLP 主要依赖词袋模型、TF-IDF、N-gram 等传统方法。这些方法存在明显局限：词袋模型丢失词序信息，TF-IDF 无法捕捉语义，N-gram 则面临维度爆炸和稀疏性问题。</p><p>Word2Vec 在 2013 年的出现是一个转折点。Skip-gram 和 CBOW 模型将词映射到稠密的低维向量空间，首次让机器能够捕捉词的语义关系——&quot;king - man + woman ≈ queen&quot; 这个著名的例子展示了词向量空间的线性结构特性。虽然 Word2Vec 本身是静态词向量（每个词只有唯一表示），但它为后续的上下文相关表示奠定了基础。</p>`,36)])])}const g=n(i,[["render",p]]);export{u as __pageData,g as default};
