import{_ as e,o as a,c as d,ah as n}from"./chunks/framework.BvDvRtye.js";const c=JSON.parse('{"title":"算子","description":"","frontmatter":{"title":"算子","order":20},"headers":[],"relativePath":"ai/compute/kernel/index.md","filePath":"ai/compute/kernel/index.md"}'),r={name:"ai/compute/kernel/index.md"};function i(o,t,h,l,s,p){return a(),d("div",null,[...t[0]||(t[0]=[n('<h1 id="张量计算算子" tabindex="-1">张量计算算子 <a class="header-anchor" href="#张量计算算子" aria-label="Permalink to “张量计算算子”">​</a></h1><p>算子是深度学习模型计算的基本单元，是 AI 引擎性能优化的基础。从矩阵乘法到注意力机制，每个算子的实现效率都会影响模型的整体性能。算子层的优化技术主要包括 FlashAttention、算子融合、量化等，这些技术从不同角度提升计算效率：FlashAttention 减少显存访问，算子融合减少 kernel 启动开销，量化降低显存占用和计算量，优化提升单 kernel 的计算效率。</p><table tabindex="0"><thead><tr><th>优化方向</th><th>核心技术</th><th>适用场景</th><th>详细介绍</th></tr></thead><tbody><tr><td>显存优化</td><td>FlashAttention</td><td>长序列 Attention</td><td><a href="./flashattention">FlashAttention</a></td></tr><tr><td>计算优化</td><td>算子融合</td><td>连续算子合并</td><td><a href="./fusion">算子融合</a></td></tr><tr><td>精度优化</td><td>量化</td><td>显存受限场景</td><td><a href="./quantization">量化</a></td></tr><tr><td>性能分析</td><td>算子优化</td><td>Kernel 优化</td><td><a href="./optimization">算子优化</a></td></tr></tbody></table><p>算子优化贯穿 AI 引擎的整个技术栈。推理引擎（如 vLLM、TGI）依赖 FlashAttention 来优化 Attention 计算；训练引擎（如 DeepSpeed、FSDP）依赖算子融合来减少通信开销；量化技术使得大模型能在显存有限的 GPU 上运行。理解算子层的优化技术，有助于深入理解 AI 引擎的性能瓶颈和优化方向。</p>',4)])])}const f=e(r,[["render",i]]);export{c as __pageData,f as default};
