import{_ as t,o as a,c as i,ah as n}from"./chunks/framework.BvDvRtye.js";const c=JSON.parse('{"title":"模型格式","description":"","frontmatter":{"title":"模型格式","order":50},"headers":[],"relativePath":"ai/compute/format/index.md","filePath":"ai/compute/format/index.md"}'),h={name:"ai/compute/format/index.md"};function e(l,s,p,d,r,k){return a(),i("div",null,[...s[0]||(s[0]=[n(`<h1 id="模型格式" tabindex="-1">模型格式 <a class="header-anchor" href="#模型格式" aria-label="Permalink to “模型格式”">​</a></h1><p>模型格式是 AI 模型的存储和交换标准，定义了模型权重、架构、元数据如何组织和序列化。一个良好的模型格式应具备：<strong>可移植性</strong>（跨平台兼容）、<strong>安全性</strong>（防止恶意代码注入）、<strong>效率</strong>（加载速度快、占用空间小）、<strong>可扩展性</strong>（支持新架构和新特性）。</p><h2 id="各格式对比" tabindex="-1">各格式对比 <a class="header-anchor" href="#各格式对比" aria-label="Permalink to “各格式对比”">​</a></h2><table tabindex="0"><thead><tr><th>格式</th><th>推出方</th><th>特点</th><th>适用场景</th><th>详细介绍</th></tr></thead><tbody><tr><td>PyTorch .pt/.pth</td><td>Meta</td><td>PyTorch 原生格式，支持完整计算图</td><td>PyTorch 训练/推理</td><td>-</td></tr><tr><td>HF 风格</td><td>Hugging Face</td><td>config + safetensors，生态标准</td><td>开源模型分发</td><td><a href="./hf">HF 风格</a></td></tr><tr><td>ONNX</td><td>Microsoft/Facebook</td><td>框架无关，跨平台推理</td><td>跨框架部署</td><td><a href="./onnx">ONNX</a></td></tr><tr><td>GGUF</td><td>llama.cpp</td><td>量化友好，CPU 推理优化</td><td>本地部署、边缘设备</td><td>-</td></tr></tbody></table><h2 id="pytorch-原生格式" tabindex="-1">PyTorch 原生格式 <a class="header-anchor" href="#pytorch-原生格式" aria-label="Permalink to “PyTorch 原生格式”">​</a></h2><p>PyTorch 的原生模型格式包括 <code>.pt</code>、<code>.pth</code>、<code>.pkl</code>（三者本质相同），使用 Python 的 pickle 模块序列化。这种格式可以保存完整的模型对象（包括架构、权重、优化器状态、训练状态），加载后可直接使用，无需重新定义模型结构。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 保存完整模型（包含架构和权重）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.save(model, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model.pth&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 保存仅权重（state_dict）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.save(model.state_dict(), </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;weights.pth&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 加载权重（需要先定义模型结构）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MyModelClass()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.load_state_dict(torch.load(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;weights.pth&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><p>PyTorch 格式的优势是<strong>完整性</strong>和<strong>灵活性</strong>——可以保存任意 Python 对象（包括自定义层、优化器、学习率调度器）。但这也是它的劣势：pickle 格式存在安全风险（反序列化可执行任意代码）、跨版本兼容性差（PyTorch 版本升级可能导致旧模型无法加载）、文件体积大（包含不必要的元数据）。</p><p>生产环境中建议使用 <code>state_dict</code> 方式保存权重，而非保存完整模型。原因有二：一是安全性（避免执行任意代码），二是兼容性（模型结构由代码定义，而非依赖序列化对象）。对于跨平台部署，建议转换为 ONNX 或其他框架无关格式。</p><h2 id="gguf" tabindex="-1">GGUF <a class="header-anchor" href="#gguf" aria-label="Permalink to “GGUF”">​</a></h2><p>GGUF（GPT-Generated Unified Format）是 llama.cpp 推出的模型格式，专为 CPU 推理优化。GGUF 的核心设计是<strong>量化友好</strong>——支持 INT4、INT5、INT8 等多种量化格式，且在加载时自动反量化到 FP16 进行计算。这使得 GGUF 格式的模型占用空间小（7B 模型的 INT4 版本约 4GB），同时保持较好的推理质量。</p><p>GGUF 文件采用二进制格式，包含模型权重、词汇表、量化参数、元数据（如模型名称、训练信息）。与 GGML（llama.cpp 的旧格式）相比，GGUF 改进了内存映射（mmap）支持，允许大模型在内存不足时通过分页加载，降低硬件门槛。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用 llama.cpp 加载 GGUF 模型</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> llama_cpp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Llama</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Llama(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model_path</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;llama-2-7b.Q4_K_M.gguf&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    n_ctx</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2048</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,              </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 上下文长度</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    n_gpu_layers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,         </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># -1 表示将所有层加载到 GPU</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    verbose</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> model(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hello, world!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">max_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">50</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>GGUF 格式在本地部署场景中非常流行。对于没有 NVIDIA GPU 的用户（如 Apple Mac、普通 PC），llama.cpp + GGUF 是运行大模型的主要方式。GGUF 的局限是不适合训练（仅支持推理），且量化后精度有所损失（INT4 的 PPL 通常比 FP16 高 10-20%）。</p><h2 id="格式选择建议" tabindex="-1">格式选择建议 <a class="header-anchor" href="#格式选择建议" aria-label="Permalink to “格式选择建议”">​</a></h2><table tabindex="0"><thead><tr><th>场景</th><th>推荐格式</th><th>理由</th></tr></thead><tbody><tr><td>PyTorch 训练</td><td>state_dict + safetensors</td><td>安全、高效</td></tr><tr><td>模型分发</td><td>HF 风格（config + safetensors）</td><td>生态标准、兼容性强</td></tr><tr><td>本地 CPU 推理</td><td>GGUF</td><td>量化友好、内存占用小</td></tr><tr><td>跨框架部署</td><td>ONNX</td><td>框架无关、硬件支持广</td></tr><tr><td>边缘设备部署</td><td>GGUF 或 TFLite</td><td>量化、低资源优化</td></tr></tbody></table><p>模型格式转换是常见的工程需求。Hugging Face 提供了 <code>transformers</code> 库的转换工具，支持从 PyTorch、TensorFlow、JAX 等格式互转。对于 ONNX 转换，可使用 <code>torch.onnx.export</code> 或 <code>onnxruntime</code> 的转换工具。对于 GGUF 转换，可使用 llama.cpp 的 <code>quantize</code> 工具将 HF 模型转换为 GGUF 格式。</p>`,17)])])}const g=t(h,[["render",e]]);export{c as __pageData,g as default};
