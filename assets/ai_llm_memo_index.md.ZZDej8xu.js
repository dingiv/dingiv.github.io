import{_ as e,o,c as t,ah as l}from"./chunks/framework.B_34VmgG.js";const m=JSON.parse('{"title":"记忆","description":"","frontmatter":{"title":"记忆","order":50},"headers":[],"relativePath":"ai/llm/memo/index.md","filePath":"ai/llm/memo/index.md"}'),i={name:"ai/llm/memo/index.md"};function r(p,a,n,h,d,c){return o(),t("div",null,[...a[0]||(a[0]=[l('<h1 id="模型记忆" tabindex="-1">模型记忆 <a class="header-anchor" href="#模型记忆" aria-label="Permalink to “模型记忆”">​</a></h1><p>现在的大模型（ChatGPT、DeepSeek 等）虽然参数量高达数千亿，但每次对话都是&quot;失忆&quot;的状态——上一次教它的操作、偏好、模板，下一次又得重新教。真正的 AI 必须拥有「记忆」能力。</p><p>模型会失去记忆的本质原因在于大模型的架构设计。大模型可以理解为一个纯函数：接受输入信息，通过神经网络预测相应的输出。预测过程中使用的参数是在训练阶段通过不断迭代更新后固定下来的。部署后的模型一般仅用于推理任务，不再修改参数——因为更新参数相当于重新训练模型，而训练所需的计算资源远大于推理部署。</p><p>目前工程上使用的为大模型添加记忆的技术主要是 Context 工程和 LoRA。</p><h2 id="context-工程" tabindex="-1">Context 工程 <a class="header-anchor" href="#context-工程" aria-label="Permalink to “Context 工程”">​</a></h2><p>Context 工程的核心思想是将记忆信息注入上下文窗口。在用户提出问题时，先检索相关的历史信息、文档或操作记录，将它们附加到 prompt 中，然后在调用模型时一并传入，使其&quot;回忆起&quot;之前的交互内容。</p><h3 id="会话管理" tabindex="-1">会话管理 <a class="header-anchor" href="#会话管理" aria-label="Permalink to “会话管理”">​</a></h3><p>是基本的 Context 工程，也是现代模型应用必备的能力，回忆和概括用户的历史对话、进行偏好补充、预设提示词等等，让用户能够更加方便地和模型进行对话。</p><p>落地简单，实现依赖于外部程序实现和架构设计，不是对模型本身的能力提升。本质是，随身携带历史数据，而不是记忆。</p><p>随着，对话的长度增加，每次 prompt 的体积会迅速增大，模型推理的上下文有大小限制，需要逐渐采用压缩和过滤，防止模型无法处理，减少推理开销。</p><h3 id="rag" tabindex="-1">RAG <a class="header-anchor" href="#rag" aria-label="Permalink to “RAG”">​</a></h3><p>RAG（检索增强生成）每次对话前，将相关文档、历史操作记录、截图描述等从知识库中检索出来，注入到 prompt 中。</p><p>实现方式：</p><ul><li>使用向量数据库（Chroma、FAISS、Qdrant、Milvus）存储记忆</li><li>截图 → 使用 CLIP/IP-Adapter 编码 → 存入向量库</li><li>代码片段、操作日志 → 文本嵌入 → 存入向量库</li></ul><p>落地简单，但受上下文窗口限制（目前最长 200k~1M token），真正可控、稳定的生产系统仍然在 8k~64k 之间，检索和推理成本较高。优化方向，使用模型概括之前的上下文，减少 Context 的体积。</p><h3 id="mcp" tabindex="-1">MCP <a class="header-anchor" href="#mcp" aria-label="Permalink to “MCP”">​</a></h3><p>MCP 是（Model Context Protocol）解决模型调用外部工具的规范协议，由 anthropic 公司提出，MCP 技术引入了模型外部调用的能力，让模型学习使用外部工具，从而扩展模型的能力。同时，MCP 技术也可以作为模型获知外界记忆的一种重要途径。</p><p>实现方式：</p><ul><li>让模型能够接受一个遵循 MCP 协议的格式的 json 字串，模型看见字串后能够返回一个 json 格式的调用命令，外部的 CPU 代码在模型返回时，检测到模型输出了调用请求，执行相应的调用任务后，重新执行一次模型生成，将结果注入到本次生成的 prompt 中。</li></ul><p>是目前模型必备的能力，缺点是需要训练模型使用 MCP，一些模型在训练的时候没有遵循 anthropic 倡导的 MCP。</p><h2 id="lora" tabindex="-1">LoRA <a class="header-anchor" href="#lora" aria-label="Permalink to “LoRA”">​</a></h2><p>LoRA（低秩适配）为大模型插入一个极小的适配器（几 MB 到几百 MB）记忆层，在保持原模型参数不变的情况下学习个性化知识。LoRA 是现在进行模型迁移学习的重要技术，通过模型<code>预训练 -&gt; 微调 -&gt; 部署</code>三部曲，实现模型向垂直领域和个性定制的迁移。同时，LoRA 也是实现模型记忆能力的重要思路。</p><p>实现方式：</p><ol><li>用户每完成一次成功操作 → 收集（截图、操作、结果）三元组</li><li>每积累 20~50 条经验，微调一次 LoRA</li><li>推理时自动加载用户专属 LoRA</li></ol><p>优点：</p><ul><li>训练快速：几分钟到几小时即可完成训练</li><li>存储高效：一个用户一个 LoRA 文件，占用空间小</li><li>灵活切换：同一套基础权重，切换不同 LoRA 即可获得不同的&quot;人格&quot;或&quot;技能&quot;</li></ul><p>LoRA 技术通过在模型中添加轻量级的参数层来保存个性化知识，无需修改原始模型的全部参数。该技术可以让模型拥有新的技能，从能力层面改变模型，而不是让简单地回想起一些数据。但是，开销相较于 Context 工程而言很大，不能适应高频调整，本质是经验蒸馏，而不是回忆。</p><h2 id="可控持续学习" tabindex="-1">可控持续学习 <a class="header-anchor" href="#可控持续学习" aria-label="Permalink to “可控持续学习”">​</a></h2><p>可控持续学习（Offline），让外部程序记录模型的调用日志，并且定期自动训练，自动微调。</p><p>实现方式：</p><ol><li>维护一个经验日志，存储过去成功执行的内容和接受的信息；</li><li>在空闲时（或者&quot;睡眠&quot;时）自动发起训练，继续微调，类似人类睡眠时巩固记忆的过程；</li></ol><p>属于程序架构层面的改变，并且目前没有成功落地示例。</p><h2 id="未来展望" tabindex="-1">未来展望 <a class="header-anchor" href="#未来展望" aria-label="Permalink to “未来展望”">​</a></h2><p>目前的 Context 工程落地简单，但是不是真正改变模型使其记住；LoRA 能够改变模型本身，但是速度太慢，实现复杂。</p><p>真正的记忆必须是在线、持续的，模型&quot;边用边学&quot;。</p><h3 id="snn" tabindex="-1">SNN <a class="header-anchor" href="#snn" aria-label="Permalink to “SNN”">​</a></h3><p>SNN（Spiking Neural Network）脉冲神经网络技术，对目前的神经网络提出架构级的改变，用“脉冲/尖峰”作为信号的神经网络，试图在计算机制层面逼近生物神经元，对模型记忆友好，自带记忆和时间的概念，能够满足动态微调，目前还处于实验室中。</p><h3 id="神经符号融合" tabindex="-1">神经符号融合 <a class="header-anchor" href="#神经符号融合" aria-label="Permalink to “神经符号融合”">​</a></h3><p>融合符号主义 AI，同时解决可解释性和可记忆性的问题，对目前的神经网络提出架构级的改变，让神经网络中融合符号主义 AI 的推理能力，落地十分遥远。</p>',39)])])}const x=e(i,[["render",r]]);export{m as __pageData,x as default};
