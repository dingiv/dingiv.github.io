import{_ as l,c as n,o as t,j as e,a as r}from"./chunks/framework.CDjunVez.js";const f=JSON.parse('{"title":"预训练","description":"","frontmatter":{"title":"预训练","order":0},"headers":[],"relativePath":"ai/llm/pretrain/index.md","filePath":"ai/llm/pretrain/index.md"}'),i={name:"ai/llm/pretrain/index.md"};function o(d,a,s,c,p,m){return t(),n("div",null,[...a[0]||(a[0]=[e("h1",{id:"预训练模型",tabindex:"-1"},[r("预训练模型 "),e("a",{class:"header-anchor",href:"#预训练模型","aria-label":'Permalink to "预训练模型"'},"​")],-1),e("ol",null,[e("li",null,"大批量语料训练，让模型学习几乎整个互联网的文本；"),e("li",null,"监督微调 SFT，人工生成高质量问答模板；"),e("li",null,"强化学习，模型自己和自己玩，自己摸索方案")],-1),e("p",null,"RLHF： reinforcement learning with human feedback 自监督：使用一个 AI 来监督训练另一个 AI",-1),e("p",null,"LLM 训练",-1),e("p",null,"预训练 自监督学习大规模语料，目标如下一 token 预测。2025 年趋势：稀疏 MoE 架构减少计算 30%。 监督微调 使用标注数据对齐任务，如指令跟随。变体包括 DPO（直接偏好优化），取代部分 RLHF 以简化过程。 强化学习 RLHF（人类反馈强化学习）为主流，结合 PPO 算法。2025 更新：合成数据增强反馈循环，降低人类标注成本。",-1)])])}const x=l(i,[["render",o]]);export{f as __pageData,x as default};
