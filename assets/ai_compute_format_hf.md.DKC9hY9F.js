import{_ as i,o as a,c as t,ah as e}from"./chunks/framework.BvDvRtye.js";const g=JSON.parse('{"title":"HF 风格","description":"","frontmatter":{"title":"HF 风格","order":50},"headers":[],"relativePath":"ai/compute/format/hf.md","filePath":"ai/compute/format/hf.md"}'),n={name:"ai/compute/format/hf.md"};function h(l,s,r,d,p,o){return a(),t("div",null,[...s[0]||(s[0]=[e(`<h1 id="hf-风格模型格式" tabindex="-1">HF 风格模型格式 <a class="header-anchor" href="#hf-风格模型格式" aria-label="Permalink to “HF 风格模型格式”">​</a></h1><p>HF 风格模型格式（Hugging Face pretrained model directory format）是由 Hugging Face 在其托管平台上所使用的模型格式规范，它没有一个独立的正式品牌名称（如 ONNX 或 GGUF），而是 transformers 库中 PreTrainedModel 和 PreTrainedConfig 类的标准保存/加载目录规范，<strong>已成为开源大模型生态的事实标准</strong>。</p><p>从系统视角看，HF 风格模型文件夹是一个<strong>&quot;带自描述元数据的二进制发布包&quot;</strong>——包含配置（定义架构）、权重（模型参数）、分词器（文本接口）三类核心文件，通过标准化的目录结构和 JSON 配置实现自描述，使得任何工具都可以解析和加载模型。</p><h2 id="文件夹结构" tabindex="-1">文件夹结构 <a class="header-anchor" href="#文件夹结构" aria-label="Permalink to “文件夹结构”">​</a></h2><p>当你下载一个 Llama-3 或 Qwen 模型时，通常会看到以下文件：</p><h3 id="权重文件-真正的数据" tabindex="-1">权重文件（真正的数据） <a class="header-anchor" href="#权重文件-真正的数据" aria-label="Permalink to “权重文件（真正的数据）”">​</a></h3><table tabindex="0"><thead><tr><th>文件</th><th>说明</th></tr></thead><tbody><tr><td><code>model.safetensors</code></td><td>单文件模型的权重（推荐格式）</td></tr><tr><td><code>model-00001-of-000xx.safetensors</code></td><td>大模型的分片权重（默认每片 50GB）</td></tr><tr><td><code>model.safetensors.index.json</code></td><td>分片索引文件（映射 tensor 名 → 分片文件）</td></tr><tr><td><code>pytorch_model.bin</code></td><td>旧格式（pickle，不推荐）</td></tr></tbody></table><h3 id="配置文件-模型元数据" tabindex="-1">配置文件（模型元数据） <a class="header-anchor" href="#配置文件-模型元数据" aria-label="Permalink to “配置文件（模型元数据）”">​</a></h3><table tabindex="0"><thead><tr><th>文件</th><th>说明</th></tr></thead><tbody><tr><td><code>config.json</code></td><td><strong>最关键的文件</strong>，定义模型架构（层数、隐藏层维度、注意力头数等）。AI 引擎通过读取它来实例化模型类。</td></tr><tr><td><code>generation_config.json</code></td><td>推理默认参数（max_length、temperature、top_p 等）</td></tr><tr><td><code>preprocessor_config.json</code></td><td>多模态模型的预处理配置（如图像/音频处理）</td></tr></tbody></table><h3 id="tokenizer-文件-文本接口" tabindex="-1">Tokenizer 文件（文本接口） <a class="header-anchor" href="#tokenizer-文件-文本接口" aria-label="Permalink to “Tokenizer 文件（文本接口）”">​</a></h3><table tabindex="0"><thead><tr><th>文件</th><th>说明</th></tr></thead><tbody><tr><td><code>tokenizer.json</code></td><td>统一的分词器格式（推荐）</td></tr><tr><td><code>tokenizer_config.json</code></td><td>分词器配置和特殊 token</td></tr><tr><td><code>vocab.json / merges.txt</code></td><td>BPE 分词器的词汇表</td></tr><tr><td><code>tokenizer.model</code></td><td>SentencePiece 二进制词典</td></tr></tbody></table><h3 id="其他文件" tabindex="-1">其他文件 <a class="header-anchor" href="#其他文件" aria-label="Permalink to “其他文件”">​</a></h3><table tabindex="0"><thead><tr><th>文件</th><th>说明</th></tr></thead><tbody><tr><td><code>adapter_config.json</code></td><td>PEFT/LoRA 适配器配置</td></tr><tr><td><code>README.md</code></td><td>模型卡片（模型描述、使用许可）</td></tr></tbody></table><div class="language-py"><button title="Copy Code" class="copy"></button><span class="lang">py</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.save_pretrained(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    save_directory,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    max_shard_size</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;50GB&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,          </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 自动分片阈值</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    safe_serialization</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,        </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 使用 safetensors（默认推荐）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    push_to_hub</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,              </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 是否直接推送到 HF Hub</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    variant</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">None</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">                    # 如 &quot;fp16&quot; → pytorch_model.fp16.bin</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>几乎所有推理引擎（vLLM、TGI、TensorRT-LLM 等）都原生支持或通过少量转换支持 HF 格式。模型作者训练完后用一次 <code>save_pretrained</code>，多个下游工具就能直接加载。HF Hub 上数万模型都遵循这个规范，确保生态兼容。</p><h2 id="safetensors" tabindex="-1">SafeTensors <a class="header-anchor" href="#safetensors" aria-label="Permalink to “SafeTensors”">​</a></h2><p>SafeTensors 是 Hugging Face 推出的安全张量序列化格式，旨在替代 PyTorch 的 pickle 格式。它只保存张量数据（名称 + 形状 + dtype + 字节流），不包含任何可执行代码，彻底杜绝了反序列化攻击的风险。</p><h3 id="文件结构" tabindex="-1">文件结构 <a class="header-anchor" href="#文件结构" aria-label="Permalink to “文件结构”">​</a></h3><p>SafeTensors 采用 <strong>Header + Data</strong> 的二进制结构：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>+------------------+</span></span>
<span class="line"><span>| JSON Length (8B) |  JSON 元数据的长度（无符号整数）</span></span>
<span class="line"><span>+------------------+</span></span>
<span class="line"><span>| JSON Metadata    |  描述每个张量的名称、形状、dtype、偏移量</span></span>
<span class="line"><span>+------------------+</span></span>
<span class="line"><span>| Tensor Data      |  纯二进制张量数据（连续存储）</span></span>
<span class="line"><span>+------------------+</span></span></code></pre></div><p>这种设计使得 AI 引擎可以使用 Linux 的 <code>mmap</code> 系统调用将文件直接映射到地址空间，实现零拷贝加载：</p><div class="language-"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span>1. open 文件</span></span>
<span class="line"><span>2. 读取 Header 获取每个 Tensor 的偏移量</span></span>
<span class="line"><span>3. mmap 数据部分到虚拟内存</span></span>
<span class="line"><span>4. 直接将磁盘数据指针通过 cudaMemcpyAsync 泵入显存</span></span></code></pre></div><h3 id="与旧格式对比" tabindex="-1">与旧格式对比 <a class="header-anchor" href="#与旧格式对比" aria-label="Permalink to “与旧格式对比”">​</a></h3><table tabindex="0"><thead><tr><th>特性</th><th>Pickle (.bin/.pt)</th><th>SafeTensors</th></tr></thead><tbody><tr><td>反序列化速度</td><td>慢（需要 Python 解释器）</td><td>极快（纯磁盘 IO/mmap）</td></tr><tr><td>安全性</td><td>差（可执行任意代码）</td><td>安全（仅包含数据）</td></tr><tr><td>内存开销</td><td>高（加载时有内存拷贝）</td><td>极低（支持零拷贝）</td></tr><tr><td>跨语言支持</td><td>难（强绑定 Python）</td><td>易（C/C++/Rust 均有轻量级解析库）</td></tr></tbody></table><h3 id="使用方式" tabindex="-1">使用方式 <a class="header-anchor" href="#使用方式" aria-label="Permalink to “使用方式”">​</a></h3><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> safetensors.torch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> save_file, load_file</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 保存权重</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">save_file({</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;weight1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: tensor1, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;weight2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: tensor2}, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model.safetensors&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 加载权重</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> load_file(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model.safetensors&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>SafeTensors 已成为 Hugging Face 模型分发的标准格式。自 2023 年起，HF Hub 上新上传的模型默认使用 safetensors 而非 pytorch_model.bin。主流推理引擎（vLLM、TGI、TensorRT-LLM）都优先支持 safetensors 格式。</p><p><strong>Safetensors 是 AI 领域的 ELF 文件格式</strong>——它规范了权重的排布，实现了高性能、高安全性的模型加载。</p><h2 id="transformers-py" tabindex="-1">transformers.py <a class="header-anchor" href="#transformers-py" aria-label="Permalink to “transformers.py”">​</a></h2><p>transformers 是 Hugging Face 推出的大模型加载库，它本质上是一个<strong>模型格式规范</strong>。深度学习框架（如 PyTorch、TensorFlow）提供底层算子和自动微分能力，但如何定义模型架构、如何加载权重、如何做推理，这些都需要开发者自己编写大量样板代码。transformers 库把这些工作标准化——模型定义、权重加载、推理接口全部统一，开发者只需几行代码就能使用预训练模型。</p><p>在 transformers 出现之前，复现一篇论文的模型是极其痛苦的过程。论文作者通常只会发布训练好的权重文件，以及一段可能在特定框架版本上才能运行的代码。模型结构定义分散在各个 GitHub 仓库，API 风格五花八门，权重文件格式互不兼容。想要对比不同模型的效果，需要花费大量时间在环境配置和代码调试上。transformers 库通过统一的接口和规范的模型格式，将模型复现成本从数天降低到数分钟。</p><p>transformers 的核心贡献是把<strong>模型结构、权重、推理接口</strong>三者标准化。开发者不再需要&quot;实现模型&quot;，只需&quot;加载模型&quot;。这种转变的意义在于将模型从&quot;算法&quot;变成了&quot;基础设施&quot;——就像调用 HTTP API 一样简单。</p><h3 id="三件套设计" tabindex="-1">三件套设计 <a class="header-anchor" href="#三件套设计" aria-label="Permalink to “三件套设计”">​</a></h3><p>transformers 库的核心设计思想是将模型抽象为三个独立组件：Config、Tokenizer、Model。</p><table tabindex="0"><thead><tr><th>组件</th><th>职责</th><th>文件来源</th></tr></thead><tbody><tr><td>Config</td><td>存储模型超参数和架构信息（层数、隐藏层维度、注意力头数、词汇表大小）</td><td>config.json</td></tr><tr><td>Tokenizer</td><td>文本与 token 之间的双向转换，存储分词规则</td><td>tokenizer.json、tokenizer_config.json</td></tr><tr><td>Model</td><td>纯粹的神经网络实现，接收 token ID 输出 logits</td><td>基于架构代码实例化</td></tr></tbody></table><p>这三者解耦的设计使得同一个模型结构可以使用不同的预训练权重，同一个 Tokenizer 可以服务于多个模型，同一个模型可以轻松切换不同的任务头。Config 的独立性使得我们可以基于同一个配置初始化多个模型实例，或者修改配置来创建模型变体（如增加层数、改变隐藏层大小）。Tokenizer 的独立性使得我们可以在不重新训练模型的情况下更换分词策略。</p><h3 id="auto-系列" tabindex="-1">Auto 系列 <a class="header-anchor" href="#auto-系列" aria-label="Permalink to “Auto 系列”">​</a></h3><p>Auto 系列类（AutoTokenizer、AutoModel、AutoModelForCausalLM 等）是 transformers 库工程化的集大成体现。传统的做法是需要明确知道使用的是哪种模型架构，然后导入对应的类。但 Auto 系列允许开发者完全不关心模型类型，只需提供模型名称或路径，库会自动推断应该加载哪个类。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer, AutoModelForCausalLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;gpt2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>这种设计的革命性在于将模型选择权交给了权重文件，而不是代码。当你使用 <code>AutoModel</code> 加载一个本地目录时，库会读取目录中的 <code>config.json</code> 文件，根据 <code>model_type</code> 字段自动选择对应的模型类。</p><h3 id="推理与训练" tabindex="-1">推理与训练 <a class="header-anchor" href="#推理与训练" aria-label="Permalink to “推理与训练”">​</a></h3><p>transformers 库的推理接口设计简洁到极致。调用 <code>model.generate()</code> 可以自动处理采样策略、温度参数、top-k/top-p 过滤等细节。对于文本分类、问答、命名实体识别等常见任务，库还提供了 <code>pipeline</code> 高级 API，一行代码就能完成从原始文本到模型输出的全流程。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pipeline</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">classifier </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pipeline(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;sentiment-analysis&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">result </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> classifier(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Hugging Face is amazing!&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 输出: [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9998}]</span></span></code></pre></div><p>训练方面，transformers 提供了 <code>Trainer</code> 类封装了训练循环的样板代码：自动批处理、混合精度训练、梯度累积、学习率调度、日志记录、检查点保存等。开发者只需定义数据集和评估指标，<code>Trainer</code> 会处理其余的工程细节。</p><h3 id="使用示例" tabindex="-1">使用示例 <a class="header-anchor" href="#使用示例" aria-label="Permalink to “使用示例”">​</a></h3><p><strong>从 Hub 加载模型</strong></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> transformers </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer, AutoModelForCausalLM</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;meta-llama/Llama-2-7b-hf&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    torch_dtype</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;auto&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    device_map</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;auto&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    trust_remote_code</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><strong>从本地加载模型</strong></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoModelForCausalLM.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./path/to/model&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> AutoTokenizer.from_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./path/to/model&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p><strong>保存模型</strong></p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model.save_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./my-model&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tokenizer.save_pretrained(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;./my-model&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="hugging-face-生态" tabindex="-1">Hugging Face 生态 <a class="header-anchor" href="#hugging-face-生态" aria-label="Permalink to “Hugging Face 生态”">​</a></h2><p>Hugging Face 起初是一家专注于聊天机器人开发的初创公司，但在 2019 年转型为 AI 开源工具和模型托管平台。如今它已成为大模型时代最重要的基础设施之一，被称为&quot;AI 界的 GitHub&quot;。</p><p>Hugging Face 生态包含多个组件：</p><table tabindex="0"><thead><tr><th>组件</th><th>功能</th></tr></thead><tbody><tr><td>模型托管平台</td><td>类似 GitHub，托管模型权重和代码</td></tr><tr><td>transformers 库</td><td>模型存储、加载和运行的规范</td></tr><tr><td>Datasets 库</td><td>数据加载和预处理</td></tr><tr><td>Evaluate 库</td><td>评估指标统一接口</td></tr><tr><td>Spaces</td><td>演示环境（免费 GPU）</td></tr></tbody></table><p>截至 2024 年，HF Hub 上已有数十万个模型被上传分享，涵盖自然语言处理、计算机视觉、音频处理、多模态等各个领域。</p><h2 id="工程实践" tabindex="-1">工程实践 <a class="header-anchor" href="#工程实践" aria-label="Permalink to “工程实践”">​</a></h2><p>在实际工程中使用 transformers 时，有几个经验值得注意：</p><ol><li><strong>缓存管理</strong>：初次使用时模型会下载到 <code>~/.cache/huggingface</code>，生产环境建议指定 <code>cache_dir</code> 参数</li><li><strong>内存优化</strong>：<code>device_map=&quot;auto&quot;</code> 会自动将模型分层分配到 CPU 和 GPU，超大规模模型可结合 <code>accelerate</code> 库使用模型并行</li><li><strong>Tokenizer 细节</strong>：不同模型的特殊 token 不同（如 BERT 的 <code>[CLS]</code>、GPT 的 <code>&lt;endoftext&gt;</code>），批量推理时设置 <code>padding=True</code> 和 <code>truncation=True</code></li></ol>`,59)])])}const c=i(n,[["render",h]]);export{g as __pageData,c as default};
