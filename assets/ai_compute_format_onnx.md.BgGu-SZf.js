import{_ as i,o as a,c as n,ah as t}from"./chunks/framework.BvDvRtye.js";const E=JSON.parse('{"title":"ONNX","description":"","frontmatter":{"title":"ONNX","order":55},"headers":[],"relativePath":"ai/compute/format/onnx.md","filePath":"ai/compute/format/onnx.md"}'),h={name:"ai/compute/format/onnx.md"};function e(p,s,l,k,r,o){return a(),n("div",null,[...s[0]||(s[0]=[t(`<h1 id="onnx" tabindex="-1">ONNX <a class="header-anchor" href="#onnx" aria-label="Permalink to “ONNX”">​</a></h1><p>ONNX（Open Neural Network Exchange）是 Microsoft 和 Facebook 主推的开放式神经网络交换格式，旨在实现 AI 模型的跨框架互操作性。ONNX 定义了一套标准的算子集合和序列化格式，使得在一个框架（如 PyTorch）中训练的模型可以导出到另一个框架（如 TensorFlow、ONNX Runtime）中进行推理。</p><h2 id="onnx-生态" tabindex="-1">ONNX 生态 <a class="header-anchor" href="#onnx-生态" aria-label="Permalink to “ONNX 生态”">​</a></h2><p>ONNX 生态包含三个核心组件：ONNX 格式（模型序列化）、ONNX Operators（标准算子集）、ONNX Runtime（高性能推理引擎）。ONNX 格式使用 Protocol Buffers 序列化，包含模型结构（计算图）、权重、元数据。ONNX Operators 定义了数百个标准算子（如 Conv、MatMul、Softmax），各推理引擎只需实现这些算子即可运行任何 ONNX 模型。</p><p>ONNX 的核心价值在于<strong>解耦</strong>——训练框架和推理引擎可以独立演进。研究者可以用 PyTorch 快速实验，工程师用 ONNX Runtime 部署到生产环境，企业用 TensorRT 加速到 NVIDIA GPU，医生用 CoreML 部署到 iPhone。这种解耦极大降低了模型迁移成本。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 将 PyTorch 模型导出为 ONNX</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.onnx</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MyModel()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">dummy_input </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.randn(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">224</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">224</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.onnx.export(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    model,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    dummy_input,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;model.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    input_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    output_names</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;output&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    dynamic_axes</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;image&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;batch_size&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;output&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: {</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;batch_size&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}},</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    opset_version</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">17</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><h2 id="onnx-runtime" tabindex="-1">ONNX Runtime <a class="header-anchor" href="#onnx-runtime" aria-label="Permalink to “ONNX Runtime”">​</a></h2><p>ONNX Runtime 是微软开发的高性能推理引擎，支持 CPU、GPU（CUDA、ROCm、TensorRT）、NPU、TPU 等多种硬件后端。ONNX Runtime 的核心优化包括：算子融合（将多个连续算子合并为一个）、图优化（常量折叠、死代码消除）、内存规划（减少内存分配和拷贝）。</p><p>ONNX Runtime 的性能通常优于原生框架。对于 ResNet-50，ONNX Runtime 在 NVIDIA GPU 上的推理性能比 PyTorch 高 1.5-2 倍，比 TensorFlow 高 2-3 倍。这得益于 ONNX Runtime 的针对性优化——它只关注推理，无需考虑训练需求，因此可以更激进地优化计算图。</p><div class="language-python"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark" style="--shiki-light:#24292e;--shiki-dark:#e1e4e8;--shiki-light-bg:#fff;--shiki-dark-bg:#24292e;" tabindex="0" dir="ltr"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> onnxruntime </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ort</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 创建推理 session</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">session </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ort.InferenceSession(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;model.onnx&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">providers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;CUDAExecutionProvider&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 获取输入输出信息</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">input_name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> session.get_inputs()[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].name</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output_name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> session.get_outputs()[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].name</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 推理</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">output </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> session.run([output_name], {input_name: dummy_input.numpy()})</span></span></code></pre></div><h2 id="onnx-算子集版本" tabindex="-1">ONNX 算子集版本 <a class="header-anchor" href="#onnx-算子集版本" aria-label="Permalink to “ONNX 算子集版本”">​</a></h2><p>ONNX 算子集（Opset）是 ONNX 的版本控制机制，每个新版本会增加新算子或修改现有算子的行为。截至 2024 年，ONNX 的最新 Opset 版本是 19。导出 ONNX 模型时，需要选择合适的 Opset 版本。</p><table tabindex="0"><thead><tr><th>Opset 版本</th><th>发布年份</th><th>主要特性</th></tr></thead><tbody><tr><td>Opset 7</td><td>2017</td><td>早期版本，算子有限</td></tr><tr><td>Opset 11</td><td>2019</td><td>支持 dynamic axes、算子标准化</td></tr><tr><td>Opset 13</td><td>2020</td><td>改进 RNN 支持、标准化常量传播</td></tr><tr><td>Opset 17</td><td>2023</td><td>支持 FlashAttention、GPTQ 量化</td></tr><tr><td>Opset 19</td><td>2024</td><td>支持 Stable Diffusion 优化</td></tr></tbody></table><p>选择 Opset 版本时需要权衡：新版本支持更多算子和优化，但旧推理引擎可能不支持。生产环境中建议选择推理引擎支持的最新 Opset 版本，以获得最佳性能。</p><h2 id="onnx-的局限" tabindex="-1">ONNX 的局限 <a class="header-anchor" href="#onnx-的局限" aria-label="Permalink to “ONNX 的局限”">​</a></h2><p>ONNX 的主要挑战在于<strong>算子覆盖不完整</strong>。虽然 ONNX 定义了数百个算子，但各框架的自定义算子（如 PyTorch 的 <code>torch.complex</code>、TensorFlow 的 <code>tf.nn.depthwise_conv2d_native</code>）可能无法直接导出。解决方案包括：使用 ONNX 的自定义算子（Custom Operator）机制、将自定义算子分解为标准算子组合、等待 ONNX 新版本支持。</p><p>另一个挑战是<strong>动态图支持有限</strong>。ONNX 的计算图是静态的，导出时需要固定输入形状（虽然 dynamic axes 可以支持部分维度动态，但并非所有算子都支持）。对于控制流（如 if-else、for loop），ONNX 提供了 If 和 Loop 算子，但 PyTorch 的动态控制流导出时可能失败。</p><h2 id="未来展望" tabindex="-1">未来展望 <a class="header-anchor" href="#未来展望" aria-label="Permalink to “未来展望”">​</a></h2><p>ONNX 的未来发展方向包括：更好的算子覆盖（支持最新的 Transformer 架构，如 Mixture of Experts）、更强的动态图支持（通过 ONNX Script 定义可微分函数）、更丰富的量化支持（INT4、GPTQ、AWQ）。ONNX Runtime 也在向边缘设备扩展（ONNX Runtime Mobile、ONNX Runtime for Microcontrollers），满足 IoT 和嵌入式场景的需求。</p><p>ONNX 的成功在于它是<strong>真正开放的标准</strong>——任何人都可以实现 ONNX Runtime，任何人都可以扩展 ONNX 算子。这与 TensorFlow 的 SavedModel（只能用 TensorFlow 运行）和 PyTorch 的 TorchScript（PyTorch 特有）形成鲜明对比。虽然 ONNX 在学术研究中使用较少（研究者更习惯原生框架），但在工业部署中，ONNX 已成为跨平台推理的事实标准。</p>`,20)])])}const g=i(h,[["render",e]]);export{E as __pageData,g as default};
