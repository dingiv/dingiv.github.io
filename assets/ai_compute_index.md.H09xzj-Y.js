import{_ as e,o as a,c as d,ah as r,ai as l}from"./chunks/framework.DYrKkesV.js";const s=JSON.parse('{"title":"算力","description":"","frontmatter":{"title":"算力","order":60},"headers":[],"relativePath":"ai/compute/index.md","filePath":"ai/compute/index.md"}'),i={name:"ai/compute/index.md"};function o(n,t,p,h,P,c){return a(),d("div",null,[...t[0]||(t[0]=[r('<h1 id="ai-算力" tabindex="-1">AI 算力 <a class="header-anchor" href="#ai-算力" aria-label="Permalink to “AI 算力”">​</a></h1><p>目前的 AI 训练需要大量的计算资源，是阻碍 AI 发展的重大绊脚石。</p><h2 id="结构层次" tabindex="-1">结构层次 <a class="header-anchor" href="#结构层次" aria-label="Permalink to “结构层次”">​</a></h2><p>AI 算力的层次结构如下：</p><ul><li>大模型应用层：大模型部署，调度，记忆</li><li>模型算法层：实现 AI 模型算法</li><li>Pytorch 框架层：屏蔽下层的不同的硬件生态，通过规定张量算子接口，要求下层的后端胶水层来实现这些算子接口，从而让 Pytorch 进行调用；</li><li>算子层：具体负责将一个 Pytorch 张量批量操作进行封装，调用自家生态的硬件加速计算接口进行提交；</li><li>加速计算接口层：用户态硬件加速计算接口，提供科学计算语法，以一个 DSL 语言的形式存在，例如 CUDA 是一个类似于 C++ 的扩展语法，它负责将 CUDA 语言代码转换为驱动程序能够看懂的中间表示语言 PTX；</li><li>系统调用层：由内核实现；</li><li>加速卡驱动层：由各硬件厂商按照操作系统的驱动接口进行实现；针对于 nVidia 的闭源显卡驱动，该驱动负责将 PTX 中间标识码表示成 GPU 能够听懂的机器码；</li><li>硬件层：Nvidia GPU，AMD GPU，Google TPU...</li></ul><p><img src="'+l+'" alt=""></p><blockquote><p>虚拟指令集 PTX (Parallel Thread Execution)，PTX 是 NVIDIA 的虚拟 ISA（指令集架构）。它类似于 Java 的 Bytecode 或 WebAssembly。PTX 还是可读的文本/汇编形式。</p><p>它是为了“兼容性”而存在的。不同代的 N 卡（从 Maxwell 到 Pascal，再到最新的 Blackwell）底层硬件架构差异巨大。PTX 提供了一套稳定的、带寄存器抽象的指令，让开发者（或编译器）不需要为每一代新显卡重写代码。</p><p>原生机器码 SASS (Streaming Assembly)，SASS 是 GPU 硬件真正执行的机器指令。由 N 卡驱动程序内置的编译器（ptxas）在后台执行 JIT（即时编译）。</p></blockquote><h2 id="硬件加速" tabindex="-1">硬件加速 <a class="header-anchor" href="#硬件加速" aria-label="Permalink to “硬件加速”">​</a></h2><p>使用 GPU 的加速可并行执行的计算任务，目前主要包括俩个领域：图形渲染和科学计算。人工智能领域主要使用科学计算 API 进行加速。</p><p>然而，硬件加速的现状并不乐观，各个硬件厂商纷纷使用自家独立的 GPU API，并且同是自家的 API 同样也被迭代和变更，导致不同的硬件设备的差异直接就被暴露到了应用层。应用层的软件编写者需要直面硬件差异。</p><p>图形加速计算使用的计算栈大体类似，不过略有不同，具体参考图形渲染章节<a href="/client/render/gpu">硬件加速</a>。</p><blockquote><p>OpenCL</p><p>曾经的 GPU 跨平台统一 API，但是随着各家的硬件生态不断割裂，分歧再次扩大，OpenCL 已逐渐退出历史舞台，但仍然被 AMD 和 Intel 所支持，不过性能往往不如各家的专用 API。</p></blockquote><h2 id="加速计算-api" tabindex="-1">加速计算 API <a class="header-anchor" href="#加速计算-api" aria-label="Permalink to “加速计算 API”">​</a></h2><table tabindex="0"><thead><tr><th>厂商</th><th>图形 API</th><th>通用计算 API</th></tr></thead><tbody><tr><td>Apple（苹果）</td><td>Metal Graphics</td><td>Metal Compute（+ Core ML / ANE）</td></tr><tr><td>NVIDIA（英伟达）</td><td>OpenGL / Vulkan / DirectX</td><td>CUDA</td></tr><tr><td>AMD（超微）</td><td>OpenGL / Vulkan / DirectX</td><td>ROCm（Radeon Open Compute）</td></tr><tr><td>Intel</td><td>OpenGL / Vulkan / DirectX</td><td>oneAPI（DPC++ / SYCL）</td></tr></tbody></table><h2 id="torch" tabindex="-1">Torch <a class="header-anchor" href="#torch" aria-label="Permalink to “Torch”">​</a></h2><p>Torch 框架为了使用硬件加速计算，规定各个 GPU 厂商的封装层，将各家的硬件 API 进行屏蔽，从而让上层的数据科学家无需触及糟心而混乱的 GPU 生态，专注于数据训练即可，在调用 torch 的 API 时，torch 将帮助识别当前的硬件环境，使用对应的硬件进行加速，常见的硬件平台包括：</p><table tabindex="0"><thead><tr><th>平台</th><th>后端</th><th>底层调用</th></tr></thead><tbody><tr><td>NVIDIA GPU</td><td>CUDA</td><td>cuBLAS、cuDNN、TensorRT</td></tr><tr><td>AMD GPU</td><td>ROCm</td><td>hipBLAS、MIOpen</td></tr><tr><td>Apple M 芯片</td><td>MPS （Metal Performance Shaders）</td><td>Metal Compute</td></tr><tr><td>Intel GPU / CPU</td><td>XPU （oneAPI）</td><td>oneDNN</td></tr><tr><td>Huawei Ascend NPU</td><td>Ascend C</td><td>CANN</td></tr><tr><td>Google TPU</td><td>XLA</td><td>HLO / MLIR</td></tr><tr><td>CPU</td><td>Native</td><td>OpenMP / MKL / BLAS</td></tr></tbody></table><p>包括国产的华为昇腾 NPU 生态。</p>',18)])])}const I=e(i,[["render",o]]);export{s as __pageData,I as default};
