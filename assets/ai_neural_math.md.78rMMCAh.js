import{_ as l,c as i,o as e,ah as t}from"./chunks/framework.C5lh6Kkj.js";const c=JSON.parse('{"title":"数学基础","description":"","frontmatter":{"title":"数学基础","order":0},"headers":[],"relativePath":"ai/neural/math.md","filePath":"ai/neural/math.md"}'),r={name:"ai/neural/math.md"};function o(n,a,s,h,p,u){return e(),i("div",null,[...a[0]||(a[0]=[t('<h1 id="数学基础" tabindex="-1">数学基础 <a class="header-anchor" href="#数学基础" aria-label="Permalink to “数学基础”">​</a></h1><p>人工智能无需对数学知识进行全方位的理论研习，而是需要掌握几个高频使用的知识点和工具模块。实际工程中，我们发现数学更像是一种语言——用于描述模型如何运作、为什么这样设计、以及当效果不好时该如何诊断。本文只列举人工智能中最常用的数学概念。</p><h2 id="线性代数" tabindex="-1">线性代数 <a class="header-anchor" href="#线性代数" aria-label="Permalink to “线性代数”">​</a></h2><p>神经网络构建于张量之上，本质上就是在不断使用线性代数做&quot;空间变换&quot;。输入数据被表示为向量，网络层被表示为矩阵乘法，而整个网络的训练过程就是在一个高维空间中不断寻找最优的数据表示。</p><p>核心概念</p><ul><li>向量、矩阵、张量：是深度学习的基本数据结构；</li><li>矩阵乘法的几何意义：对空间进行旋转、缩放、剪切等线性变换</li><li>转置、逆、伪逆：转置操作用于调整数据维度，逆矩阵用于求解线性方程组，伪逆则在处理非方阵或奇异矩阵时发挥作用</li><li>秩（rank）：秩（rank）衡量矩阵包含的独立信息量，秩亏意味着数据存在冗余或特征之间存在线性相关</li><li>特征值 / 特征向量：特征值和特征向量描述了线性变换的&quot;固有方向&quot;和&quot;强度&quot;</li><li>正交、投影：正交和投影概念在注意力机制中大量应用，Attention 本质上就是 Query 向量在 Key 向量空间上的投影加权和</li><li>范数（L1 / L2）：范数（L1/L2）则用于正则化，防止模型过拟合</li></ul><h2 id="微积分" tabindex="-1">微积分 <a class="header-anchor" href="#微积分" aria-label="Permalink to “微积分”">​</a></h2><p>训练神经网络就是在高维地形上找下坡路。模型训练的本质是通过微积分找到让损失函数最小的参数组合，损失函数最小化是训练的直接目标，无论是分类任务常用的交叉熵损失，还是回归任务的均方误差损失。在实践中，我们使用<strong>反向传播</strong>和<strong>梯度下降</strong>算法来求解。</p><p>核心概念</p><ul><li>导数、偏导、多元函数极值：导数描述函数在某一点的变化率，偏导数则处理多元函数（神经网络的损失函数通常有数百万个参数）；</li><li>梯度、链式法则：指导反向传播和梯度下降的基础；反向传播用于沿着神经网络反向计算出各个连接层之间的梯度，梯度下降使用反向传播计算的结果和<strong>学习率</strong>超参数更新神经网络中的参数</li><li>Jacobian、Hessian：Jacobian 矩阵用于向量值函数，Hessian 矩阵则是梯度的梯度，在二阶优化方法中使用；</li></ul><h3 id="收敛优化" tabindex="-1">收敛优化 <a class="header-anchor" href="#收敛优化" aria-label="Permalink to “收敛优化”">​</a></h3><p>收敛优化直接决定了训练的效率和最终模型的质量。</p><ul><li>凸优化问题有唯一全局最优解，但神经网络是非凸的，存在大量局部最优和鞍点。局部最优与鞍点的区分是在调试训练问题时经常需要考虑。</li><li>学习率过大可能导致训练震荡甚至发散，过小则收敛极慢，训练时间变长</li><li>正则化（L1/L2、Dropout）通过限制模型复杂度来防止过拟合</li></ul><h2 id="概率论" tabindex="-1">概率论 <a class="header-anchor" href="#概率论" aria-label="Permalink to “概率论”">​</a></h2><p>模型不是&quot;知道答案&quot;，而是在&quot;押概率&quot;。神经网络输出的不是确定性结论，而是对各种可能性的概率分布估计。</p><p>核心概念</p><ul><li>随机变量：是概率论的基本研究对象，可以是离散的（如词表中的词）或连续的（如图像像素值）；</li><li>概率分布：常见分布包括高斯分布（Gaussian，用于连续变量的自然假设）、伯努利分布（Bernoulli，二分类问题）、分类分布（Categorical，多分类 softmax 输出）</li><li>条件概率：描述一个事件发生后另一个事件发生的概率，这是理解贝叶斯定理和序列模型的基础；</li><li>贝叶斯定理：描述了如何根据新信息更新先验概率得到后验概率，虽然深度学习很少直接使用贝叶斯方法，但贝叶斯思维在理解正则化、不确定性估计时很有帮助；</li><li>期望、方差：分别是随机变量的中心趋势和离散程度的度量；</li><li>最大似然估计（MLE）：从数据中估计模型参数的标准方法，训练神经网络的过程在很多情况下等价于最大化似然函数；</li><li>KL 散度和交叉熵衡量两个概率分布的差异，交叉熵损失本质上就是最小化预测分布与真实分布之间的 KL 散度；</li></ul><p>模型输出是概率分布，softmax 将 logits 归一化为概率分布，而交叉熵损失函数直接优化这个分布的准确性。不确定性建模在自动驾驶、医疗诊断等安全关键场景中尤为重要，模型不仅需要给出预测，还需要知道自己&quot;不知道什么&quot;。生成模型（如 GAN、VAE、扩散模型）的核心就是学习数据的概率分布，然后从学到的分布中采样生成新样本。</p><h2 id="信息论" tabindex="-1">信息论 <a class="header-anchor" href="#信息论" aria-label="Permalink to “信息论”">​</a></h2><p>训练模型的过程本质上是在压缩世界。信息论为理解和设计机器学习系统提供了独特的视角。</p><ul><li>熵：衡量随机变量的不确定性，也是编码该变量所需的平均最小比特数；</li><li>交叉熵：使用一个分布来编码另一个分布时的平均编码长度；</li><li>KL 散度：则衡量两个分布的差异——可以理解为&quot;用错误的分布编码需要多花的比特数；</li><li>信息增益：描述了知道某个特征后能减少多少不确定性，这是决策树算法的核心；</li></ul><p>分类损失函数通常使用交叉熵，语言模型的训练目标是最小化预测词分布与真实词分布之间的交叉熵，这等价于最大化测试集上的似然。表示学习（Representation Learning）希望学到的编码能够用尽可能少的维度保留尽可能多的信息，这与信息论中的率失真理论直接相关。</p><h2 id="离散数学" tabindex="-1">离散数学 <a class="header-anchor" href="#离散数学" aria-label="Permalink to “离散数学”">​</a></h2><p>虽然现代深度学习主要基于连续优化，但离散数学仍然是理解某些 AI 范式的基础。符号 AI 时代的人工智能完全基于离散数学的规则推理，知识图谱使用图结构表示实体关系，而图神经网络（GNN）则将深度学习扩展到图结构数据上。</p><ul><li>集合论：是数学的基础语言，关系描述对象之间的联系；</li><li>图：则由节点和边组成，广泛用于表示社交网络、分子结构、知识图谱等；</li><li>逻辑推理基础（与、或、非、蕴含）：构建符号 AI 系统，有助于思考模型的可解释性和推理能力；</li></ul>',25)])])}const m=l(r,[["render",o]]);export{c as __pageData,m as default};
