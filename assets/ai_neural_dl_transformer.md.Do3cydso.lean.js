import{_ as l,c as e,o as s,ae as a,j as Q,a as t}from"./chunks/framework.CDjunVez.js";const o="/assets/transformer_arch.BkGQFdnc.png",r="/assets/qkv_calc.Dyg6bmiB.png",n="/assets/GPT_arch.CmvYk1xj.svg",J=JSON.parse('{"title":"Transformer","description":"","frontmatter":{},"headers":[],"relativePath":"ai/neural/dl/transformer.md","filePath":"ai/neural/dl/transformer.md"}'),d={name:"ai/neural/dl/transformer.md"},m={tabindex:"0",class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},i={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.439ex"},xmlns:"http://www.w3.org/2000/svg",width:"38.881ex",height:"2.5ex",role:"img",focusable:"false",viewBox:"0 -910.8 17185.3 1104.8","aria-hidden":"true"},h={tabindex:"0",class:"MathJax",jax:"SVG",display:"true",style:{direction:"ltr",display:"block","text-align":"center",margin:"1em 0",position:"relative"}},p={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-2.308ex"},xmlns:"http://www.w3.org/2000/svg",width:"41.428ex",height:"5.741ex",role:"img",focusable:"false",viewBox:"0 -1517.7 18311.4 2537.7","aria-hidden":"true"},H={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},g={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.372ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.128ex",height:"2.398ex",role:"img",focusable:"false",viewBox:"0 -895.6 1824.4 1060","aria-hidden":"true"},u={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},V={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.372ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.128ex",height:"2.398ex",role:"img",focusable:"false",viewBox:"0 -895.6 1824.4 1060","aria-hidden":"true"},w={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},x={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.372ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.128ex",height:"2.398ex",role:"img",focusable:"false",viewBox:"0 -895.6 1824.4 1060","aria-hidden":"true"},L={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},f={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"17.794ex",height:"2.655ex",role:"img",focusable:"false",viewBox:"0 -923.4 7864.8 1173.4","aria-hidden":"true"},c={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},M={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"18.042ex",height:"2.655ex",role:"img",focusable:"false",viewBox:"0 -923.4 7974.8 1173.4","aria-hidden":"true"},y={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},k={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"28.725ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 12696.4 1000","aria-hidden":"true"},Z={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"28.725ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 12696.4 1000","aria-hidden":"true"},_={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},v={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.357ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.599ex",height:"1.927ex",role:"img",focusable:"false",viewBox:"0 -694 2474.7 851.8","aria-hidden":"true"},q={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},D={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.667ex"},xmlns:"http://www.w3.org/2000/svg",width:"3.124ex",height:"2.237ex",role:"img",focusable:"false",viewBox:"0 -694 1380.8 989","aria-hidden":"true"},A={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},C={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"7.616ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 3366.4 1000","aria-hidden":"true"},S={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},P={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"4.688ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 2072 1000","aria-hidden":"true"},N={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},j={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.832ex",height:"2.452ex",role:"img",focusable:"false",viewBox:"0 -833.9 2577.6 1083.9","aria-hidden":"true"},R={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},E={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"5.832ex",height:"2.452ex",role:"img",focusable:"false",viewBox:"0 -833.9 2577.6 1083.9","aria-hidden":"true"};function F(I,T,B,G,O,K){return s(),e("div",null,[T[52]||(T[52]=a("",16)),Q("mjx-container",m,[(s(),e("svg",i,[...T[0]||(T[0]=[a("",1)])])),T[1]||(T[1]=Q("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[Q("mi",null,"Q"),Q("mo",null,"="),Q("mi",null,"X"),Q("msup",null,[Q("mi",null,"W"),Q("mi",null,"Q")]),Q("mo",null,","),Q("mstyle",{scriptlevel:"0"},[Q("mspace",{width:"1em"})]),Q("mi",null,"K"),Q("mo",null,"="),Q("mi",null,"X"),Q("msup",null,[Q("mi",null,"W"),Q("mi",null,"K")]),Q("mo",null,","),Q("mstyle",{scriptlevel:"0"},[Q("mspace",{width:"1em"})]),Q("mi",null,"V"),Q("mo",null,"="),Q("mi",null,"X"),Q("msup",null,[Q("mi",null,"W"),Q("mi",null,"V")])])],-1))]),T[53]||(T[53]=Q("p",null,[Q("img",{src:r,alt:""})],-1)),T[54]||(T[54]=Q("p",null,[t("Q K V 都是通过输入的词向量和各自的参数矩阵点乘得到，初期这三者的参数矩阵均为随机初始化。符合"),Q("strong",null,"自注意"),t("中的"),Q("strong",null,"自"),t("的概念。")],-1)),T[55]||(T[55]=Q("p",null,"注意力计算的核心公式是：",-1)),Q("mjx-container",h,[(s(),e("svg",p,[...T[2]||(T[2]=[a("",1)])])),T[3]||(T[3]=Q("mjx-assistive-mml",{unselectable:"on",display:"block",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",overflow:"hidden",width:"100%"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[Q("mtext",null,"Attention"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"Q"),Q("mo",null,","),Q("mi",null,"K"),Q("mo",null,","),Q("mi",null,"V"),Q("mo",{stretchy:"false"},")"),Q("mo",null,"="),Q("mtext",null,"softmax"),Q("mrow",{"data-mjx-texclass":"INNER"},[Q("mo",{"data-mjx-texclass":"OPEN"},"("),Q("mfrac",null,[Q("mrow",null,[Q("mi",null,"Q"),Q("msup",null,[Q("mi",null,"K"),Q("mi",null,"T")])]),Q("msqrt",null,[Q("msub",null,[Q("mi",null,"d"),Q("mi",null,"k")])])]),Q("mo",{"data-mjx-texclass":"CLOSE"},")")]),Q("mi",null,"V")])],-1))]),Q("p",null,[T[6]||(T[6]=t("这个公式的直观理解是：QK^T 计算每对词之间的相似度，得到注意力分数矩阵；除以 ",-1)),Q("mjx-container",H,[(s(),e("svg",g,[...T[4]||(T[4]=[a("",1)])])),T[5]||(T[5]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("msqrt",null,[Q("msub",null,[Q("mi",null,"d"),Q("mi",null,"k")])])])],-1))]),T[7]||(T[7]=t(" 进行缩放，防止点积过大导致 softmax 进入饱和区；softmax 将分数归一化为概率分布；最后用这个分布对 V 加权求和。",-1))]),T[56]||(T[56]=Q("p",null,'可以把 Q、K、V 理解为检索系统中的三个角色：Query 是"我想找什么"，Key 是"有什么标签可以匹配"，Value 是"实际内容"。每个词同时扮演三种角色——既发出查询，也作为被查询的目标，最终得到的是所有词对其的"关注程度"加权后的信息聚合。通过自注意机制，模型可以分析一个句子中的各个词语之间具有怎样的关系，充分地理解每一词的语义，此即为自注意的含义。',-1)),T[57]||(T[57]=Q("p",null,'举例来说，处理句子 "The cat sat on the mat" 时，"sat" 这个词的 Query 会与所有词的 Key 计算相似度。由于主谓关系，"cat" 会得到较高权重；由于动宾关系，"mat" 也会有显著权重。这样 "sat" 的最终表示就融合了主语和宾语的信息。',-1)),Q("p",null,[T[12]||(T[12]=t("缩放因子 ",-1)),Q("mjx-container",u,[(s(),e("svg",V,[...T[8]||(T[8]=[a("",1)])])),T[9]||(T[9]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("msqrt",null,[Q("msub",null,[Q("mi",null,"d"),Q("mi",null,"k")])])])],-1))]),T[13]||(T[13]=t(" 经常被忽略，但对训练稳定性至关重要。当 d_k 较大（如 512）时，点积的方差会达到 512，导致 softmax 函数的输入值过大，梯度接近零。除以 ",-1)),Q("mjx-container",w,[(s(),e("svg",x,[...T[10]||(T[10]=[a("",1)])])),T[11]||(T[11]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("msqrt",null,[Q("msub",null,[Q("mi",null,"d"),Q("mi",null,"k")])])])],-1))]),T[14]||(T[14]=t(" 将方差归一化为 1，确保梯度不会消失。在实践中，这个细节决定了模型能否正常训练。",-1))]),T[58]||(T[58]=a("",16)),Q("p",null,[T[19]||(T[19]=t("原论文采用正弦-余弦编码方案：偶数维用 ",-1)),Q("mjx-container",L,[(s(),e("svg",f,[...T[15]||(T[15]=[a("",1)])])),T[16]||(T[16]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",null,"sin"),Q("mo",{"data-mjx-texclass":"NONE"},"⁡"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"p"),Q("mi",null,"o"),Q("mi",null,"s"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mo",null,"/")]),Q("msup",null,[Q("mn",null,"10000"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mn",null,"2"),Q("mi",null,"i"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mo",null,"/")]),Q("mi",null,"d")])]),Q("mo",{stretchy:"false"},")")])],-1))]),T[20]||(T[20]=t("，奇数维用 ",-1)),Q("mjx-container",c,[(s(),e("svg",M,[...T[17]||(T[17]=[a("",1)])])),T[18]||(T[18]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",null,"cos"),Q("mo",{"data-mjx-texclass":"NONE"},"⁡"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"p"),Q("mi",null,"o"),Q("mi",null,"s"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mo",null,"/")]),Q("msup",null,[Q("mn",null,"10000"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mn",null,"2"),Q("mi",null,"i"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mo",null,"/")]),Q("mi",null,"d")])]),Q("mo",{stretchy:"false"},")")])],-1))]),T[21]||(T[21]=t("。这种设计的巧妙之处在于不同频率对应不同粒度的位置信息（高频关注相邻位置，低频关注全局位置），且可以通过线性变换表示相对位置关系。更重要的是，这种固定编码可以外推到训练时未见过的序列长度。",-1))]),T[59]||(T[59]=Q("p",null,"后来的工作提出了多种改进方案。BERT 使用可学习的位置嵌入，简单直接但缺乏外推能力；T5 使用相对位置编码，显式建模位置间的相对距离；RoPE（旋转位置编码）通过旋转变换将位置信息注入 Q 和 K，在长文本场景下表现优异，被 LLaMA 等模型采用。",-1)),T[60]||(T[60]=Q("p",null,"位置编码的选择影响训练稳定性和长文本处理能力。固定编码收敛更快，可学习编码更灵活但需要更多调参；相对位置编码在处理超长文本时更有优势。",-1)),T[61]||(T[61]=Q("h2",{id:"残差连接",tabindex:"-1"},[t("残差连接 "),Q("a",{class:"header-anchor",href:"#残差连接","aria-label":'Permalink to "残差连接"'},"​")],-1)),Q("p",null,[T[24]||(T[24]=t("每个 Transformer 子层后都紧跟着残差连接和层归一化，结构为 ",-1)),Q("mjx-container",y,[(s(),e("svg",k,[...T[22]||(T[22]=[a("",1)])])),T[23]||(T[23]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mtext",null,"LayerNorm"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"x"),Q("mo",null,"+"),Q("mtext",null,"Sublayer"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"x"),Q("mo",{stretchy:"false"},")"),Q("mo",{stretchy:"false"},")")])],-1))]),T[25]||(T[25]=t('。残差连接的核心思想是让网络学习"残差"而非直接学习目标函数，如果最优变换接近恒等映射，网络只需将权重置零即可。这种设计极大缓解了深层网络的梯度消失问题——梯度可以直接通过恒等连接传回前面的层，无需经过多次矩阵乘法。信息流动层面，残差连接提供了"高速公路"，让原始表示能够直接传递到高层，确保浅层学到的低级特征不会被深层的变换完全覆盖。',-1))]),T[62]||(T[62]=Q("p",null,"层归一化的作用是稳定训练。与批量归一化（BatchNorm）不同，LayerNorm 对每个样本独立计算均值和方差，不依赖批量大小，这使得它非常适合处理变长序列和分布式训练。LayerNorm 将每个位置的表示归一化到相似的数值范围，防止梯度在深层网络中爆炸或消失，加速模型收敛。",-1)),Q("p",null,[T[28]||(T[28]=t("Pre-Norm 与 Post-Norm 的选择在工程实践中影响显著。原始 Transformer 使用 Post-Norm（先做子层变换，再加残差，最后归一化），这在浅层网络（6 层）中工作良好，但当网络加深到 24 层、32 层甚至更多时，Post-Norm 会导致训练不稳定——梯度要经过多次子层变换才能回流，数值容易失控。Pre-Norm 将 LayerNorm 移到残差连接之前，即 ",-1)),Q("mjx-container",Z,[(s(),e("svg",b,[...T[26]||(T[26]=[a("",1)])])),T[27]||(T[27]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mtext",null,"Sublayer"),Q("mo",{stretchy:"false"},"("),Q("mtext",null,"LayerNorm"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"x"),Q("mo",{stretchy:"false"},")"),Q("mo",{stretchy:"false"},")"),Q("mo",null,"+"),Q("mi",null,"x")])],-1))]),T[29]||(T[29]=t("，梯度可以直接通过残差路径传播，训练变得稳定得多。现代大语言模型如 GPT-3、LLaMA 都采用 Pre-Norm 配合更深层的网络（32-96 层），这已成为标准配置。",-1))]),T[63]||(T[63]=Q("h2",{id:"前馈网络",tabindex:"-1"},[t("前馈网络 "),Q("a",{class:"header-anchor",href:"#前馈网络","aria-label":'Permalink to "前馈网络"'},"​")],-1)),Q("p",null,[T[34]||(T[34]=t("每个 Transformer 层除了注意力机制外，还有一个位置无关的前馈网络（Feed-Forward Network，FFN）。标准的 FFN 结构是两层全连接：第一层将维度从 ",-1)),Q("mjx-container",_,[(s(),e("svg",v,[...T[30]||(T[30]=[a("",1)])])),T[31]||(T[31]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("msub",null,[Q("mi",null,"d"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mi",null,"m"),Q("mi",null,"o"),Q("mi",null,"d"),Q("mi",null,"e"),Q("mi",null,"l")])])])],-1))]),T[35]||(T[35]=t("（如 512）扩展到 ",-1)),Q("mjx-container",q,[(s(),e("svg",D,[...T[32]||(T[32]=[a("",1)])])),T[33]||(T[33]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("msub",null,[Q("mi",null,"d"),Q("mrow",{"data-mjx-texclass":"ORD"},[Q("mi",null,"f"),Q("mi",null,"f")])])])],-1))]),T[36]||(T[36]=t("（通常是 4 倍，即 2048），第二层再压缩回原维度。中间的激活函数常用 ReLU 或 GELU，GELU 在 Transformer 中表现更好，因为它是平滑函数，梯度变化更连续。",-1))]),T[64]||(T[64]=Q("p",null,'这种"扩展-压缩"结构看似简单，实则精妙。扩展层增加了模型的容量，让每个位置有足够的维度来存储和变换信息；压缩层迫使网络学习高效的表示，将重要特征编码到有限的空间中。从信息论角度看，FFN 相当于对每个位置的特征进行编码-解码过程，这种非线性变换是模型表达能力的关键来源。如果只有注意力而没有 FFN，Transformer 将退化为线性模型。',-1)),Q("p",null,[T[41]||(T[41]=t('激活函数的选择影响 FFN 的效果。ReLU 是最简单的选择，计算高效，但存在"死亡 ReLU"问题——负值区域梯度为零，神经元可能永久失活。GELU（Gaussian Error Linear Unit）是概率性的激活函数，定义为 ',-1)),Q("mjx-container",A,[(s(),e("svg",C,[...T[37]||(T[37]=[a("",1)])])),T[38]||(T[38]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",null,"x"),Q("mo",null,"⋅"),Q("mi",{mathvariant:"normal"},"Φ"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"x"),Q("mo",{stretchy:"false"},")")])],-1))]),T[42]||(T[42]=t("，其中 ",-1)),Q("mjx-container",S,[(s(),e("svg",P,[...T[39]||(T[39]=[a("",1)])])),T[40]||(T[40]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",{mathvariant:"normal"},"Φ"),Q("mo",{stretchy:"false"},"("),Q("mi",null,"x"),Q("mo",{stretchy:"false"},")")])],-1))]),T[43]||(T[43]=t(" 是标准正态分布的累积分布函数。GELU 在负值区域保留小梯度，避免了神经元死亡，同时其平滑特性使训练更加稳定，这也是 BERT、GPT-2 及后续模型都采用 GELU 的原因。",-1))]),T[65]||(T[65]=a("",8)),Q("p",null,[T[46]||(T[46]=t("Transformer 的 ",-1)),Q("mjx-container",N,[(s(),e("svg",j,[...T[44]||(T[44]=[a("",1)])])),T[45]||(T[45]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",null,"O"),Q("mo",{stretchy:"false"},"("),Q("msup",null,[Q("mi",null,"n"),Q("mn",null,"2")]),Q("mo",{stretchy:"false"},")")])],-1))]),T[47]||(T[47]=t(" 复杂度（n 是序列长度）是主要瓶颈。处理长文本时，注意力矩阵的内存占用和计算量迅速增长。工程上常用的优化包括：稀疏注意力（只关注部分位置，如 Longformer）、线性近似（Performer 使用核方法降低复杂度）、分层处理（先局部注意力再全局聚合）。此外，FlashAttention 等工程技巧通过优化内存访问模式，在不改变计算结果的前提下显著加速训练。",-1))]),T[66]||(T[66]=Q("p",null,"位置编码的选择也影响工程实践。固定编码无需额外参数，但可学习编码在某些任务上效果更好。相对位置编码处理变长序列更鲁棒，但实现复杂度更高。RoPE 在长文本场景下表现优异，是目前的主流选择。",-1)),T[67]||(T[67]=Q("p",null,"训练稳定性是另一个关键点。Transformer 对超参数较敏感，学习率 warm-up、梯度裁剪、LayerNorm epsilon 等细节都需要精心调优。混合精度训练（FP16/BF16）能显著加速，但需要处理数值稳定性问题；分布式训练时，张量并行、流水线并行、数据并行的组合使用是大规模训练的必备技能。",-1)),T[68]||(T[68]=Q("h2",{id:"与-rnn-lstm-的对比",tabindex:"-1"},[t("与 RNN/LSTM 的对比 "),Q("a",{class:"header-anchor",href:"#与-rnn-lstm-的对比","aria-label":'Permalink to "与 RNN/LSTM 的对比"'},"​")],-1)),Q("p",null,[T[50]||(T[50]=t("从实际应用角度总结三种架构的差异：RNN 早已被淘汰，其串行计算和梯度消失问题在工程上无法接受；LSTM 在某些对延迟敏感、序列长度可控的场景下仍有价值（如实时语音识别），但新项目基本不会采用；Transformer 已成为标准架构，尽管有 ",-1)),Q("mjx-container",R,[(s(),e("svg",E,[...T[48]||(T[48]=[a("",1)])])),T[49]||(T[49]=Q("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[Q("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[Q("mi",null,"O"),Q("mo",{stretchy:"false"},"("),Q("msup",null,[Q("mi",null,"n"),Q("mn",null,"2")]),Q("mo",{stretchy:"false"},")")])],-1))]),T[51]||(T[51]=t(" 的复杂度缺陷，但其并行化能力和可扩展性使其在 GPU/TPU 上的实际速度远快于 RNN 类模型。",-1))]),T[69]||(T[69]=Q("p",null,'特别值得注意的是，Transformer 的可扩展性催生了"规模即能力"的发现——通过增加参数量、数据量、计算量，模型会涌现出预训练时未明确教给它的能力（如上下文学习、思维链推理）。这既是技术突破，也带来了新的工程挑战：如何高效训练超大规模模型、如何评估模型能力、如何确保安全性。',-1)),T[70]||(T[70]=Q("h2",{id:"前沿趋势",tabindex:"-1"},[t("前沿趋势 "),Q("a",{class:"header-anchor",href:"#前沿趋势","aria-label":'Permalink to "前沿趋势"'},"​")],-1)),T[71]||(T[71]=Q("p",null,"从研究趋势看，Transformer 仍在演进。理解 Transformer 的设计思想，是跟进这些前沿工作的基础。",-1)),T[72]||(T[72]=Q("ul",null,[Q("li",null,"Mixture-of-Experts（MoE）架构通过稀疏激活降低计算量（如 Mixtral 8x7B）；"),Q("li",null,"状态空间模型（如 Mamba）试图在保持线性复杂度的同时逼近 Transformer 的表达能力；"),Q("li",null,"长上下文技术（如 Ring Attention、滑动窗口）将有效上下文扩展到百万 token 级别；")],-1))])}const X=l(d,[["render",F]]);export{J as __pageData,X as default};
