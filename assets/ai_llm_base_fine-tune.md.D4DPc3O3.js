import{_ as e,o as r,c as o,ah as t}from"./chunks/framework.BwbIerCg.js";const c=JSON.parse('{"title":"微调","description":"","frontmatter":{"title":"微调","order":10},"headers":[],"relativePath":"ai/llm/base/fine-tune.md","filePath":"ai/llm/base/fine-tune.md"}'),n={name:"ai/llm/base/fine-tune.md"};function i(p,a,l,h,d,s){return r(),o("div",null,[...a[0]||(a[0]=[t('<h1 id="微调" tabindex="-1">微调 <a class="header-anchor" href="#微调" aria-label="Permalink to “微调”">​</a></h1><p>预训练模型像是一个博学但不懂规矩的学者——掌握了大量知识，但不知道如何按照期望的方式运用这些知识。微调的目的就是让模型学会&quot;听话&quot;，将通用能力转化为特定任务的能力。</p><h2 id="基座模型" tabindex="-1">基座模型 <a class="header-anchor" href="#基座模型" aria-label="Permalink to “基座模型”">​</a></h2><p>我们常说的微调不是指大模型的预训练阶段中的<strong>监督微调</strong>，而是指在大模型的落地过程中，将预训练大模型从通用模型<strong>向垂直领域进行再训练</strong>的过程。从实践角度看，微调是让大语言模型真正可用的关键环节，也是大多数团队和开发者实际参与的训练阶段，例如：我们希望开发一个辅助医生进行治疗诊断的特定领域模型，就必须使用医院和更多的医学领域数据继续对通用的大模型进行再训练，从而使得大模型能够适应医学领域的应用要求。</p><h2 id="全参数微调" tabindex="-1">全参数微调 <a class="header-anchor" href="#全参数微调" aria-label="Permalink to “全参数微调”">​</a></h2><p>全参数微调是最直观的方式——更新模型的全部参数。这种方式的表达能力最强，理论上能让模型完全适应新任务，但代价也最大。训练时需要存储所有参数的梯度和优化器状态，显存占用是模型本身参数量的 3-4 倍。对于 7B 模型，单卡 A100（80GB）勉强能够训练；对于 30B 以上的模型，全参数微调需要数十张 GPU 的并行训练，成本极高。</p><p>灾难性遗忘是全参数微调的主要风险。模型在学习新任务时会逐渐忘记预训练时学到的通用知识，导致在新任务上表现提升，但在其他任务上性能急剧下降。这种现象在小数据集上尤为明显——模型过拟合到少量微调数据上，丢失了泛化能力。实践中，全参数微调只在少数场景下使用：7B 以下的小模型、极其明确的新任务（如特定领域的翻译）、数据质量极高且规模足够大。</p><p>从本质上看，全参数微调是&quot;重新长一个脑子&quot;——重塑模型的参数分布。除非有充足的计算资源和高质量的领域数据，否则这种方式往往得不偿失。这也是为什么 2025 年的实践中，全参数微调几乎被参数高效微取代。</p><h2 id="参数高效微调" tabindex="-1">参数高效微调 <a class="header-anchor" href="#参数高效微调" aria-label="Permalink to “参数高效微调”">​</a></h2><p>参数高效微调（PEFT）的核心思想是冻结大部分预训练参数，只训练少量新增参数。这种方式大幅降低显存占用和训练成本，同时避免破坏预训练学到的通用知识。PEFT 已经成为微调的主流方案，其中最流行的是 LoRA（Low-Rank Adaptation）。</p><h3 id="lora" tabindex="-1">LoRA <a class="header-anchor" href="#lora" aria-label="Permalink to “LoRA”">​</a></h3><p>LoRA 的原理很优雅：不直接修改模型的原始权重矩阵，而是在原始权重矩阵旁边添加一个低秩&quot;偏移量&quot;。数学上，原始权重 W 被替换为 W + AB，其中 A 是 d×r 矩阵，B 是 r×d 矩阵，r &lt;&lt; d（秩通常设为 8-64）。训练时只更新 A 和 B，推理时可以将 AB 合并到 W 中，不增加推理开销。一个 7B 模型，全参数微调需要训练 70 亿参数，而使用 LoRA（r=16）只需要训练约 2300 万参数，减少了 99.7%。</p><p>LoRA 的优势明显：显存友好（单卡 A100 可以微调 70B 模型）、可插拔（不同任务的 LoRA 权重可以快速切换）、可叠加（理论上可以在同一基础模型上叠加多个 LoRA）。实践中，LoRA 常用于行业模型（金融、医疗、法律）、风格模型（特定写作风格）、角色模型（角色扮演对话）、工具使用能力增强（函数调用、代码执行）。</p><p>使用 LoRA 时需要注意几点。首先，LoRA 是能力记忆不是事实记忆——它教会模型&quot;怎么回答&quot;而非&quot;回答什么&quot;。如果需要注入新知识（如公司内部文档），应该结合检索增强生成（RAG）。其次，LoRA 叠加多了会冲突——同一基础模型上的多个 LoRA 可能相互干扰，实践中建议使用合并或路由机制。最后，LoRA 的秩需要根据任务调整——简单任务用小秩（r=8），复杂任务用大秩（r=64 或更大）。</p><h3 id="其他-peft-方法" tabindex="-1">其他 PEFT 方法 <a class="header-anchor" href="#其他-peft-方法" aria-label="Permalink to “其他 PEFT 方法”">​</a></h3><p>Adapter 是另一种常见的 PEFT 技术，在 Transformer 层之间插入小型神经网络。这些 Adapter 网络通常只有两层全连接，中间维度很小（如 64-256），只占模型参数的 1-5%。训练时冻结原模型，只训练 Adapter。优点是比 LoRA 更稳定（不修改原权重的计算路径），但缺点是推理时需要为每个任务单独加载 Adapter，工程复杂度高。IA³ 进一步简化，只训练缩放系数而不引入额外参数，效果通常略弱于 LoRA 和 Adapter。</p><p>Prefix Tuning 和 Prompt Tuning 的思路是在输入层学一段&quot;可训练的 prompt 向量&quot;，不修改模型结构。这些虚拟的 prompt 向量在训练时通过梯度下降优化，推理时拼接到真实输入前。优点是参数极少（数万级别）、快速试验不同任务，缺点是表达能力有限、长期性能不如 LoRA。这种方法适合快速原型验证，但不适合生产环境。</p><p>从工程实践看，LoRA 在 2025 年几乎赢得了 PEFT 的竞争——生态完善、效果稳定、工具支持好（PEFT、trl、llama-factory 等库都有成熟实现）。除非有特殊需求，否则建议直接使用 LoRA。</p><h2 id="对齐型微调" tabindex="-1">对齐型微调 <a class="header-anchor" href="#对齐型微调" aria-label="Permalink to “对齐型微调”">​</a></h2><p>对齐型微调关注的是模型的行为边界，而非特定任务的性能。这类微调让模型学会&quot;如何回答&quot;而非&quot;回答什么&quot;，包括遵循指令、输出格式、安全边界、帮助性等方面。</p><h3 id="监督微调-sft" tabindex="-1">监督微调（SFT） <a class="header-anchor" href="#监督微调-sft" aria-label="Permalink to “监督微调（SFT）”">​</a></h3><p>SFT 使用人工标注的&quot;输入→理想输出&quot;对进行训练，奠定模型的&quot;基本人格&quot;。数据通常由人类专家编写，覆盖多样化任务（写作、编程、翻译、数学、问答），规模在 1 万到 10 万条之间。与预训练的海量数据相比，SFT 数据量虽小但质量要求极高，一条高质量标注数据的价值远超一千条原始文本。</p><p>一个常见误区是认为 SFT 是教知识，实际上是教&quot;怎么回答&quot;。模型的知识来自预训练，SFT 只是教会它如何运用这些知识回答问题。例如，预训练模型看到&quot;今天天气&quot;会继续生成&quot;真不错&quot;这样的续写，而经过 SFT 的模型能理解&quot;今天天气怎么样&quot;是一个需要回答的问题，并生成&quot;今天天气晴朗，温度 25 度&quot;这样的响应。这种能力转化是 SFT 的核心价值。</p><p>SFT 数据的质量决定效果。标注时要注意：多样性（避免模式坍塌，模型总是生成相似的回答）、准确性（答案必须正确，错误数据会误导模型）、完整性（对于复杂任务，提供详细的推理过程而非仅给出结果）。自动化标注是 2025 年的趋势——使用强模型（如 GPT-4）生成训练数据，然后用人工校验少量样本，既降低成本又能保证质量。</p><h3 id="rlhf-与-dpo" tabindex="-1">RLHF 与 DPO <a class="header-anchor" href="#rlhf-与-dpo" aria-label="Permalink to “RLHF 与 DPO”">​</a></h3><p>RLHF（Reinforcement Learning from Human Feedback，人类反馈强化学习）通过奖励模型和强化学习优化模型的主观质量。训练分为两步：首先收集人类排序数据训练奖励模型（对同一问题的多个回答进行排序），然后使用 PPO 算法优化模型使高奖励回答的出现概率增加。RLHF 能显著改善模型的安全性、帮助性和诚实性，是 ChatGPT 成功的关键。</p><p>但 RLHF 的代价高昂——训练复杂、稳定性差、需要大量人类标注。2024 年兴起的 DPO（Direct Preference Optimization，直接偏好优化）通过简化问题彻底改变了这一格局。DPO 不需要训练奖励模型，直接使用偏好对（好答案 vs 坏答案）优化模型，数学上等价于 RLHF 的静态版本，但实现简单、训练稳定、成本降低 90% 以上。DPO 已成为 2025 年的主流对齐方式。</p><h2 id="工具与平台" tabindex="-1">工具与平台 <a class="header-anchor" href="#工具与平台" aria-label="Permalink to “工具与平台”">​</a></h2><p>微调工具的成熟度直接影响开发效率。llama-factory 是当前最流行的开源微调框架之一，支持全参数微调、LoRA、QLoRA（量化版 LoRA，4-bit 量化训练）、DPO 等多种方法，提供可视化界面和丰富的预训练模型支持。Hugging Face TRL（Transformer Reinforcement Learning）库提供了 RLHF 和 DPO 的实现，与 PEFT 库无缝集成。对于开发者来说，这些工具将微调的门槛降低到了笔记本级别——单张消费级显卡（如 RTX 4090）就可以微调 7B 模型。</p><p>云平台提供了开箱即用的微调服务。Hugging Face AutoTrain、Anyscale、Modal 等平台支持上传数据、选择模型、自动训练，无需关心底层基础设施。这些平台适合快速试验，但成本较高（按 GPU 时间计费）。对于生产环境，建议在自有 GPU 集群上使用开源框架，长期成本更低且可控性更强。</p><h2 id="实践建议" tabindex="-1">实践建议 <a class="header-anchor" href="#实践建议" aria-label="Permalink to “实践建议”">​</a></h2><p>微调之前，先评估是否真的需要微调。如果任务是通过提示工程就能解决（如文本分类、信息提取），那么直接使用 API 或开源模型可能更经济。微调适合的场景包括：需要特定领域的知识表达、需要特定的输出格式、需要持续的低延迟响应（微调模型比 API 响应更快）、数据隐私要求（不能将数据发送到第三方）。</p><p>选择基础模型时，越大越好吗？不一定。对于大多数任务，7B-13B 的开源模型（如 LLaMA-2、Mistral、Qwen）已经足够。大模型（30B+）在复杂推理任务上表现更好，但微调成本也更高。建议从小模型开始试验，验证可行性后再考虑大模型。基础模型的选择要考虑任务类型——代码生成选 CodeLLaMA，中文任务选 Qwen，多语言任务选 Mistral。</p><p>LoRA 配置上，秩 r=16 是好的起点。如果发现模型学习能力不足（无法学习新任务），可以增加到 r=32 或 r=64；如果发现过拟合（训练集表现好但测试集差），可以减小到 r=8。学习率建议从 1e-4 开始，如果训练不稳定可以降低到 5e-5 或更低。训练轮数通常 3-5 轮足够，过多会导致过拟合。</p><p>数据质量比数量更重要。在投入计算资源之前，先人工检查标注数据——答案是否准确、风格是否一致、是否覆盖了任务的主要场景。一个有用的技巧是先用小模型（如 1B 参数）快速试验，如果小模型无法从数据中学到有用的模式，那么大模型也不会更好。</p><p>评估微调效果时，除了自动指标（如准确率、BLEU 分数），一定要进行人工评估。生成式任务的自动指标往往不可靠（两个看似不同的答案可能都正确），人工抽样评估能发现模型的实际问题和改进方向。建立评估集（与训练集分离），定期评估模型表现，防止训练过程中的退化。</p>',36)])])}const u=e(n,[["render",i]]);export{c as __pageData,u as default};
