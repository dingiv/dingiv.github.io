import{_ as d,o as e,c as a,ah as r}from"./chunks/framework.BvDvRtye.js";const P=JSON.parse('{"title":"集群","description":"","frontmatter":{"title":"集群","order":60},"headers":[],"relativePath":"ai/compute/distributed/index.md","filePath":"ai/compute/distributed/index.md"}'),i={name:"ai/compute/distributed/index.md"};function n(o,t,h,s,l,p){return e(),a("div",null,[...t[0]||(t[0]=[r('<h1 id="集群" tabindex="-1">集群 <a class="header-anchor" href="#集群" aria-label="Permalink to “集群”">​</a></h1><p>AI 的计算需要消耗大量的计算资源，AI 引擎需要基于分布式集群为前提进行设计和实现。单张 GPU 的算力和显存有限，训练大模型（如 GPT-3 175B）需要数千张 GPU 协同工作，推理高并发请求也需要多 GPU 甚至多机集群。分布式集群涉及通信拓扑、硬件互联、集合通信等多个层面，需要在算法、系统和工程三个层面进行协同优化。</p><h2 id="硬件互联" tabindex="-1">硬件互联 <a class="header-anchor" href="#硬件互联" aria-label="Permalink to “硬件互联”">​</a></h2><p>集群内部的通信带宽直接影响分布式训练和推理的性能。从 GPU 到 GPU，从节点到节点，不同层次的通信技术带宽差异巨大。</p><table tabindex="0"><thead><tr><th>互联技术</th><th>带宽</th><th>延迟</th><th>覆盖范围</th><th>适用场景</th></tr></thead><tbody><tr><td>NVLink</td><td>450-900 GB/s</td><td>&lt;1 μs</td><td>节点内（8 卡）</td><td>张量并行、高频通信</td></tr><tr><td>PCIe 5.0</td><td>64 GB/s</td><td>~1 μs</td><td>节点内（CPU-GPU）</td><td>数据传输</td></tr><tr><td>InfiniBand NDR</td><td>400 Gbps (50 GB/s)</td><td>~1 μs</td><td>节点间</td><td>数据并行、跨节点通信</td></tr><tr><td>RoCE v2</td><td>100-200 Gbps (12.5-25 GB/s)</td><td>~2 μs</td><td>节点间</td><td>以太网 RDMA</td></tr><tr><td>以太网</td><td>25-100 Gbps (3-12 GB/s)</td><td>~10 μs</td><td>节点间</td><td>成本敏感场景</td></tr></tbody></table><p>NVLink 是 NVIDIA 的 GPU 间高速互联技术，带宽远超 PCIe，适用于节点内的高频通信（如张量并行）。InfiniBand 是数据中心级的高性能网络，带宽接近 PCIe、延迟低，适用于跨节点通信。RoCE（RDMA over Converged Ethernet）允许在以太网上进行 RDMA，成本低于 InfiniBand 但性能也略低。</p><h2 id="通信层次" tabindex="-1">通信层次 <a class="header-anchor" href="#通信层次" aria-label="Permalink to “通信层次”">​</a></h2><p>分布式训练和推理的通信可分为三个层次：机内通信（GPU-GPU）、机间通信（节点-节点）、跨机房通信（数据中心-数据中心）。</p><p>机内通信通过 NVLink 或 PCIe 完成，带宽高延迟低，适合张量并行等高频通信场景。机间通信通过 InfiniBand 或以太网完成，带宽较低，适合数据并行等低频通信场景。跨机房通信用于跨数据中心训练（如联邦学习），带宽最低且延迟最高，需要专门的优化算法（如梯度压缩、异步训练）。</p><h2 id="通信优化" tabindex="-1">通信优化 <a class="header-anchor" href="#通信优化" aria-label="Permalink to “通信优化”">​</a></h2><p>通信与计算重叠（overlap）是将通信时间隐藏在计算时间中的关键技术。DeepSpeed 的梯度预取在前向传播计算层 i 的梯度时，预取层 i+1 的参数到显存，同时同步层 i-1 的梯度，实现三级流水。梯度累积是简单示例：在前向传播计算 mini-batch 1 的同时，同步 mini-batch 0 的梯度。</p><p>拓扑感知通信根据网络拓扑优化通信路径。8 张 GPU 组成的单机，ring allreduce 比树状通信效率更高；64 台机器组成的集群，hierarchical allreduce（节点内用 ring，节点间用 tree）最优。NCCL 会自动检测拓扑，选择最优通信算法。</p><h2 id="通信库" tabindex="-1">通信库 <a class="header-anchor" href="#通信库" aria-label="Permalink to “通信库”">​</a></h2><p>NCCL（NVIDIA Collective Communications Library）是 NVIDIA 提供的集合通信库，针对 NVIDIA GPU 和网络优化，性能远高于开源实现（如 Gloo、MPI）。RCCL 是 AMD GPU 的对应实现。Gloo 是 PyTorch 的通用通信后端，支持 CPU 和 GPU，适合开发测试环境。</p>',14)])])}const b=d(i,[["render",n]]);export{P as __pageData,b as default};
